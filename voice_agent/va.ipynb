{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb9a87c",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '‚Üì' (U+2193) (2676855577.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 10\u001b[1;36m\u001b[0m\n\u001b[1;33m    ‚Üì\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '‚Üì' (U+2193)\n"
     ]
    }
   ],
   "source": [
    "# Voice Agent for CV Scoring Explainability\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "This notebook implements a conversational AI agent to explain CV scoring decisions to HR recruiters.\n",
    "\n",
    "### System Flow:\n",
    "```\n",
    "[HR Recruiter Voice Input]\n",
    "    ‚Üì\n",
    "[VAD] Voice Activity Detection\n",
    "    ‚Üì\n",
    "[STT] Speech-to-Text (Whisper)\n",
    "    ‚Üì\n",
    "[RAG Engine] LangChain + Ollama llama3.2\n",
    "    ‚îú‚îÄ Context: CV + JD + Scoring Results\n",
    "    ‚îú‚îÄ Retrieval: ChromaDB (CV sections)\n",
    "    ‚îî‚îÄ LLM: Generate explanations\n",
    "    ‚Üì\n",
    "[TTS] Text-to-Speech\n",
    "    ‚Üì\n",
    "[Audio Output]\n",
    "```\n",
    "\n",
    "## Implementation Phases:\n",
    "- **Phase 1**: Text-based RAG chat (current)\n",
    "- **Phase 2**: Add STT (voice input)\n",
    "- **Phase 3**: Add TTS (voice output)\n",
    "- **Phase 4**: Real-time VAD\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 1: Text-Based RAG Chat Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb561d8",
   "metadata": {},
   "source": [
    "### 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "318f453e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dependencies imported successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(os.getcwd())))\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "# MongoDB for fetching CV and JD data\n",
    "from pymongo import MongoClient\n",
    "import yaml\n",
    "\n",
    "print(\"‚úÖ Dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2090b56e",
   "metadata": {},
   "source": [
    "### 2. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa0fa296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded\n",
      "‚úÖ Connected to MongoDB: CV\n",
      "‚úÖ Ollama LLM initialized: llama3.2:latest\n",
      "‚úÖ Embeddings model: mxbai-embed-large\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "config_path = \"../config.yaml\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# MongoDB connection\n",
    "mongo_client = MongoClient(config[\"mongodb\"][\"connection_string\"])\n",
    "cv_db = mongo_client[config[\"mongodb\"][\"cv_db_name\"]]\n",
    "cv_collection = cv_db[config[\"mongodb\"][\"cv_collection_name\"]]\n",
    "jd_db = mongo_client[config[\"mongodb\"][\"jd_db_name\"]]\n",
    "jd_collection = jd_db[config[\"mongodb\"][\"jd_collection_name\"]]\n",
    "\n",
    "# Ollama LLM setup\n",
    "llm = Ollama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Ollama embeddings (same as your existing setup)\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=config[\"embedding\"][\"model\"],\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"‚úÖ Connected to MongoDB: {config['mongodb']['cv_db_name']}\")\n",
    "print(f\"‚úÖ Ollama LLM initialized: llama3.2:latest\")\n",
    "print(f\"‚úÖ Embeddings model: {config['embedding']['model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110fd708",
   "metadata": {},
   "source": [
    "### 3. Context Builder - Load CV, JD, and Scoring Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd8a7624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CVScoringContext class defined\n"
     ]
    }
   ],
   "source": [
    "class CVScoringContext:\n",
    "    \"\"\"\n",
    "    Builds context for the RAG agent by loading:\n",
    "    1. Candidate's CV (raw + structured)\n",
    "    2. Job Description\n",
    "    3. Scoring results (vector, BM25, cross-encoder scores)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cv_id, jd_id, company_name: str = None):\n",
    "        self.cv_id = cv_id\n",
    "        self.jd_id = jd_id\n",
    "        self.company_name = company_name\n",
    "        self.cv_data = None\n",
    "        self.jd_data = None\n",
    "        self.scoring_results = None\n",
    "    \n",
    "    @staticmethod\n",
    "    def safe_join(value, default=\"N/A\"):\n",
    "        \"\"\"Safely join a list or return string value\"\"\"\n",
    "        if value is None:\n",
    "            return default\n",
    "        if isinstance(value, list):\n",
    "            return ', '.join(str(v) for v in value) if value else default\n",
    "        if isinstance(value, str):\n",
    "            return value\n",
    "        return str(value)\n",
    "        \n",
    "    def load_cv_data(self):\n",
    "        \"\"\"Load CV from MongoDB - try multiple ID field names\"\"\"\n",
    "        # Try cv_id field first, then _id (MongoDB default)\n",
    "        query = {\"cv_id\": self.cv_id}\n",
    "        self.cv_data = cv_collection.find_one(query)\n",
    "        \n",
    "        if not self.cv_data:\n",
    "            # Try using _id field\n",
    "            try:\n",
    "                from bson import ObjectId\n",
    "                if isinstance(self.cv_id, str) and len(self.cv_id) == 24:\n",
    "                    query = {\"_id\": ObjectId(self.cv_id)}\n",
    "                else:\n",
    "                    query = {\"_id\": self.cv_id}\n",
    "                self.cv_data = cv_collection.find_one(query)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if not self.cv_data:\n",
    "            raise ValueError(f\"CV not found with ID: {self.cv_id}\")\n",
    "        \n",
    "        return self.cv_data\n",
    "    \n",
    "    def load_jd_data(self):\n",
    "        \"\"\"Load Job Description from MongoDB - try multiple ID field names\"\"\"\n",
    "        # Try jd_id field first, then _id (MongoDB default)\n",
    "        query = {\"jd_id\": self.jd_id}\n",
    "        if self.company_name:\n",
    "            query[\"company_name\"] = self.company_name\n",
    "            \n",
    "        self.jd_data = jd_collection.find_one(query)\n",
    "        \n",
    "        if not self.jd_data:\n",
    "            # Try using _id field\n",
    "            try:\n",
    "                from bson import ObjectId\n",
    "                if isinstance(self.jd_id, str) and len(self.jd_id) == 24:\n",
    "                    query = {\"_id\": ObjectId(self.jd_id)}\n",
    "                else:\n",
    "                    query = {\"_id\": self.jd_id}\n",
    "                if self.company_name:\n",
    "                    query[\"company_name\"] = self.company_name\n",
    "                self.jd_data = jd_collection.find_one(query)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if not self.jd_data:\n",
    "            raise ValueError(f\"JD not found with ID: {self.jd_id}\")\n",
    "        \n",
    "        return self.jd_data\n",
    "    \n",
    "    def build_context_document(self):\n",
    "        \"\"\"\n",
    "        Build a comprehensive context document for the RAG agent\n",
    "        Returns a formatted string with all relevant information\n",
    "        \"\"\"\n",
    "        if not self.cv_data or not self.jd_data:\n",
    "            self.load_cv_data()\n",
    "            self.load_jd_data()\n",
    "        \n",
    "        context = f\"\"\"\n",
    "=== CANDIDATE INFORMATION ===\n",
    "CV ID: {self.cv_id}\n",
    "Name: {self.cv_data.get('name', 'N/A')}\n",
    "Email: {self.cv_data.get('email', 'N/A')}\n",
    "Phone: {self.cv_data.get('phone', 'N/A')}\n",
    "\n",
    "Professional Summary:\n",
    "{self.cv_data.get('summary', 'N/A')}\n",
    "\n",
    "Years of Professional Experience: {self.cv_data.get('years_of_experience', 'N/A')}\n",
    "\n",
    "Skills:\n",
    "{self.safe_join(self.cv_data.get('skills'))}\n",
    "\n",
    "Soft Skills:\n",
    "{self.safe_join(self.cv_data.get('soft_skills'))}\n",
    "\n",
    "Work Experience:\n",
    "\"\"\"\n",
    "        # Add work experience details\n",
    "        work_exp = self.cv_data.get('work_experience', [])\n",
    "        if isinstance(work_exp, list):\n",
    "            for exp in work_exp:\n",
    "                context += f\"\\n- {exp.get('title', 'N/A')} at {exp.get('company', 'N/A')}\"\n",
    "                context += f\" ({exp.get('start_date', 'N/A')} - {exp.get('end_date', 'Present')})\"\n",
    "                if exp.get('responsibilities'):\n",
    "                    resp = exp['responsibilities']\n",
    "                    if isinstance(resp, list):\n",
    "                        context += f\"\\n  Responsibilities: {'; '.join(resp[:3])}\"\n",
    "        else:\n",
    "            context += \"\\nN/A\"\n",
    "        \n",
    "        context += f\"\"\"\n",
    "\n",
    "Education:\n",
    "\"\"\"\n",
    "        # Add education details\n",
    "        education = self.cv_data.get('education', [])\n",
    "        if isinstance(education, list):\n",
    "            for edu in education:\n",
    "                context += f\"\\n- {edu.get('degree', 'N/A')} in {edu.get('field_of_study', 'N/A')}\"\n",
    "                context += f\" from {edu.get('institution', 'N/A')}\"\n",
    "        else:\n",
    "            context += \"\\nN/A\"\n",
    "        \n",
    "        # Add certifications\n",
    "        certs = self.cv_data.get('certifications', [])\n",
    "        cert_names = []\n",
    "        if isinstance(certs, list):\n",
    "            cert_names = [cert.get('name', 'N/A') for cert in certs if isinstance(cert, dict)]\n",
    "        \n",
    "        context += f\"\"\"\n",
    "\n",
    "Certifications:\n",
    "{', '.join(cert_names) if cert_names else 'N/A'}\n",
    "\n",
    "=== JOB DESCRIPTION ===\n",
    "JD ID: {self.jd_id}\n",
    "Job Title: {self.jd_data.get('job_title', 'N/A')}\n",
    "Company: {self.jd_data.get('company_name', 'N/A')}\n",
    "\n",
    "Required Skills:\n",
    "{self.safe_join(self.jd_data.get('required_skills'))}\n",
    "\n",
    "Preferred Skills:\n",
    "{self.safe_join(self.jd_data.get('preferred_skills'))}\n",
    "\n",
    "Required Qualifications:\n",
    "{self.safe_join(self.jd_data.get('required_qualifications'))}\n",
    "\n",
    "Education Requirements:\n",
    "{self.safe_join(self.jd_data.get('education_requirements'))}\n",
    "\n",
    "Experience Requirements:\n",
    "{json.dumps(self.jd_data.get('experience_requirements', {}), indent=2)}\n",
    "\n",
    "Technical Skills:\n",
    "{self.safe_join(self.jd_data.get('technical_skills'))}\n",
    "\n",
    "Soft Skills:\n",
    "{self.safe_join(self.jd_data.get('soft_skills'))}\n",
    "\n",
    "Certifications:\n",
    "{self.safe_join(self.jd_data.get('certifications'))}\n",
    "\n",
    "=== END OF CONTEXT ===\n",
    "\"\"\"\n",
    "        return context\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Return a brief summary for display\"\"\"\n",
    "        if not self.cv_data or not self.jd_data:\n",
    "            self.load_cv_data()\n",
    "            self.load_jd_data()\n",
    "            \n",
    "        return {\n",
    "            \"candidate_name\": self.cv_data.get('name', 'N/A'),\n",
    "            \"cv_id\": self.cv_id,\n",
    "            \"job_title\": self.jd_data.get('job_title', 'N/A'),\n",
    "            \"jd_id\": self.jd_id,\n",
    "            \"company\": self.jd_data.get('company_name', 'N/A')\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ CVScoringContext class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76ab25b",
   "metadata": {},
   "source": [
    "### 4. RAG Engine with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5301541b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CVExplainabilityAgent class defined\n"
     ]
    }
   ],
   "source": [
    "class CVExplainabilityAgent:\n",
    "    \"\"\"\n",
    "    RAG-based agent that explains CV scoring decisions to HR recruiters.\n",
    "    Uses LangChain + Ollama llama3.2 + ChromaDB retrieval\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, context: CVScoringContext, company_name: str = None):\n",
    "        self.context = context\n",
    "        self.company_name = company_name\n",
    "        self.llm = llm\n",
    "        self.embeddings = embeddings\n",
    "        self.vector_store = None\n",
    "        self.qa_chain = None\n",
    "        \n",
    "        # Build the context\n",
    "        self.context_text = self.context.build_context_document()\n",
    "        \n",
    "        # Setup vector store retriever\n",
    "        self._setup_vector_store()\n",
    "        \n",
    "        # Setup QA chain\n",
    "        self._setup_qa_chain()\n",
    "    \n",
    "    def _setup_vector_store(self):\n",
    "        \"\"\"Connect to existing ChromaDB or create in-memory store with context\"\"\"\n",
    "        # Option 1: Use existing CV ChromaDB\n",
    "        cv_persist_dir = config[\"chroma\"][\"cv_persist_dir\"]\n",
    "        \n",
    "        # Build company-specific collection name if provided\n",
    "        if self.company_name:\n",
    "            from backend.core.identifiers import sanitize_fragment\n",
    "            company_fragment = sanitize_fragment(self.company_name)\n",
    "            cv_collection_name = f\"{company_fragment}_cv_sections\"\n",
    "            cv_persist_dir = os.path.join(cv_persist_dir, company_fragment)\n",
    "        else:\n",
    "            cv_collection_name = config[\"chroma\"][\"cv_collection_name\"]\n",
    "        \n",
    "        try:\n",
    "            self.vector_store = Chroma(\n",
    "                collection_name=cv_collection_name,\n",
    "                embedding_function=self.embeddings,\n",
    "                persist_directory=cv_persist_dir\n",
    "            )\n",
    "            print(f\"‚úÖ Connected to ChromaDB: {cv_collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not connect to existing ChromaDB: {e}\")\n",
    "            print(\"Creating in-memory vector store with context...\")\n",
    "            \n",
    "            # Fallback: Create in-memory vector store with context document\n",
    "            docs = [Document(page_content=self.context_text, metadata={\"source\": \"cv_jd_context\"})]\n",
    "            self.vector_store = Chroma.from_documents(\n",
    "                documents=docs,\n",
    "                embedding=self.embeddings\n",
    "            )\n",
    "            print(\"‚úÖ In-memory vector store created\")\n",
    "    \n",
    "    def _setup_qa_chain(self):\n",
    "        \"\"\"Setup the QA chain with custom prompt for explainability\"\"\"\n",
    "        \n",
    "        # Custom prompt template for CV scoring explainability\n",
    "        template = \"\"\"You are an AI assistant helping HR recruiters understand CV scoring decisions.\n",
    "\n",
    "Context Information:\n",
    "{context}\n",
    "\n",
    "Based on the above context, answer the HR recruiter's question clearly and transparently.\n",
    "\n",
    "Guidelines:\n",
    "1. Be specific and cite exact details from the CV and job description\n",
    "2. Explain scoring factors (skills match, experience, qualifications)\n",
    "3. Use a professional but conversational tone\n",
    "4. If you don't have enough information, say so clearly\n",
    "5. Provide actionable insights when possible\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "        PROMPT = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        \n",
    "        # Create retrieval QA chain\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": PROMPT}\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ QA Chain initialized\")\n",
    "    \n",
    "    def ask(self, question: str):\n",
    "        \"\"\"\n",
    "        Ask a question about the CV scoring\n",
    "        Returns the answer and source documents\n",
    "        \"\"\"\n",
    "        result = self.qa_chain({\"query\": question})\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": result[\"result\"],\n",
    "            \"sources\": result[\"source_documents\"]\n",
    "        }\n",
    "    \n",
    "    def chat(self):\n",
    "        \"\"\"Interactive chat interface\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CV SCORING EXPLAINABILITY AGENT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        summary = self.context.get_summary()\n",
    "        print(f\"\\nCandidate: {summary['candidate_name']} (CV ID: {summary['cv_id']})\")\n",
    "        print(f\"Job Title: {summary['job_title']} (JD ID: {summary['jd_id']})\")\n",
    "        if summary['company']:\n",
    "            print(f\"Company: {summary['company']}\")\n",
    "        \n",
    "        print(\"\\nType your question (or 'quit' to exit)\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"\\nüé§ HR: \").strip()\n",
    "            \n",
    "            if question.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"\\nüëã Ending session. Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            if not question:\n",
    "                continue\n",
    "            \n",
    "            # Get answer from RAG agent\n",
    "            print(\"\\nü§ñ Agent: \", end=\"\", flush=True)\n",
    "            result = self.ask(question)\n",
    "            print(result[\"answer\"])\n",
    "            \n",
    "            # Optionally show sources\n",
    "            if result[\"sources\"]:\n",
    "                print(f\"\\nüìö Sources: {len(result['sources'])} document(s) retrieved\")\n",
    "\n",
    "print(\"‚úÖ CVExplainabilityAgent class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ca1378",
   "metadata": {},
   "source": [
    "### 5. Test the Agent - Interactive Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fda2f56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CVs:\n",
      "------------------------------------------------------------\n",
      "1. Enoch Kwadwo Aidoo (CV ID: 2e538000bef0ba2c6bfd10f0fb99b0d97843da9e35f46b255c59141bc3660484)\n",
      "   Skills: Python, R, AWS, Microsoft Excel, Google Sheets\n",
      "\n",
      "2. Evans Kwarteng (CV ID: 4fadad2ef2dd12998395da7788fda9d02ed793c28e70f11afad1da0f74651fa2)\n",
      "   Skills: Microsoft Excel, Google Sheets, SPSS, Data Analysis, Data Visualization\n",
      "\n",
      "\n",
      "Available Job Descriptions:\n",
      "------------------------------------------------------------\n",
      "1. Data Analyst (JD ID: 22c009485a6d8f139582719426054c126f7f8b426351dbfb5681cddb42ae180d)\n",
      "   Required Skills: Strong problem-solving abilities with attention to detail, Experience with experimental design and statistical inference, Ability to work with ambiguous requirements and define analytical approaches, Understanding of business metrics and KPI development\n",
      "\n",
      "2. Field and Forest Resource Officer (JD ID: d2af1b809ff9ed2e34048e5a97c946bf02935715fd21e906ca2f8a3e26952e18)\n",
      "   Required Skills: Forest inventory, Field data collection, Environmental monitoring, Sustainable forest management principles, Field operations\n",
      "\n",
      "3. Field and Forest Resource Officer (JD ID: 62190a4f97058cfb86507ece76bb7a5a06b29277ae720d8bbbddbe6b4cabb55d)\n",
      "   Company: Really Great Tech\n",
      "   Required Skills: Hands-on experience in forest inventory, field data collection, or environmental monitoring, Strong understanding of sustainable forest management principles and field operations\n",
      "\n",
      "\n",
      "üîç Debug: First CV structure (keys only):\n",
      "Keys: ['_id', 'name', 'email', 'phone', 'summary', 'years_of_experience', 'work_experience', 'education', 'skills', 'soft_skills', 'certifications', 'projects', 'languages', 'hobbies', 'other', 'inserted_at', 'version']\n",
      "\n",
      "üîç Debug: First JD structure (keys only):\n",
      "Keys: ['_id', 'job_title', 'required_skills', 'required_qualifications', 'preferred_skills', 'education_requirements', 'experience_requirements', 'technical_skills', 'soft_skills', 'certifications', 'responsibilities', 'inserted_at', 'version']\n"
     ]
    }
   ],
   "source": [
    "# Example: Get a CV ID and JD ID from your database\n",
    "# First, let's see what CVs we have\n",
    "sample_cvs = list(cv_collection.find().limit(5))\n",
    "\n",
    "print(\"Available CVs:\")\n",
    "print(\"-\"*60)\n",
    "for i, cv in enumerate(sample_cvs, 1):\n",
    "    # Try different possible ID fields\n",
    "    cv_id = cv.get('cv_id') or cv.get('_id') or f\"cv_{i}\"\n",
    "    print(f\"{i}. {cv.get('name', 'N/A')} (CV ID: {cv_id})\")\n",
    "    print(f\"   Skills: {', '.join(cv.get('skills', [])[:5])}\")\n",
    "    print()\n",
    "\n",
    "# Get JD samples\n",
    "sample_jds = list(jd_collection.find().limit(5))\n",
    "\n",
    "print(\"\\nAvailable Job Descriptions:\")\n",
    "print(\"-\"*60)\n",
    "for i, jd in enumerate(sample_jds, 1):\n",
    "    # Try different possible ID fields\n",
    "    jd_id = jd.get('jd_id') or jd.get('_id') or f\"jd_{i}\"\n",
    "    print(f\"{i}. {jd.get('job_title', 'N/A')} (JD ID: {jd_id})\")\n",
    "    if jd.get('company_name'):\n",
    "        print(f\"   Company: {jd.get('company_name')}\")\n",
    "    print(f\"   Required Skills: {', '.join(jd.get('required_skills', [])[:5])}\")\n",
    "    print()\n",
    "\n",
    "# Debug: Show full structure of first CV and JD\n",
    "if sample_cvs:\n",
    "    print(\"\\nüîç Debug: First CV structure (keys only):\")\n",
    "    print(f\"Keys: {list(sample_cvs[0].keys())}\")\n",
    "    \n",
    "if sample_jds:\n",
    "    print(\"\\nüîç Debug: First JD structure (keys only):\")\n",
    "    print(f\"Keys: {list(sample_jds[0].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30256068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing agent for:\n",
      "  CV: Enoch Kwadwo Aidoo (ID: 2e538000bef0ba2c6bfd10f0fb99b0d97843da9e35f46b255c59141bc3660484)\n",
      "  JD: Data Analyst (ID: 22c009485a6d8f139582719426054c126f7f8b426351dbfb5681cddb42ae180d)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enoch\\AppData\\Local\\Temp\\ipykernel_5876\\1340520858.py:39: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  self.vector_store = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to ChromaDB: cv_sections\n",
      "‚úÖ QA Chain initialized\n",
      "\n",
      "‚úÖ Agent ready! You can now ask questions.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the agent with a specific CV and JD\n",
    "# Get the ID from cv_id field or MongoDB's _id field\n",
    "\n",
    "if not sample_cvs or not sample_jds:\n",
    "    print(\"‚ùå No CVs or JDs found in database. Please upload some first.\")\n",
    "else:\n",
    "    # Get CV ID (try cv_id first, then _id)\n",
    "    cv_id = sample_cvs[0].get('cv_id') or sample_cvs[0].get('_id')\n",
    "    cv_name = sample_cvs[0].get('name', 'Unknown')\n",
    "    \n",
    "    # Get JD ID (try jd_id first, then _id)\n",
    "    jd_id = sample_jds[0].get('jd_id') or sample_jds[0].get('_id')\n",
    "    jd_title = sample_jds[0].get('job_title', 'Unknown')\n",
    "    company_name = sample_jds[0].get('company_name')\n",
    "    \n",
    "    print(f\"Initializing agent for:\")\n",
    "    print(f\"  CV: {cv_name} (ID: {cv_id})\")\n",
    "    print(f\"  JD: {jd_title} (ID: {jd_id})\")\n",
    "    if company_name:\n",
    "        print(f\"  Company: {company_name}\")\n",
    "    \n",
    "    # Create context\n",
    "    context = CVScoringContext(cv_id=cv_id, jd_id=jd_id, company_name=company_name)\n",
    "    \n",
    "    # Create agent\n",
    "    agent = CVExplainabilityAgent(context=context, company_name=company_name)\n",
    "    \n",
    "    print(\"\\n‚úÖ Agent ready! You can now ask questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0cda708e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CV SCORING EXPLAINABILITY AGENT\n",
      "============================================================\n",
      "\n",
      "Candidate: Enoch Kwadwo Aidoo (CV ID: 2e538000bef0ba2c6bfd10f0fb99b0d97843da9e35f46b255c59141bc3660484)\n",
      "Job Title: Data Analyst (JD ID: 22c009485a6d8f139582719426054c126f7f8b426351dbfb5681cddb42ae180d)\n",
      "Company: N/A\n",
      "\n",
      "Type your question (or 'quit' to exit)\n",
      "------------------------------------------------------------\n",
      "\n",
      "ü§ñ Agent: I'd be happy to help facilitate a discussion on the CV scoring decision.\n",
      "\n",
      "To better understand the situation, could you please provide more context or details about the CV being scored? Specifically:\n",
      "\n",
      "1. The job description and requirements\n",
      "2. The relevant sections of the CV (e.g., skills, work experience, education)\n",
      "3. The score given to the candidate and the specific criteria used for scoring\n",
      "\n",
      "Once I have this information, I can offer insights on whether the score seems reasonable or not, and what factors might have influenced the decision.\n",
      "\n",
      "Please share more details, and I'll do my best to provide a helpful analysis.\n",
      "\n",
      "ü§ñ Agent: The CV scores were indeed shared with me as part of our initial review process. However, I'd like to clarify that the scoring system used is not a straightforward numerical value, but rather a weighted assessment of key criteria.\n",
      "\n",
      "Our algorithm considers three primary factors when evaluating each CV:\n",
      "\n",
      "1. **Skills Match**: This score evaluates how well the candidate's skills and qualifications align with the job requirements. For instance, the CV mentions \"Proficient in Adobe Creative Suite,\" which matches one of the required skills listed in the job description.\n",
      "2. **Experience**: This factor assesses the relevance and depth of the candidate's work experience to the job role. The CV highlights 3+ years of experience in digital marketing, which aligns with the job requirements.\n",
      "3. **Qualifications**: This score evaluates the candidate's academic qualifications and any relevant certifications or training. The CV mentions a Bachelor's degree in Marketing, which is one of the preferred qualifications listed in the job description.\n",
      "\n",
      "When we shared the scores with you, it was based on this weighted assessment. If you have any specific questions about the scores or would like me to elaborate on any particular factor, please feel free to ask!\n",
      "\n",
      "ü§ñ Agent: Let's take a closer look at the CV of this candidate.\n",
      "\n",
      "Upon reviewing their CV, I notice that they have around 2 years of experience in their field, which aligns well with our job description requirements. However, I'd like to highlight that their experience is mostly focused on similar tasks and responsibilities, rather than direct role-specific skills or leadership experiences.\n",
      "\n",
      "Regarding qualifications, the candidate has a degree in [Field] from a reputable institution, which meets our qualification standards. They also hold relevant certifications, such as [Certification], which demonstrates their commitment to staying up-to-date with industry developments.\n",
      "\n",
      "Now, let's score their CV based on our scoring factors:\n",
      "\n",
      "* Skills match: 7/10 (they have some of the required skills, but not all of them)\n",
      "* Experience: 6.5/10 (while they have relevant experience, it's mostly similar tasks and responsibilities rather than direct role-specific skills or leadership experiences)\n",
      "* Qualifications: 9/10 (their degree and certifications meet our qualification standards)\n",
      "\n",
      "Overall, I'd score their CV around 7.2/10. While they have some great qualifications and a decent amount of experience, there are areas where we can improve.\n",
      "\n",
      "Actionable insights:\n",
      "\n",
      "1. Consider asking for more specific examples of how they've applied their skills in previous roles to demonstrate their expertise.\n",
      "2. Look for evidence of leadership experiences or role-specific skills that align with our job description requirements.\n",
      "3. Weigh the importance of certifications versus degree qualifications; while both are valuable, a strong degree can sometimes carry more weight.\n",
      "\n",
      "What do you think? Would you like me to elaborate on any specific points or provide further scoring analysis?\n",
      "\n",
      "üëã Ending session. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Start the interactive chat\n",
    "# Type your questions and the agent will respond\n",
    "agent.chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b5bdea",
   "metadata": {},
   "source": [
    "### 6. Single Question Example (Non-Interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7cb9c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the candidate's main technical skills?\n",
      "\n",
      "Answer:\n",
      "Based on the CV provided, I've identified the candidate's main technical skills as follows:\n",
      "\n",
      "1. **Programming languages:** The candidate has listed proficiency in Java, Python, and C++. According to the job description, we're looking for a developer with expertise in at least two programming languages. The candidate meets this requirement.\n",
      "2. **Data structures and algorithms:** The CV mentions experience with data structures (arrays, linked lists, stacks, and queues) and algorithms (sorting, searching, and graph traversal). These skills are highly relevant to the job requirements, which emphasize proficiency in designing efficient data structures and algorithms.\n",
      "3. **Cloud computing:** Although not explicitly stated, the candidate has listed experience with Amazon Web Services (AWS), which is a cloud computing platform mentioned in the job description. This demonstrates their ability to work in a cloud environment.\n",
      "4. **Database management:** The CV indicates familiarity with relational databases (MySQL) and NoSQL databases (MongoDB). These skills align with the job requirements, which mention experience with various database systems.\n",
      "\n",
      "To further assess the candidate's technical skills, I'd like to clarify:\n",
      "\n",
      "* Are there any specific programming languages or technologies mentioned in the job description that weren't listed on the CV?\n",
      "* Can you provide more details about the data structures and algorithms mentioned in the job description?\n",
      "\n",
      "Actionable insights:\n",
      "\n",
      "* Given the candidate's strong background in programming languages, data structures, and algorithms, they seem like a strong contender for this role.\n",
      "* However, to further evaluate their fit, it would be helpful to know if there are any specific cloud computing or database management technologies mentioned in the job description that weren't covered on the CV.\n",
      "\n",
      "Please let me know if you have any additional information or questions!\n",
      "\n",
      "Retrieved 0 source document(s)\n"
     ]
    }
   ],
   "source": [
    "# Example questions you can ask:\n",
    "questions = [\n",
    "    \"What are the candidate's main technical skills?\",\n",
    "    \"Does this candidate have experience with the required technologies?\",\n",
    "    \"Why would this candidate be a good fit for this role?\",\n",
    "    \"What skills is the candidate missing for this position?\",\n",
    "    \"How many years of professional experience does this candidate have?\"\n",
    "]\n",
    "\n",
    "# Ask a single question\n",
    "question = questions[0]\n",
    "print(f\"Question: {question}\\n\")\n",
    "\n",
    "result = agent.ask(question)\n",
    "print(f\"Answer:\\n{result['answer']}\\n\")\n",
    "print(f\"Retrieved {len(result['sources'])} source document(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58477dbc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps: Adding Voice Capabilities\n",
    "\n",
    "### Phase 2: Speech-to-Text (STT)\n",
    "- Install Whisper: `pip install openai-whisper` or use Whisper API\n",
    "- Capture audio from microphone\n",
    "- Convert speech to text in real-time\n",
    "\n",
    "### Phase 3: Text-to-Speech (TTS)\n",
    "- Use `pyttsx3` (local, fast) or Coqui TTS (better quality)\n",
    "- Convert agent responses to speech\n",
    "- Play audio through speakers\n",
    "\n",
    "### Phase 4: Voice Activity Detection (VAD)\n",
    "- Install `silero-vad` or use WebRTC VAD\n",
    "- Detect when HR starts/stops speaking\n",
    "- Enable seamless conversation flow\n",
    "\n",
    "---\n",
    "\n",
    "## Example Questions HR Can Ask:\n",
    "\n",
    "1. **Scoring Explanation**\n",
    "   - \"Why did this candidate get a score of 0.87?\"\n",
    "   - \"How was the final score calculated?\"\n",
    "   - \"What contributed most to their score?\"\n",
    "\n",
    "2. **Skills Matching**\n",
    "   - \"What skills match the job requirements?\"\n",
    "   - \"What skills is the candidate missing?\"\n",
    "   - \"Does the candidate have experience with Python?\"\n",
    "\n",
    "3. **Experience Comparison**\n",
    "   - \"How does their experience compare to requirements?\"\n",
    "   - \"Do they have enough years of experience?\"\n",
    "   - \"What companies have they worked for?\"\n",
    "\n",
    "4. **Education & Certifications**\n",
    "   - \"What degree does this candidate have?\"\n",
    "   - \"Do they have relevant certifications?\"\n",
    "   - \"Does their education meet the requirements?\"\n",
    "\n",
    "5. **Comparative Analysis**\n",
    "   - \"Why is candidate A ranked higher than candidate B?\"\n",
    "   - \"Which candidate has more relevant experience?\"\n",
    "   - \"Who has the best technical skills for this role?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9200e6e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 2 & 3: Voice Integration (STT + TTS)\n",
    "\n",
    "Now let's add speech-to-text and text-to-speech capabilities!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7927de",
   "metadata": {},
   "source": [
    "### 7. Voice Dependencies - Install Required Packages\n",
    "\n",
    "Run this cell to install voice-related packages:\n",
    "```bash\n",
    "pip install openai-whisper sounddevice pyttsx3 webrtcvad numpy scipy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2160d3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Voice libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Install voice dependencies (run once)\n",
    "# Uncomment and run if not installed:\n",
    "# !pip install openai-whisper sounddevice pyttsx3 webrtcvad numpy scipy\n",
    "\n",
    "# Import voice libraries\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import whisper\n",
    "import pyttsx3\n",
    "import wave\n",
    "import tempfile\n",
    "import threading\n",
    "import queue\n",
    "from scipy.io import wavfile\n",
    "\n",
    "print(\"‚úÖ Voice libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d08bfd9",
   "metadata": {},
   "source": [
    "### 8. Speech-to-Text (STT) Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf17a6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SpeechToText class defined\n"
     ]
    }
   ],
   "source": [
    "class SpeechToText:\n",
    "    \"\"\"\n",
    "    Speech-to-Text using Whisper\n",
    "    Captures audio from microphone and transcribes to text\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"base\", sample_rate=16000):\n",
    "        \"\"\"\n",
    "        Initialize STT with Whisper model\n",
    "        \n",
    "        Args:\n",
    "            model_name: Whisper model size ('tiny', 'base', 'small', 'medium', 'large')\n",
    "            sample_rate: Audio sample rate (16000 Hz for Whisper)\n",
    "        \"\"\"\n",
    "        print(f\"Loading Whisper model: {model_name}...\")\n",
    "        self.model = whisper.load_model(model_name)\n",
    "        self.sample_rate = sample_rate\n",
    "        print(f\"‚úÖ Whisper {model_name} model loaded\")\n",
    "    \n",
    "    def record_audio(self, duration=5, silence_threshold=0.01):\n",
    "        \"\"\"\n",
    "        Record audio from microphone\n",
    "        \n",
    "        Args:\n",
    "            duration: Maximum recording duration in seconds\n",
    "            silence_threshold: Volume threshold to detect silence\n",
    "        \n",
    "        Returns:\n",
    "            numpy array of audio data\n",
    "        \"\"\"\n",
    "        print(f\"\\nüé§ Recording... (speak now, max {duration}s)\")\n",
    "        \n",
    "        # Record audio\n",
    "        audio = sd.rec(\n",
    "            int(duration * self.sample_rate),\n",
    "            samplerate=self.sample_rate,\n",
    "            channels=1,\n",
    "            dtype='float32'\n",
    "        )\n",
    "        sd.wait()  # Wait for recording to complete\n",
    "        \n",
    "        print(\"‚úÖ Recording complete\")\n",
    "        return audio.flatten()\n",
    "    \n",
    "    def transcribe_audio(self, audio_data):\n",
    "        \"\"\"\n",
    "        Transcribe audio to text using Whisper\n",
    "        \n",
    "        Args:\n",
    "            audio_data: numpy array of audio data\n",
    "        \n",
    "        Returns:\n",
    "            Transcribed text\n",
    "        \"\"\"\n",
    "        print(\"üîÑ Transcribing...\")\n",
    "        \n",
    "        # Whisper expects audio as numpy array\n",
    "        result = self.model.transcribe(audio_data, fp16=False)\n",
    "        text = result[\"text\"].strip()\n",
    "        \n",
    "        print(f\"üìù Transcribed: \\\"{text}\\\"\")\n",
    "        return text\n",
    "    \n",
    "    def listen(self, duration=5):\n",
    "        \"\"\"\n",
    "        Record audio and transcribe to text\n",
    "        \n",
    "        Args:\n",
    "            duration: Maximum recording duration in seconds\n",
    "        \n",
    "        Returns:\n",
    "            Transcribed text\n",
    "        \"\"\"\n",
    "        audio = self.record_audio(duration=duration)\n",
    "        text = self.transcribe_audio(audio)\n",
    "        return text\n",
    "\n",
    "print(\"‚úÖ SpeechToText class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d070953",
   "metadata": {},
   "source": [
    "### 9. Text-to-Speech (TTS) Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8580bacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TextToSpeech class defined\n"
     ]
    }
   ],
   "source": [
    "class TextToSpeech:\n",
    "    \"\"\"\n",
    "    Text-to-Speech using pyttsx3 (local, fast)\n",
    "    Converts text to speech and plays it\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rate=150, volume=0.9):\n",
    "        \"\"\"\n",
    "        Initialize TTS engine\n",
    "        \n",
    "        Args:\n",
    "            rate: Speech rate (words per minute)\n",
    "            volume: Volume level (0.0 to 1.0)\n",
    "        \"\"\"\n",
    "        self.engine = pyttsx3.init()\n",
    "        self.engine.setProperty('rate', rate)\n",
    "        self.engine.setProperty('volume', volume)\n",
    "        \n",
    "        # Optional: Set voice (uncomment to choose)\n",
    "        # voices = self.engine.getProperty('voices')\n",
    "        # self.engine.setProperty('voice', voices[0].id)  # 0=male, 1=female (usually)\n",
    "        \n",
    "        print(\"‚úÖ TTS engine initialized\")\n",
    "    \n",
    "    def speak(self, text):\n",
    "        \"\"\"\n",
    "        Convert text to speech and play it\n",
    "        \n",
    "        Args:\n",
    "            text: Text to speak\n",
    "        \"\"\"\n",
    "        print(f\"üîä Speaking: \\\"{text[:50]}...\\\"\")\n",
    "        self.engine.say(text)\n",
    "        self.engine.runAndWait()\n",
    "    \n",
    "    def speak_async(self, text):\n",
    "        \"\"\"\n",
    "        Speak text in a separate thread (non-blocking)\n",
    "        \n",
    "        Args:\n",
    "            text: Text to speak\n",
    "        \"\"\"\n",
    "        thread = threading.Thread(target=self.speak, args=(text,))\n",
    "        thread.start()\n",
    "        return thread\n",
    "\n",
    "print(\"‚úÖ TextToSpeech class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cbe889",
   "metadata": {},
   "source": [
    "### 10. Voice-Enabled Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fcc08183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ VoiceEnabledAgent class defined\n"
     ]
    }
   ],
   "source": [
    "class VoiceEnabledAgent(CVExplainabilityAgent):\n",
    "    \"\"\"\n",
    "    Voice-enabled CV Explainability Agent\n",
    "    Extends the text-based agent with voice input/output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, context: CVScoringContext, company_name: str = None, \n",
    "                 whisper_model=\"base\", speech_rate=150):\n",
    "        \"\"\"\n",
    "        Initialize voice-enabled agent\n",
    "        \n",
    "        Args:\n",
    "            context: CVScoringContext with CV and JD data\n",
    "            company_name: Optional company name\n",
    "            whisper_model: Whisper model size ('tiny', 'base', 'small', 'medium', 'large')\n",
    "            speech_rate: TTS speech rate (words per minute)\n",
    "        \"\"\"\n",
    "        # Initialize parent text-based agent\n",
    "        super().__init__(context, company_name)\n",
    "        \n",
    "        # Initialize STT and TTS\n",
    "        print(\"\\nüéôÔ∏è Initializing voice capabilities...\")\n",
    "        self.stt = SpeechToText(model_name=whisper_model)\n",
    "        self.tts = TextToSpeech(rate=speech_rate)\n",
    "        \n",
    "        print(\"‚úÖ Voice-enabled agent ready!\")\n",
    "    \n",
    "    def voice_chat(self, recording_duration=5):\n",
    "        \"\"\"\n",
    "        Interactive voice chat interface\n",
    "        \n",
    "        Args:\n",
    "            recording_duration: Maximum recording duration per question (seconds)\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üé§ VOICE-ENABLED CV SCORING EXPLAINABILITY AGENT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        summary = self.context.get_summary()\n",
    "        print(f\"\\nCandidate: {summary['candidate_name']} (CV ID: {summary['cv_id']})\")\n",
    "        print(f\"Job Title: {summary['job_title']} (JD ID: {summary['jd_id']})\")\n",
    "        if summary['company']:\n",
    "            print(f\"Company: {summary['company']}\")\n",
    "        \n",
    "        # Welcome message\n",
    "        welcome = f\"Hello! I'm your AI assistant. I can answer questions about {summary['candidate_name']}'s application for the {summary['job_title']} position.\"\n",
    "        print(f\"\\nü§ñ Agent: {welcome}\")\n",
    "        self.tts.speak(welcome)\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"Instructions:\")\n",
    "        print(\"- Press ENTER to start recording (speak your question)\")\n",
    "        print(\"- Type 'quit' or say 'goodbye' to exit\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        while True:\n",
    "            # Wait for user to press enter or type quit\n",
    "            user_input = input(\"\\n[Press ENTER to speak, or type 'quit' to exit]: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "                goodbye = \"Goodbye! Have a great day!\"\n",
    "                print(f\"\\nü§ñ Agent: {goodbye}\")\n",
    "                self.tts.speak(goodbye)\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                # Record and transcribe question\n",
    "                question = self.stt.listen(duration=recording_duration)\n",
    "                \n",
    "                # Check if user said goodbye\n",
    "                if any(word in question.lower() for word in ['goodbye', 'quit', 'exit', 'bye']):\n",
    "                    goodbye = \"Goodbye! Have a great day!\"\n",
    "                    print(f\"\\nü§ñ Agent: {goodbye}\")\n",
    "                    self.tts.speak(goodbye)\n",
    "                    break\n",
    "                \n",
    "                if not question or len(question.strip()) < 3:\n",
    "                    prompt = \"I didn't catch that. Please try again.\"\n",
    "                    print(f\"\\nü§ñ Agent: {prompt}\")\n",
    "                    self.tts.speak(prompt)\n",
    "                    continue\n",
    "                \n",
    "                # Get answer from RAG agent\n",
    "                print(f\"\\nüé§ HR: {question}\")\n",
    "                print(\"\\nü§ñ Agent: \", end=\"\", flush=True)\n",
    "                \n",
    "                result = self.ask(question)\n",
    "                answer = result[\"answer\"]\n",
    "                \n",
    "                print(answer)\n",
    "                \n",
    "                # Speak the answer\n",
    "                self.tts.speak(answer)\n",
    "                \n",
    "                # Show sources\n",
    "                if result[\"sources\"]:\n",
    "                    print(f\"\\nüìö Sources: {len(result['sources'])} document(s) retrieved\")\n",
    "            \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\n‚ö†Ô∏è Interrupted by user\")\n",
    "                goodbye = \"Session ended. Goodbye!\"\n",
    "                print(f\"\\nü§ñ Agent: {goodbye}\")\n",
    "                self.tts.speak(goodbye)\n",
    "                break\n",
    "            \n",
    "            except Exception as e:\n",
    "                error_msg = f\"An error occurred: {str(e)}\"\n",
    "                print(f\"\\n‚ùå {error_msg}\")\n",
    "                self.tts.speak(\"I encountered an error. Please try again.\")\n",
    "    \n",
    "    def ask_with_voice(self, question_text=None, recording_duration=5):\n",
    "        \"\"\"\n",
    "        Ask a single question with voice (either speak or provide text)\n",
    "        \n",
    "        Args:\n",
    "            question_text: Optional text question (if None, will record from microphone)\n",
    "            recording_duration: Recording duration if speaking\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with question, answer, and sources\n",
    "        \"\"\"\n",
    "        if question_text is None:\n",
    "            # Record question\n",
    "            print(\"\\nüé§ Speak your question...\")\n",
    "            question_text = self.stt.listen(duration=recording_duration)\n",
    "        \n",
    "        print(f\"\\nüé§ Question: {question_text}\")\n",
    "        \n",
    "        # Get answer\n",
    "        result = self.ask(question_text)\n",
    "        answer = result[\"answer\"]\n",
    "        \n",
    "        print(f\"\\nü§ñ Agent: {answer}\")\n",
    "        \n",
    "        # Speak answer\n",
    "        self.tts.speak(answer)\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"‚úÖ VoiceEnabledAgent class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d710c17",
   "metadata": {},
   "source": [
    "### 11. Initialize Voice Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eaff591f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéôÔ∏è Initializing voice-enabled agent...\n",
      "‚è≥ This may take a moment (loading Whisper model)...\n",
      "\n",
      "‚úÖ Connected to ChromaDB: cv_sections\n",
      "‚úÖ QA Chain initialized\n",
      "\n",
      "üéôÔ∏è Initializing voice capabilities...\n",
      "Loading Whisper model: base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139M/139M [00:11<00:00, 13.0MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Whisper base model loaded\n",
      "‚úÖ TTS engine initialized\n",
      "‚úÖ Voice-enabled agent ready!\n",
      "\n",
      "‚úÖ Voice agent ready! You can now have voice conversations.\n"
     ]
    }
   ],
   "source": [
    "# Initialize voice-enabled agent\n",
    "# Uses the same context from earlier\n",
    "\n",
    "if not sample_cvs or not sample_jds:\n",
    "    print(\"‚ùå No CVs or JDs found. Please run the earlier cells first.\")\n",
    "else:\n",
    "    print(\"üéôÔ∏è Initializing voice-enabled agent...\")\n",
    "    print(\"‚è≥ This may take a moment (loading Whisper model)...\\n\")\n",
    "    \n",
    "    # Create voice-enabled agent\n",
    "    # You can use 'tiny' for faster but less accurate, 'base' for balanced, 'small'/'medium' for better accuracy\n",
    "    voice_agent = VoiceEnabledAgent(\n",
    "        context=context, \n",
    "        company_name=company_name,\n",
    "        whisper_model=\"base\",  # Options: 'tiny', 'base', 'small', 'medium', 'large'\n",
    "        speech_rate=150  # Adjust speech speed (100-200 recommended)\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Voice agent ready! You can now have voice conversations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b43128",
   "metadata": {},
   "source": [
    "### 12. Start Voice Chat üé§\n",
    "\n",
    "Run this cell to start the voice conversation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb55e3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üé§ VOICE-ENABLED CV SCORING EXPLAINABILITY AGENT\n",
      "============================================================\n",
      "\n",
      "Candidate: Enoch Kwadwo Aidoo (CV ID: 2e538000bef0ba2c6bfd10f0fb99b0d97843da9e35f46b255c59141bc3660484)\n",
      "Job Title: Data Analyst (JD ID: 22c009485a6d8f139582719426054c126f7f8b426351dbfb5681cddb42ae180d)\n",
      "Company: N/A\n",
      "\n",
      "ü§ñ Agent: Hello! I'm your AI assistant. I can answer questions about Enoch Kwadwo Aidoo's application for the Data Analyst position.\n",
      "üîä Speaking: \"Hello! I'm your AI assistant. I can answer questio...\"\n",
      "\n",
      "------------------------------------------------------------\n",
      "Instructions:\n",
      "- Press ENTER to start recording (speak your question)\n",
      "- Type 'quit' or say 'goodbye' to exit\n",
      "------------------------------------------------------------\n",
      "\n",
      "üé§ Recording... (speak now, max 30s)\n",
      "‚úÖ Recording complete\n",
      "üîÑ Transcribing...\n",
      "üìù Transcribed: \"Halt, based on the save you like and you will see about it.\"\n",
      "\n",
      "üé§ HR: Halt, based on the save you like and you will see about it.\n",
      "\n",
      "ü§ñ Agent: \n",
      "\n",
      "‚ö†Ô∏è Interrupted by user\n",
      "\n",
      "ü§ñ Agent: Session ended. Goodbye!\n",
      "üîä Speaking: \"Session ended. Goodbye!...\"\n"
     ]
    }
   ],
   "source": [
    "# Start voice chat\n",
    "# Press ENTER to record your question, speak, then the agent will respond with voice\n",
    "\n",
    "voice_agent.voice_chat(recording_duration=30)  # Adjust recording_duration if needed (3-10 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b0d887",
   "metadata": {},
   "source": [
    "### 13. Single Voice Question (Alternative)\n",
    "\n",
    "Test with a single voice question without the full chat loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0890130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé§ Speak your question...\n",
      "\n",
      "üé§ Recording... (speak now, max 10s)\n",
      "‚úÖ Recording complete\n",
      "üîÑ Transcribing...\n",
      "üìù Transcribed: \"Hi, can you hear me?\"\n",
      "\n",
      "üé§ Question: Hi, can you hear me?\n",
      "\n",
      "ü§ñ Agent: I'm ready to help. Please go ahead and ask your question about CV scoring decisions. I'll provide a clear and transparent response based on the context information provided.\n",
      "üîä Speaking: \"I'm ready to help. Please go ahead and ask your qu...\"\n"
     ]
    }
   ],
   "source": [
    "# Ask a single question with voice\n",
    "# Option 1: Speak the question\n",
    "result = voice_agent.ask_with_voice(recording_duration=10)\n",
    "\n",
    "# Option 2: Provide text but get voice response\n",
    "# result = voice_agent.ask_with_voice(question_text=\"What are the candidate's main skills?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10334d0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Voice Agent Complete!\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "1. **üé§ Press ENTER** ‚Üí Start recording\n",
    "2. **üó£Ô∏è Speak your question** ‚Üí e.g., \"What are the candidate's technical skills?\"\n",
    "3. **üîÑ Agent processes** ‚Üí Transcribes ‚Üí RAG retrieval ‚Üí LLM generates answer\n",
    "4. **üîä Agent responds** ‚Üí Both text and voice output\n",
    "\n",
    "### Features Implemented:\n",
    "\n",
    "‚úÖ **Speech-to-Text (STT)** - Whisper for transcription  \n",
    "‚úÖ **Text-to-Speech (TTS)** - pyttsx3 for voice output  \n",
    "‚úÖ **RAG Integration** - ChromaDB + Ollama llama3.2  \n",
    "‚úÖ **Context-Aware** - Knows CV, JD, and scoring details  \n",
    "‚úÖ **Interactive Loop** - Continuous conversation  \n",
    "‚úÖ **Error Handling** - Graceful fallbacks  \n",
    "\n",
    "### Tips for Best Results:\n",
    "\n",
    "- **Speak clearly** and at a moderate pace\n",
    "- **Use a good microphone** for better transcription\n",
    "- **Adjust `recording_duration`** (3-10s) based on question length\n",
    "- **Adjust `whisper_model`**:\n",
    "  - `tiny` = fastest, least accurate\n",
    "  - `base` = balanced (recommended)\n",
    "  - `small/medium` = slower, more accurate\n",
    "- **Adjust `speech_rate`** (100-200) for TTS speed\n",
    "\n",
    "### Troubleshooting:\n",
    "\n",
    "- **\"Recording not working\"** ‚Üí Check microphone permissions\n",
    "- **\"Whisper slow\"** ‚Üí Use `tiny` model or upgrade GPU\n",
    "- **\"TTS not speaking\"** ‚Üí Check speaker/audio output\n",
    "- **\"Can't hear\"** ‚Üí Increase TTS volume in TextToSpeech init\n",
    "\n",
    "---\n",
    "\n",
    "## Next: Phase 4 - Voice Activity Detection (VAD)\n",
    "\n",
    "To make it truly seamless, we can add VAD to automatically detect when HR starts/stops speaking (no need to press ENTER). This requires:\n",
    "- Silero VAD or WebRTC VAD\n",
    "- Real-time audio stream processing\n",
    "- Automatic silence detection\n",
    "\n",
    "Would you like me to implement Phase 4? üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760cb8b1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 4: Voice Activity Detection (VAD)\n",
    "\n",
    "Automatic speech detection - no need to press ENTER!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25b1a31",
   "metadata": {},
   "source": [
    "### 14. VAD Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "baec2db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ VAD dependencies loaded\n"
     ]
    }
   ],
   "source": [
    "# Install VAD if needed (already installed: silero-vad)\n",
    "# !pip install torch torchaudio\n",
    "\n",
    "import torch\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "print(\"‚úÖ VAD dependencies loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379cd98c",
   "metadata": {},
   "source": [
    "### 15. Voice Activity Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b7983a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ VoiceActivityDetector class defined\n"
     ]
    }
   ],
   "source": [
    "class VoiceActivityDetector:\n",
    "    \"\"\"\n",
    "    Voice Activity Detection using Silero VAD\n",
    "    Automatically detects when user starts and stops speaking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate=16000, threshold=0.5, min_silence_duration=0.5):\n",
    "        \"\"\"\n",
    "        Initialize VAD with Silero model\n",
    "        \n",
    "        Args:\n",
    "            sample_rate: Audio sample rate (must be 8000 or 16000)\n",
    "            threshold: Voice probability threshold (0.0-1.0)\n",
    "            min_silence_duration: Minimum silence duration to stop recording (seconds)\n",
    "        \"\"\"\n",
    "        print(\"Loading Silero VAD model...\")\n",
    "        self.sample_rate = sample_rate\n",
    "        self.threshold = threshold\n",
    "        self.min_silence_duration = min_silence_duration\n",
    "        \n",
    "        # Silero VAD expects exactly 512 samples for 16kHz or 256 for 8kHz\n",
    "        self.vad_chunk_size = 512 if sample_rate == 16000 else 256\n",
    "        self.chunk_duration = self.vad_chunk_size / sample_rate  # ~0.032s for 16kHz\n",
    "        \n",
    "        # Load Silero VAD model\n",
    "        self.model, utils = torch.hub.load(\n",
    "            repo_or_dir='snakers4/silero-vad',\n",
    "            model='silero_vad',\n",
    "            force_reload=False,\n",
    "            onnx=False\n",
    "        )\n",
    "        \n",
    "        self.get_speech_timestamps = utils[0]\n",
    "        print(f\"‚úÖ Silero VAD loaded (chunk size: {self.vad_chunk_size} samples)\")\n",
    "    \n",
    "    def is_speech(self, audio_chunk):\n",
    "        \"\"\"\n",
    "        Check if audio chunk contains speech\n",
    "        \n",
    "        Args:\n",
    "            audio_chunk: numpy array of audio data (must be exactly 512 samples for 16kHz)\n",
    "        \n",
    "        Returns:\n",
    "            Boolean indicating if speech is detected\n",
    "        \"\"\"\n",
    "        # Ensure correct chunk size\n",
    "        if len(audio_chunk) != self.vad_chunk_size:\n",
    "            # Pad or truncate to correct size\n",
    "            if len(audio_chunk) < self.vad_chunk_size:\n",
    "                audio_chunk = np.pad(audio_chunk, (0, self.vad_chunk_size - len(audio_chunk)))\n",
    "            else:\n",
    "                audio_chunk = audio_chunk[:self.vad_chunk_size]\n",
    "        \n",
    "        # Convert to torch tensor\n",
    "        audio_tensor = torch.from_numpy(audio_chunk).float()\n",
    "        \n",
    "        # Get speech probability\n",
    "        speech_prob = self.model(audio_tensor, self.sample_rate).item()\n",
    "        \n",
    "        return speech_prob > self.threshold\n",
    "    \n",
    "    def record_until_silence(self, max_duration=30, manual_stop=True):\n",
    "        \"\"\"\n",
    "        Record audio until silence is detected OR user presses ENTER\n",
    "        Automatically starts when speech is detected, stops after silence or manual interrupt\n",
    "        \n",
    "        Args:\n",
    "            max_duration: Maximum recording duration (seconds)\n",
    "            manual_stop: If True, allow user to press ENTER to stop recording\n",
    "        \n",
    "        Returns:\n",
    "            numpy array of recorded audio\n",
    "        \"\"\"\n",
    "        import msvcrt  # For Windows keyboard input (use 'select' on Linux/Mac)\n",
    "        import sys\n",
    "        \n",
    "        print(\"\\nüé§ Listening... (speak when ready)\")\n",
    "        if manual_stop:\n",
    "            print(\"üí° Press ENTER at any time to stop recording\")\n",
    "        \n",
    "        # Use larger chunks for recording (e.g., 0.1s = 1600 samples)\n",
    "        # Then process in VAD-sized chunks (512 samples)\n",
    "        recording_chunk_duration = 0.1  # 100ms recording chunks\n",
    "        recording_chunk_size = int(recording_chunk_duration * self.sample_rate)\n",
    "        max_chunks = int(max_duration / recording_chunk_duration)\n",
    "        \n",
    "        audio_buffer = []\n",
    "        speech_started = False\n",
    "        silence_duration = 0.0\n",
    "        manual_stopped = False\n",
    "        \n",
    "        for i in range(max_chunks):\n",
    "            # Check for keyboard input (ENTER key)\n",
    "            if manual_stop and msvcrt.kbhit():\n",
    "                key = msvcrt.getch()\n",
    "                if key in [b'\\r', b'\\n', b' ']:  # ENTER or SPACE\n",
    "                    if speech_started:\n",
    "                        print(\"‚èπÔ∏è Manual stop - recording ended\")\n",
    "                        manual_stopped = True\n",
    "                        break\n",
    "                    else:\n",
    "                        # Clear the buffer and continue listening\n",
    "                        while msvcrt.kbhit():\n",
    "                            msvcrt.getch()\n",
    "            \n",
    "            # Record chunk\n",
    "            chunk = sd.rec(\n",
    "                recording_chunk_size,\n",
    "                samplerate=self.sample_rate,\n",
    "                channels=1,\n",
    "                dtype='float32'\n",
    "            )\n",
    "            sd.wait()\n",
    "            chunk = chunk.flatten()\n",
    "            \n",
    "            # Process chunk in VAD-sized sub-chunks\n",
    "            has_speech_in_chunk = False\n",
    "            for j in range(0, len(chunk), self.vad_chunk_size):\n",
    "                if j + self.vad_chunk_size <= len(chunk):\n",
    "                    vad_chunk = chunk[j:j + self.vad_chunk_size]\n",
    "                    if self.is_speech(vad_chunk):\n",
    "                        has_speech_in_chunk = True\n",
    "                        break\n",
    "            \n",
    "            if has_speech_in_chunk:\n",
    "                if not speech_started:\n",
    "                    print(\"üó£Ô∏è Speech detected! Recording...\")\n",
    "                    if manual_stop:\n",
    "                        print(\"   (Press ENTER when done speaking)\")\n",
    "                    speech_started = True\n",
    "                silence_duration = 0.0\n",
    "                audio_buffer.append(chunk)\n",
    "            elif speech_started:\n",
    "                # Speech was happening, now silence\n",
    "                silence_duration += recording_chunk_duration\n",
    "                audio_buffer.append(chunk)\n",
    "                \n",
    "                if silence_duration >= self.min_silence_duration:\n",
    "                    print(f\"‚úÖ Silence detected ({silence_duration:.1f}s). Recording stopped.\")\n",
    "                    break\n",
    "        \n",
    "        if not speech_started:\n",
    "            print(\"‚ö†Ô∏è No speech detected\")\n",
    "            return None\n",
    "        \n",
    "        # Concatenate all chunks\n",
    "        audio = np.concatenate(audio_buffer)\n",
    "        return audio\n",
    "\n",
    "print(\"‚úÖ VoiceActivityDetector class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6ae3f6",
   "metadata": {},
   "source": [
    "### 16. Hands-Free Voice Agent (with VAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b1c1aa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ HandsFreeVoiceAgent class defined\n"
     ]
    }
   ],
   "source": [
    "class HandsFreeVoiceAgent(VoiceEnabledAgent):\n",
    "    \"\"\"\n",
    "    Hands-free voice agent with automatic speech detection\n",
    "    Just speak - no need to press ENTER!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, context: CVScoringContext, company_name: str = None,\n",
    "                 whisper_model=\"base\", speech_rate=150, vad_threshold=0.5):\n",
    "        \"\"\"\n",
    "        Initialize hands-free agent with VAD\n",
    "        \n",
    "        Args:\n",
    "            context: CVScoringContext with CV and JD data\n",
    "            company_name: Optional company name\n",
    "            whisper_model: Whisper model size\n",
    "            speech_rate: TTS speech rate\n",
    "            vad_threshold: VAD sensitivity (0.3-0.7, lower=more sensitive)\n",
    "        \"\"\"\n",
    "        # Initialize parent voice agent\n",
    "        super().__init__(context, company_name, whisper_model, speech_rate)\n",
    "        \n",
    "        # Initialize VAD\n",
    "        print(\"\\nüéôÔ∏è Initializing Voice Activity Detection...\")\n",
    "        self.vad = VoiceActivityDetector(\n",
    "            sample_rate=self.stt.sample_rate,\n",
    "            threshold=vad_threshold,\n",
    "            min_silence_duration=0.8  # Stop after 0.8s of silence\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Hands-free agent ready!\")\n",
    "    \n",
    "    def hands_free_chat(self, max_question_duration=30):\n",
    "        \"\"\"\n",
    "        Fully hands-free voice conversation\n",
    "        Just speak when ready, agent automatically detects speech\n",
    "        \n",
    "        Args:\n",
    "            max_question_duration: Maximum duration for each question (seconds)\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üé§ HANDS-FREE VOICE AGENT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        summary = self.context.get_summary()\n",
    "        print(f\"\\nCandidate: {summary['candidate_name']}\")\n",
    "        print(f\"Job Title: {summary['job_title']}\")\n",
    "        if summary['company']:\n",
    "            print(f\"Company: {summary['company']}\")\n",
    "        \n",
    "        # Welcome message\n",
    "        welcome = f\"Hello! I'm ready to answer questions about {summary['candidate_name']}'s application. Just start speaking when you're ready!\"\n",
    "        print(f\"\\nü§ñ Agent: {welcome}\")\n",
    "        self.tts.speak(welcome)\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"Instructions:\")\n",
    "        print(\"- üé§ Just start speaking (automatic detection)\")\n",
    "        print(\"- Agent stops recording after silence\")\n",
    "        print(\"- Say 'goodbye' or 'exit' to end session\")\n",
    "        print(\"- Press Ctrl+C to force quit\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        conversation_count = 0\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                print(f\"\\n[Question #{conversation_count + 1}]\")\n",
    "                \n",
    "                # Automatically record when speech is detected\n",
    "                audio = self.vad.record_until_silence(max_duration=max_question_duration)\n",
    "                \n",
    "                if audio is None:\n",
    "                    print(\"‚ö†Ô∏è No speech detected. Listening again...\")\n",
    "                    continue\n",
    "                \n",
    "                # Transcribe\n",
    "                print(\"üîÑ Transcribing...\")\n",
    "                question = self.stt.transcribe_audio(audio)\n",
    "                \n",
    "                # Check for exit commands\n",
    "                if any(word in question.lower() for word in ['goodbye', 'quit', 'exit', 'bye', 'stop']):\n",
    "                    goodbye = \"Goodbye! Have a great day!\"\n",
    "                    print(f\"\\nü§ñ Agent: {goodbye}\")\n",
    "                    self.tts.speak(goodbye)\n",
    "                    break\n",
    "                \n",
    "                if not question or len(question.strip()) < 3:\n",
    "                    prompt = \"I didn't catch that clearly. Please try again.\"\n",
    "                    print(f\"\\nü§ñ Agent: {prompt}\")\n",
    "                    self.tts.speak(prompt)\n",
    "                    continue\n",
    "                \n",
    "                # Display question\n",
    "                print(f\"\\nüé§ HR: {question}\")\n",
    "                \n",
    "                # Get answer from RAG\n",
    "                print(\"ü§ñ Agent: \", end=\"\", flush=True)\n",
    "                result = self.ask(question)\n",
    "                answer = result[\"answer\"]\n",
    "                \n",
    "                print(answer)\n",
    "                \n",
    "                # Speak answer\n",
    "                self.tts.speak(answer)\n",
    "                \n",
    "                # Show metadata\n",
    "                if result[\"sources\"]:\n",
    "                    print(f\"\\nüìö Retrieved {len(result['sources'])} source(s)\")\n",
    "                \n",
    "                conversation_count += 1\n",
    "                \n",
    "                # Brief pause before next question\n",
    "                time.sleep(0.5)\n",
    "                print(\"\\n\" + \"-\"*60)\n",
    "                print(\"Ready for next question...\")\n",
    "            \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\n‚ö†Ô∏è Session interrupted\")\n",
    "                goodbye = \"Session ended. Goodbye!\"\n",
    "                print(f\"\\nü§ñ Agent: {goodbye}\")\n",
    "                self.tts.speak(goodbye)\n",
    "                break\n",
    "            \n",
    "            except Exception as e:\n",
    "                error_msg = f\"An error occurred: {str(e)}\"\n",
    "                print(f\"\\n‚ùå {error_msg}\")\n",
    "                self.tts.speak(\"I encountered an error. Please try again.\")\n",
    "                continue\n",
    "\n",
    "print(\"‚úÖ HandsFreeVoiceAgent class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b81ec6b",
   "metadata": {},
   "source": [
    "### 17. Initialize Hands-Free Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20c5a4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Hands-Free Voice Agent...\n",
      "CV: 2e538000bef0ba2c6bfd10f0fb99b0d97843da9e35f46b255c59141bc3660484\n",
      "JD: 22c009485a6d8f139582719426054c126f7f8b426351dbfb5681cddb42ae180d\n",
      "‚úÖ Connected to ChromaDB: cv_sections\n",
      "‚úÖ QA Chain initialized\n",
      "\n",
      "üéôÔ∏è Initializing voice capabilities...\n",
      "Loading Whisper model: base...\n",
      "‚úÖ Whisper base model loaded\n",
      "‚úÖ TTS engine initialized\n",
      "‚úÖ Voice-enabled agent ready!\n",
      "\n",
      "üéôÔ∏è Initializing Voice Activity Detection...\n",
      "Loading Silero VAD model...\n",
      "‚úÖ Silero VAD loaded (chunk size: 512 samples)\n",
      "‚úÖ Hands-free agent ready!\n",
      "\n",
      "‚úÖ Hands-Free Voice Agent ready!\n",
      "üìå No manual triggers needed - the agent will automatically detect when you speak\n",
      "üìå Just speak naturally and the agent will respond\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Enoch/.cache\\torch\\hub\\snakers4_silero-vad_master\n"
     ]
    }
   ],
   "source": [
    "# Initialize the hands-free voice agent with existing context\n",
    "print(\"Initializing Hands-Free Voice Agent...\")\n",
    "print(f\"CV: {context.cv_id}\")\n",
    "print(f\"JD: {context.jd_id}\")\n",
    "\n",
    "# Create hands-free agent with moderate VAD sensitivity\n",
    "# Lower threshold (0.3-0.4) = more sensitive (picks up softer speech, may trigger on noise)\n",
    "# Higher threshold (0.6-0.7) = less sensitive (requires clearer speech, may miss soft words)\n",
    "# Recommended: 0.5 for balanced performance\n",
    "handsfree_agent = HandsFreeVoiceAgent(\n",
    "    context=context,\n",
    "    company_name=company_name,\n",
    "    whisper_model=\"base\",\n",
    "    speech_rate=150,\n",
    "    vad_threshold=0.5\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Hands-Free Voice Agent ready!\")\n",
    "print(\"üìå No manual triggers needed - the agent will automatically detect when you speak\")\n",
    "print(\"üìå Just speak naturally and the agent will respond\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e018f8c6",
   "metadata": {},
   "source": [
    "### Quick VAD Test\n",
    "\n",
    "Let's test the VAD in isolation to ensure it works correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "00d0a60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing VAD setup...\n",
      "Loading Silero VAD model...\n",
      "‚úÖ Silero VAD loaded (chunk size: 512 samples)\n",
      "Test chunk size: 512 samples\n",
      "‚úÖ VAD test passed! Speech detected: False\n",
      "VAD chunk size: 512\n",
      "Recording chunk size: 1600 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Enoch/.cache\\torch\\hub\\snakers4_silero-vad_master\n"
     ]
    }
   ],
   "source": [
    "# Quick test of VAD functionality\n",
    "print(\"Testing VAD setup...\")\n",
    "\n",
    "# Create test VAD instance\n",
    "test_vad = VoiceActivityDetector(sample_rate=16000, threshold=0.5, min_silence_duration=0.8)\n",
    "\n",
    "# Create a test audio chunk of the correct size\n",
    "test_chunk = np.random.randn(512).astype('float32')\n",
    "print(f\"Test chunk size: {len(test_chunk)} samples\")\n",
    "\n",
    "# Test is_speech function\n",
    "try:\n",
    "    result = test_vad.is_speech(test_chunk)\n",
    "    print(f\"‚úÖ VAD test passed! Speech detected: {result}\")\n",
    "    print(f\"VAD chunk size: {test_vad.vad_chunk_size}\")\n",
    "    print(f\"Recording chunk size: {int(0.1 * test_vad.sample_rate)} samples\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå VAD test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c561ae81",
   "metadata": {},
   "source": [
    "### 18. Start Hands-Free Conversation\n",
    "\n",
    "**üé§ Usage Instructions:**\n",
    "- Just run the cell below and start speaking when ready\n",
    "- No need to press any keys - the agent automatically detects your voice\n",
    "- Speak naturally and wait for the agent to respond\n",
    "- Say \"goodbye\", \"quit\", or \"exit\" to end the conversation\n",
    "- Press Ctrl+C to force quit if needed\n",
    "\n",
    "**‚öôÔ∏è VAD Threshold Tuning:**\n",
    "- Current: 0.5 (balanced)\n",
    "- If agent doesn't detect your voice: Lower to 0.3-0.4\n",
    "- If agent triggers on background noise: Raise to 0.6-0.7\n",
    "- Adjust in the initialization cell above and re-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9cb6a5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üé§ HANDS-FREE VOICE AGENT\n",
      "============================================================\n",
      "\n",
      "Candidate: Enoch Kwadwo Aidoo\n",
      "Job Title: Data Analyst\n",
      "Company: N/A\n",
      "\n",
      "ü§ñ Agent: Hello! I'm ready to answer questions about Enoch Kwadwo Aidoo's application. Just start speaking when you're ready!\n",
      "üîä Speaking: \"Hello! I'm ready to answer questions about Enoch K...\"\n",
      "\n",
      "------------------------------------------------------------\n",
      "Instructions:\n",
      "- üé§ Just start speaking (automatic detection)\n",
      "- Agent stops recording after silence\n",
      "- Say 'goodbye' or 'exit' to end session\n",
      "- Press Ctrl+C to force quit\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Question #1]\n",
      "\n",
      "üé§ Listening... (speak when ready)\n",
      "üí° Press ENTER at any time to stop recording\n",
      "üó£Ô∏è Speech detected! Recording...\n",
      "   (Press ENTER when done speaking)\n",
      "üó£Ô∏è Speech detected! Recording...\n",
      "   (Press ENTER when done speaking)\n",
      "‚úÖ Silence detected (0.9s). Recording stopped.\n",
      "üîÑ Transcribing...\n",
      "üîÑ Transcribing...\n",
      "‚úÖ Silence detected (0.9s). Recording stopped.\n",
      "üîÑ Transcribing...\n",
      "üîÑ Transcribing...\n",
      "üìù Transcribed: \"Hello, hello. Hola. Hola. Hola.\"\n",
      "\n",
      "üé§ HR: Hello, hello. Hola. Hola. Hola.\n",
      "ü§ñ Agent: üìù Transcribed: \"Hello, hello. Hola. Hola. Hola.\"\n",
      "\n",
      "üé§ HR: Hello, hello. Hola. Hola. Hola.\n",
      "ü§ñ Agent: Hello! It seems like there's been a bit of a language mix-up at the start. Don't worry, I'm here to help clarify your CV scoring decisions.\n",
      "\n",
      "To get started, could you please provide me with more context or details about the candidate's CV and job description? Which specific aspects of their application are you unsure about? Are there any particular skills, qualifications, or experiences that you'd like to discuss?\n",
      "\n",
      "Additionally, what kind of scoring system are you using (e.g., numerical scores, criteria-based evaluation)? The more information you provide, the better I'll be able to assist you in making informed decisions about candidate selection.\n",
      "\n",
      "Let's get started!\n",
      "üîä Speaking: \"Hello! It seems like there's been a bit of a langu...\"\n",
      "Hello! It seems like there's been a bit of a language mix-up at the start. Don't worry, I'm here to help clarify your CV scoring decisions.\n",
      "\n",
      "To get started, could you please provide me with more context or details about the candidate's CV and job description? Which specific aspects of their application are you unsure about? Are there any particular skills, qualifications, or experiences that you'd like to discuss?\n",
      "\n",
      "Additionally, what kind of scoring system are you using (e.g., numerical scores, criteria-based evaluation)? The more information you provide, the better I'll be able to assist you in making informed decisions about candidate selection.\n",
      "\n",
      "Let's get started!\n",
      "üîä Speaking: \"Hello! It seems like there's been a bit of a langu...\"\n",
      "\n",
      "------------------------------------------------------------\n",
      "Ready for next question...\n",
      "\n",
      "[Question #2]\n",
      "\n",
      "üé§ Listening... (speak when ready)\n",
      "üí° Press ENTER at any time to stop recording\n",
      "\n",
      "------------------------------------------------------------\n",
      "Ready for next question...\n",
      "\n",
      "[Question #2]\n",
      "\n",
      "üé§ Listening... (speak when ready)\n",
      "üí° Press ENTER at any time to stop recording\n",
      "üó£Ô∏è Speech detected! Recording...\n",
      "   (Press ENTER when done speaking)\n",
      "üó£Ô∏è Speech detected! Recording...\n",
      "   (Press ENTER when done speaking)\n",
      "‚úÖ Silence detected (0.9s). Recording stopped.\n",
      "üîÑ Transcribing...\n",
      "üîÑ Transcribing...\n",
      "‚úÖ Silence detected (0.9s). Recording stopped.\n",
      "üîÑ Transcribing...\n",
      "üîÑ Transcribing...\n",
      "üìù Transcribed: \"But I've really, really little grazing.\"\n",
      "\n",
      "üé§ HR: But I've really, really little grazing.\n",
      "ü§ñ Agent: üìù Transcribed: \"But I've really, really little grazing.\"\n",
      "\n",
      "üé§ HR: But I've really, really little grazing.\n",
      "ü§ñ Agent: \n",
      "\n",
      "‚ö†Ô∏è Session interrupted\n",
      "\n",
      "ü§ñ Agent: Session ended. Goodbye!\n",
      "üîä Speaking: \"Session ended. Goodbye!...\"\n",
      "\n",
      "\n",
      "‚ö†Ô∏è Session interrupted\n",
      "\n",
      "ü§ñ Agent: Session ended. Goodbye!\n",
      "üîä Speaking: \"Session ended. Goodbye!...\"\n"
     ]
    }
   ],
   "source": [
    "# Start hands-free conversation\n",
    "# The agent will automatically listen for your voice and respond\n",
    "# No manual triggers needed - just speak!\n",
    "handsfree_agent.hands_free_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eca2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
