{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7889ba96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text extraction from: ./CV_Image.png\n",
      "\n",
      "--- Extracted Text ---\n",
      "# Aidoo Enoch Kwadwo\n",
      "## Data Analyst\n",
      "\n",
      "## Personal Info\n",
      "**Phone**\n",
      "0240542834\n",
      "\n",
      "**Email**\n",
      "aidooenochkwadwo@gmail.com\n",
      "\n",
      "**Kumasi, Ghana**\n",
      "\n",
      "## Qualities\n",
      "* Curiosity\n",
      "* Problem Solving\n",
      "* System Understanding\n",
      "* Technical Skills\n",
      "* Analytical Thinking\n",
      "* Problem Solving Skills\n",
      "* Teamwork\n",
      "* Initiative and Self-motivation\n",
      "* Discipline and Resilient\n",
      "\n",
      "## Key Skills\n",
      "**Tools:** Python, R, AWS, Microsoft Excel, Google Sheets, Power BI, SQL\n",
      "**Packages/Frameworks:** NumPy, Pandas, Scikit-Learn, Matplotlib, Pytorch\n",
      "**Machine Learning:** Data Analysis, Classification Modeling, Deep Neural Networks, Regression Modelling, MLOPs, Computer Vision, Natural Language Processing, Recommendation Systems\n",
      "\n",
      "## About Me\n",
      "A Data Analyst with about two years of professional experience specialized in transforming complex datasets into strategic business solutions. I've consistently delivered actionable insights that have driven key business decisions. Combining strong analytical skills with business acumen, I excel at identifying data patterns and translating them into clear, implementable strategies that solve real-world challenges.\n",
      "\n",
      "## Education\n",
      "**Bachelor of Science in Computer Science.**\n",
      "KNUST, Ghana\n",
      "2023\n",
      "\n",
      "## Courses and Certifications\n",
      "* Coursera Crash Course on Python.\n",
      "  Oct 2021\n",
      "* AWS Machine Learning Foundation.\n",
      "  Oct 2021\n",
      "* Introduction to Deep Learning with Pytorch.\n",
      "  Sep 2022\n",
      "* Machine Learning for Trading on Udacity.\n",
      "  Offered at Georgia Tech as CS 7646.\n",
      "  May 2023\n",
      "* Machine Learning.\n",
      "  Offered by Stanford University on Coursera.\n",
      "  Present\n",
      "* Google Data Analytics Professional Certificate.\n",
      "  Present\n",
      "* ALX Data Science\n",
      "  May 2023\n",
      "* AWS Certified Cloud Practitioner\n",
      "  Aug 2025\n",
      "\n",
      "## Projects\n",
      "* Conducted advanced Excel analysis on water access datasets, applying data cleaning, formulas, statistical functions, and pivot tables. Designed visualizations (stacked bar charts, box & whisker plots) to uncover disparities across urban/rural areas and income groups, supporting evidence-based resource allocation.\n",
      "* Built a predictive model for quantitative finance using historical market data, achieving a validation MSE of 0.998. The model improved risk forecasting accuracy and provided actionable insights for financial decision-making.\n",
      "* Led SQL-based analysis of 60,000+ water infrastructure records, using advanced queries and aggregation techniques to identify critical gaps in community water access. Delivered data-driven recommendations that guided infrastructure planning and resource distribution.\n",
      "* Developed stakeholder-driven Power BI dashboards with advanced DAX calculations and multi-level drill-downs to monitor project progress, budgets, and impact. The dashboards improved transparency, performance tracking, and decision-making at national, provincial, and local levels.\n",
      "\n",
      "## Work Experience\n",
      "**Data Analytics/AI/ML Engineer**\n",
      "@Really Great Tech\n",
      "November 2023 – October 2024\n",
      "* Conducted Shapelet Analysis on trained machine learning models to interpret performance patterns and identify opportunities for optimization in an AI project.\n",
      "* Created a dynamic Google Sheets pivot table to track projects, employee assignments, and mentorship relationships for improved team visibility.\n",
      "* Analyzed patient data and developed a predictive model for heart disease diagnosis, integrated into a user-friendly web application to support clinical decision-making\n",
      "\n",
      "**Data Scientist**\n",
      "@Freelancer (ALX venturers)\n",
      "September 2024 – Present\n",
      "Led a freelance team to analyze tourist consumer behavior using advanced data analytics techniques to uncover trends and patterns and built a predictive model to forecast revenue and optimize customer pricing strategies for a tourist agency.\n",
      "----------------------\n",
      "--- Extracted Text ---\n",
      "# Aidoo Enoch Kwadwo\n",
      "## Data Analyst\n",
      "\n",
      "## Personal Info\n",
      "**Phone**\n",
      "0240542834\n",
      "\n",
      "**Email**\n",
      "aidooenochkwadwo@gmail.com\n",
      "\n",
      "**Kumasi, Ghana**\n",
      "\n",
      "## Qualities\n",
      "* Curiosity\n",
      "* Problem Solving\n",
      "* System Understanding\n",
      "* Technical Skills\n",
      "* Analytical Thinking\n",
      "* Problem Solving Skills\n",
      "* Teamwork\n",
      "* Initiative and Self-motivation\n",
      "* Discipline and Resilient\n",
      "\n",
      "## Key Skills\n",
      "**Tools:** Python, R, AWS, Microsoft Excel, Google Sheets, Power BI, SQL\n",
      "**Packages/Frameworks:** NumPy, Pandas, Scikit-Learn, Matplotlib, Pytorch\n",
      "**Machine Learning:** Data Analysis, Classification Modeling, Deep Neural Networks, Regression Modelling, MLOPs, Computer Vision, Natural Language Processing, Recommendation Systems\n",
      "\n",
      "## About Me\n",
      "A Data Analyst with about two years of professional experience specialized in transforming complex datasets into strategic business solutions. I've consistently delivered actionable insights that have driven key business decisions. Combining strong analytical skills with business acumen, I excel at identifying data patterns and translating them into clear, implementable strategies that solve real-world challenges.\n",
      "\n",
      "## Education\n",
      "**Bachelor of Science in Computer Science.**\n",
      "KNUST, Ghana\n",
      "2023\n",
      "\n",
      "## Courses and Certifications\n",
      "* Coursera Crash Course on Python.\n",
      "  Oct 2021\n",
      "* AWS Machine Learning Foundation.\n",
      "  Oct 2021\n",
      "* Introduction to Deep Learning with Pytorch.\n",
      "  Sep 2022\n",
      "* Machine Learning for Trading on Udacity.\n",
      "  Offered at Georgia Tech as CS 7646.\n",
      "  May 2023\n",
      "* Machine Learning.\n",
      "  Offered by Stanford University on Coursera.\n",
      "  Present\n",
      "* Google Data Analytics Professional Certificate.\n",
      "  Present\n",
      "* ALX Data Science\n",
      "  May 2023\n",
      "* AWS Certified Cloud Practitioner\n",
      "  Aug 2025\n",
      "\n",
      "## Projects\n",
      "* Conducted advanced Excel analysis on water access datasets, applying data cleaning, formulas, statistical functions, and pivot tables. Designed visualizations (stacked bar charts, box & whisker plots) to uncover disparities across urban/rural areas and income groups, supporting evidence-based resource allocation.\n",
      "* Built a predictive model for quantitative finance using historical market data, achieving a validation MSE of 0.998. The model improved risk forecasting accuracy and provided actionable insights for financial decision-making.\n",
      "* Led SQL-based analysis of 60,000+ water infrastructure records, using advanced queries and aggregation techniques to identify critical gaps in community water access. Delivered data-driven recommendations that guided infrastructure planning and resource distribution.\n",
      "* Developed stakeholder-driven Power BI dashboards with advanced DAX calculations and multi-level drill-downs to monitor project progress, budgets, and impact. The dashboards improved transparency, performance tracking, and decision-making at national, provincial, and local levels.\n",
      "\n",
      "## Work Experience\n",
      "**Data Analytics/AI/ML Engineer**\n",
      "@Really Great Tech\n",
      "November 2023 – October 2024\n",
      "* Conducted Shapelet Analysis on trained machine learning models to interpret performance patterns and identify opportunities for optimization in an AI project.\n",
      "* Created a dynamic Google Sheets pivot table to track projects, employee assignments, and mentorship relationships for improved team visibility.\n",
      "* Analyzed patient data and developed a predictive model for heart disease diagnosis, integrated into a user-friendly web application to support clinical decision-making\n",
      "\n",
      "**Data Scientist**\n",
      "@Freelancer (ALX venturers)\n",
      "September 2024 – Present\n",
      "Led a freelance team to analyze tourist consumer behavior using advanced data analytics techniques to uncover trends and patterns and built a predictive model to forecast revenue and optimize customer pricing strategies for a tourist agency.\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from docstrange import DocumentExtractor\n",
    "\n",
    "def extract_document_text(document_path: str):\n",
    "    \"\"\"\n",
    "    Extracts and prints the text content from a given document.\n",
    "\n",
    "    Args:\n",
    "        document_path (str): The file path to the document (e.g., 'document.pdf').\n",
    "    \"\"\"\n",
    "    print(f\"Starting text extraction from: {document_path}\\n\")\n",
    "\n",
    "    # Initialize the DocumentExtractor in local CPU mode.\n",
    "    # This ensures that all processing happens on your machine and no data\n",
    "    # is sent to a cloud API.\n",
    "    try:\n",
    "        extractor = DocumentExtractor()\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing DocumentExtractor: {e}\")\n",
    "        print(\"Please ensure you have installed the necessary dependencies.\")\n",
    "        print(\"If you are running for the first time, you may need an internet connection to download models.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # The extract() method processes the document.\n",
    "        # It handles various formats like PDF, DOCX, and images.\n",
    "        result = extractor.extract(document_path)\n",
    "        \n",
    "        # We can extract the content in various formats. Here, we'll get\n",
    "        # the cleaned, LLM-optimized Markdown text.\n",
    "        extracted_text = result.extract_markdown()\n",
    "\n",
    "        if extracted_text:\n",
    "            print(\"--- Extracted Text ---\")\n",
    "            print(extracted_text)\n",
    "            print(\"----------------------\")\n",
    "        else:\n",
    "            print(\"No text could be extracted from the document.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{document_path}' was not found.\")\n",
    "        print(\"Please check the file path and try again.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during extraction: {e}\")\n",
    "    \n",
    "    return extracted_text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # In a real-world scenario, you would replace 'your_document.pdf'\n",
    "    # with the actual path to your document.\n",
    "    # For a command-line script, you could also use sys.argv to get the path.\n",
    "    sample_document_path = './CV_Image.png'\n",
    "\n",
    " \n",
    "\n",
    "    extract_document_text(sample_document_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd6c1234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text extraction from: ./CV_Image.png\n",
      "\n",
      "--- Extracted Text ---\n",
      "# Aidoo Enoch Kwadwo\n",
      "## Data Analyst\n",
      "\n",
      "## Personal Info\n",
      "**Phone**\n",
      "0240542834\n",
      "\n",
      "**Email**\n",
      "aidooenochkwadwo@gmail.com\n",
      "\n",
      "**Kumasi, Ghana**\n",
      "\n",
      "## Qualities\n",
      "* Curiosity\n",
      "* Problem Solving\n",
      "* System Understanding\n",
      "* Technical Skills\n",
      "* Analytical Thinking\n",
      "* Problem Solving Skills\n",
      "* Teamwork\n",
      "* Initiative and Self-motivation\n",
      "* Discipline and Resilient\n",
      "\n",
      "## Key Skills\n",
      "**Tools:** Python, R, AWS, Microsoft Excel, Google Sheets, Power BI, SQL\n",
      "**Packages/Frameworks:** NumPy, Pandas, Scikit-Learn, Matplotlib, Pytorch\n",
      "**Machine Learning:** Data Analysis, Classification Modeling, Deep Neural Networks, Regression Modelling, MLOPs, Computer Vision, Natural Language Processing, Recommendation Systems\n",
      "\n",
      "## About Me\n",
      "A Data Analyst with about two years of professional experience specialized in transforming complex datasets into strategic business solutions. I've consistently delivered actionable insights that have driven key business decisions. Combining strong analytical skills with business acumen, I excel at identifying data patterns and translating them into clear, implementable strategies that solve real-world challenges.\n",
      "\n",
      "## Education\n",
      "**Bachelor of Science in Computer Science.**\n",
      "KNUST, Ghana\n",
      "2023\n",
      "\n",
      "## Courses and Certifications\n",
      "* Coursera Crash Course on Python.\n",
      "  Oct 2021\n",
      "* AWS Machine Learning Foundation.\n",
      "  Oct 2021\n",
      "* Introduction to Deep Learning with Pytorch.\n",
      "  Sep 2022\n",
      "* Machine Learning for Trading on Udacity.\n",
      "  Offered at Georgia Tech as CS 7646.\n",
      "  May 2023\n",
      "* Machine Learning.\n",
      "  Offered by Stanford University on Coursera.\n",
      "  Present\n",
      "* Google Data Analytics Professional Certificate.\n",
      "  Present\n",
      "* ALX Data Science\n",
      "  May 2023\n",
      "* AWS Certified Cloud Practitioner\n",
      "  Aug 2025\n",
      "\n",
      "## Projects\n",
      "* Conducted advanced Excel analysis on water access datasets, applying data cleaning, formulas, statistical functions, and pivot tables. Designed visualizations (stacked bar charts, box & whisker plots) to uncover disparities across urban/rural areas and income groups, supporting evidence-based resource allocation.\n",
      "* Built a predictive model for quantitative finance using historical market data, achieving a validation MSE of 0.998. The model improved risk forecasting accuracy and provided actionable insights for financial decision-making.\n",
      "* Led SQL-based analysis of 60,000+ water infrastructure records, using advanced queries and aggregation techniques to identify critical gaps in community water access. Delivered data-driven recommendations that guided infrastructure planning and resource distribution.\n",
      "* Developed stakeholder-driven Power BI dashboards with advanced DAX calculations and multi-level drill-downs to monitor project progress, budgets, and impact. The dashboards improved transparency, performance tracking, and decision-making at national, provincial, and local levels.\n",
      "\n",
      "## Work Experience\n",
      "**Data Analytics/AI/ML Engineer**\n",
      "@Really Great Tech\n",
      "November 2023 – October 2024\n",
      "* Conducted Shapelet Analysis on trained machine learning models to interpret performance patterns and identify opportunities for optimization in an AI project.\n",
      "* Created a dynamic Google Sheets pivot table to track projects, employee assignments, and mentorship relationships for improved team visibility.\n",
      "* Analyzed patient data and developed a predictive model for heart disease diagnosis, integrated into a user-friendly web application to support clinical decision-making\n",
      "\n",
      "**Data Scientist**\n",
      "@Freelancer (ALX venturers)\n",
      "September 2024 – Present\n",
      "Led a freelance team to analyze tourist consumer behavior using advanced data analytics techniques to uncover trends and patterns and built a predictive model to forecast revenue and optimize customer pricing strategies for a tourist agency.\n",
      "----------------------\n",
      "--- Extracted Text ---\n",
      "# Aidoo Enoch Kwadwo\n",
      "## Data Analyst\n",
      "\n",
      "## Personal Info\n",
      "**Phone**\n",
      "0240542834\n",
      "\n",
      "**Email**\n",
      "aidooenochkwadwo@gmail.com\n",
      "\n",
      "**Kumasi, Ghana**\n",
      "\n",
      "## Qualities\n",
      "* Curiosity\n",
      "* Problem Solving\n",
      "* System Understanding\n",
      "* Technical Skills\n",
      "* Analytical Thinking\n",
      "* Problem Solving Skills\n",
      "* Teamwork\n",
      "* Initiative and Self-motivation\n",
      "* Discipline and Resilient\n",
      "\n",
      "## Key Skills\n",
      "**Tools:** Python, R, AWS, Microsoft Excel, Google Sheets, Power BI, SQL\n",
      "**Packages/Frameworks:** NumPy, Pandas, Scikit-Learn, Matplotlib, Pytorch\n",
      "**Machine Learning:** Data Analysis, Classification Modeling, Deep Neural Networks, Regression Modelling, MLOPs, Computer Vision, Natural Language Processing, Recommendation Systems\n",
      "\n",
      "## About Me\n",
      "A Data Analyst with about two years of professional experience specialized in transforming complex datasets into strategic business solutions. I've consistently delivered actionable insights that have driven key business decisions. Combining strong analytical skills with business acumen, I excel at identifying data patterns and translating them into clear, implementable strategies that solve real-world challenges.\n",
      "\n",
      "## Education\n",
      "**Bachelor of Science in Computer Science.**\n",
      "KNUST, Ghana\n",
      "2023\n",
      "\n",
      "## Courses and Certifications\n",
      "* Coursera Crash Course on Python.\n",
      "  Oct 2021\n",
      "* AWS Machine Learning Foundation.\n",
      "  Oct 2021\n",
      "* Introduction to Deep Learning with Pytorch.\n",
      "  Sep 2022\n",
      "* Machine Learning for Trading on Udacity.\n",
      "  Offered at Georgia Tech as CS 7646.\n",
      "  May 2023\n",
      "* Machine Learning.\n",
      "  Offered by Stanford University on Coursera.\n",
      "  Present\n",
      "* Google Data Analytics Professional Certificate.\n",
      "  Present\n",
      "* ALX Data Science\n",
      "  May 2023\n",
      "* AWS Certified Cloud Practitioner\n",
      "  Aug 2025\n",
      "\n",
      "## Projects\n",
      "* Conducted advanced Excel analysis on water access datasets, applying data cleaning, formulas, statistical functions, and pivot tables. Designed visualizations (stacked bar charts, box & whisker plots) to uncover disparities across urban/rural areas and income groups, supporting evidence-based resource allocation.\n",
      "* Built a predictive model for quantitative finance using historical market data, achieving a validation MSE of 0.998. The model improved risk forecasting accuracy and provided actionable insights for financial decision-making.\n",
      "* Led SQL-based analysis of 60,000+ water infrastructure records, using advanced queries and aggregation techniques to identify critical gaps in community water access. Delivered data-driven recommendations that guided infrastructure planning and resource distribution.\n",
      "* Developed stakeholder-driven Power BI dashboards with advanced DAX calculations and multi-level drill-downs to monitor project progress, budgets, and impact. The dashboards improved transparency, performance tracking, and decision-making at national, provincial, and local levels.\n",
      "\n",
      "## Work Experience\n",
      "**Data Analytics/AI/ML Engineer**\n",
      "@Really Great Tech\n",
      "November 2023 – October 2024\n",
      "* Conducted Shapelet Analysis on trained machine learning models to interpret performance patterns and identify opportunities for optimization in an AI project.\n",
      "* Created a dynamic Google Sheets pivot table to track projects, employee assignments, and mentorship relationships for improved team visibility.\n",
      "* Analyzed patient data and developed a predictive model for heart disease diagnosis, integrated into a user-friendly web application to support clinical decision-making\n",
      "\n",
      "**Data Scientist**\n",
      "@Freelancer (ALX venturers)\n",
      "September 2024 – Present\n",
      "Led a freelance team to analyze tourist consumer behavior using advanced data analytics techniques to uncover trends and patterns and built a predictive model to forecast revenue and optimize customer pricing strategies for a tourist agency.\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "extracted_text = extract_document_text('./CV_Image.png')\n",
    "with open('extracted_text.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58372fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def structure_cv_schema(text):\n",
    "    schema = {\n",
    "        \"name\": None,\n",
    "        \"email\": None,\n",
    "        \"phone\": None,\n",
    "        \"summary\": None,\n",
    "        \"work_experience\": [],\n",
    "        \"education\": [],\n",
    "        \"skills\": [],\n",
    "        \"soft_skills\": [],\n",
    "        \"certifications\": [],\n",
    "        \"projects\": [],\n",
    "        \"languages\": None,\n",
    "        \"hobbies\": None,\n",
    "        \"other\": None\n",
    "    }\n",
    "\n",
    "    # Extract email\n",
    "    email_match = re.search(r'[\\w\\.-]+@[\\w\\.-]+', text)\n",
    "    if email_match:\n",
    "        schema['email'] = email_match.group(0)\n",
    "\n",
    "    # Extract phone (simple pattern, may need adjustment)\n",
    "    phone_match = re.search(r'(\\+?\\d[\\d\\s\\-]{7,}\\d)', text)\n",
    "    if phone_match:\n",
    "        schema['phone'] = phone_match.group(0)\n",
    "\n",
    "    # Extract name (assume first line is name if it looks like a name)\n",
    "    lines = text.splitlines()\n",
    "    if lines:\n",
    "        first_line = lines[0].strip()\n",
    "        if len(first_line.split()) >= 2 and not any(x in first_line.lower() for x in ['curriculum', 'resume', 'cv']):\n",
    "            schema['name'] = first_line\n",
    "\n",
    "    # Extract sections by keywords\n",
    "    section_patterns = {\n",
    "        'education': r'(education|academic background|qualifications)',\n",
    "        'work_experience': r'(experience|employment|work history|professional experience|work experience)',\n",
    "        'skills': r'(skills|technical skills|competencies)',\n",
    "        'summary': r'(summary|profile|about me)',\n",
    "        'certifications': r'(certifications|certificates)',\n",
    "        'projects': r'(projects|project experience)',\n",
    "        'soft_skills': r'(soft skills|personal skills|interpersonal skills)',\n",
    "        'languages': r'(languages|language proficiency)',\n",
    "        'hobbies': r'(hobbies|interests)',\n",
    "        'other': r'(other|additional information)'\n",
    "    }\n",
    "\n",
    "    # Find section indices\n",
    "    section_indices = {}\n",
    "    for key, pattern in section_patterns.items():\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            section_indices[key] = match.start()\n",
    "\n",
    "    # Sort sections by appearance\n",
    "    sorted_sections = sorted(section_indices.items(), key=lambda x: x[1])\n",
    "\n",
    "    # Extract section contents\n",
    "    for i, (section, start_idx) in enumerate(sorted_sections):\n",
    "        end_idx = sorted_sections[i+1][1] if i+1 < len(sorted_sections) else len(text)\n",
    "        content = text[start_idx:end_idx].strip()\n",
    "        if section in ['education', 'work_experience', 'certifications', 'projects', 'soft_skills']:\n",
    "            schema[section] = [line.strip() for line in content.split('\\n') if line.strip() and not re.search(section_patterns[section], line, re.IGNORECASE)]\n",
    "        elif section == 'skills':\n",
    "            skills_lines = [line.strip() for line in content.split('\\n') if line.strip() and not re.search(section_patterns['skills'], line, re.IGNORECASE)]\n",
    "            if skills_lines:\n",
    "                schema['skills'] = re.split(r',|;|\\n', ' '.join(skills_lines))\n",
    "                schema['skills'] = [s.strip() for s in schema['skills'] if s.strip()]\n",
    "        elif section == 'summary':\n",
    "            summary_lines = [line.strip() for line in content.split('\\n') if line.strip() and not re.search(section_patterns['summary'], line, re.IGNORECASE)]\n",
    "            schema['summary'] = ' '.join(summary_lines)\n",
    "        elif section in ['languages', 'hobbies', 'other']:\n",
    "            lines = [line.strip() for line in content.split('\\n') if line.strip() and not re.search(section_patterns[section], line, re.IGNORECASE)]\n",
    "            schema[section] = ', '.join(lines) if lines else None\n",
    "\n",
    "    return schema\n",
    "\n",
    "# Example usage: structure the extracted markdown or text\n",
    "# Replace 'markdown_content' with your extracted text variable\n",
    "# structured_cv = structure_cv_schema(markdown_content)\n",
    "# print(json.dumps(structured_cv, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47a3a0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_cv = structure_cv_schema(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64607f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': '# Aidoo Enoch Kwadwo',\n",
       " 'email': 'aidooenochkwadwo@gmail.com',\n",
       " 'phone': '0240542834',\n",
       " 'summary': 'A Data Analyst with about two years of',\n",
       " 'work_experience': ['##'],\n",
       " 'education': ['**Bachelor of Science in Computer Science.**',\n",
       "  'KNUST, Ghana',\n",
       "  '2023',\n",
       "  '## Courses and'],\n",
       " 'skills': ['* Analytical Thinking * Teamwork * Initiative and Self-motivation * Discipline and Resilient **Tools:** Python',\n",
       "  'R',\n",
       "  'AWS',\n",
       "  'Microsoft Excel',\n",
       "  'Google Sheets',\n",
       "  'Power BI',\n",
       "  'SQL **Packages/Frameworks:** NumPy',\n",
       "  'Pandas',\n",
       "  'Scikit-Learn',\n",
       "  'Matplotlib',\n",
       "  'Pytorch **Machine Learning:** Data Analysis',\n",
       "  'Classification Modeling',\n",
       "  'Deep Neural Networks',\n",
       "  'Regression Modelling',\n",
       "  'MLOPs',\n",
       "  'Computer Vision',\n",
       "  'Natural Language Processing',\n",
       "  'Recommendation Systems ##'],\n",
       " 'soft_skills': [],\n",
       " 'certifications': ['* Coursera Crash Course on Python.',\n",
       "  'Oct 2021',\n",
       "  '* AWS Machine Learning Foundation.',\n",
       "  'Oct 2021',\n",
       "  '* Introduction to Deep Learning with Pytorch.',\n",
       "  'Sep 2022',\n",
       "  '* Machine Learning for Trading on Udacity.',\n",
       "  'Offered at Georgia Tech as CS 7646.',\n",
       "  'May 2023',\n",
       "  '* Machine Learning.',\n",
       "  'Offered by Stanford University on Coursera.',\n",
       "  'Present',\n",
       "  '* Google Data Analytics Professional Certificate.',\n",
       "  'Present',\n",
       "  '* ALX Data Science',\n",
       "  'May 2023',\n",
       "  '* AWS Certified Cloud Practitioner',\n",
       "  'Aug 2025',\n",
       "  '##'],\n",
       " 'projects': ['* Conducted advanced Excel analysis on water access datasets, applying data cleaning, formulas, statistical functions, and pivot tables. Designed visualizations (stacked bar charts, box & whisker plots) to uncover disparities across urban/rural areas and income groups, supporting evidence-based resource allocation.',\n",
       "  '* Built a predictive model for quantitative finance using historical market data, achieving a validation MSE of 0.998. The model improved risk forecasting accuracy and provided actionable insights for financial decision-making.',\n",
       "  '* Led SQL-based analysis of 60,000+ water infrastructure records, using advanced queries and aggregation techniques to identify critical gaps in community water access. Delivered data-driven recommendations that guided infrastructure planning and resource distribution.',\n",
       "  '* Developed stakeholder-driven Power BI dashboards with advanced DAX calculations and multi-level drill-downs to monitor project progress, budgets, and impact. The dashboards improved transparency, performance tracking, and decision-making at national, provincial, and local levels.',\n",
       "  '## Work Experience',\n",
       "  '**Data Analytics/AI/ML Engineer**',\n",
       "  '@Really Great Tech',\n",
       "  'November 2023 – October 2024',\n",
       "  '* Conducted Shapelet Analysis on trained machine learning models to interpret performance patterns and identify opportunities for optimization in an AI project.',\n",
       "  '* Analyzed patient data and developed a predictive model for heart disease diagnosis, integrated into a user-friendly web application to support clinical decision-making',\n",
       "  '**Data Scientist**',\n",
       "  '@Freelancer (ALX venturers)',\n",
       "  'September 2024 – Present',\n",
       "  'Led a freelance team to analyze tourist consumer behavior using advanced data analytics techniques to uncover trends and patterns and built a predictive model to forecast revenue and optimize customer pricing strategies for a tourist agency.'],\n",
       " 'languages': None,\n",
       " 'hobbies': None,\n",
       " 'other': None}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc81a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"structured_data\": {\n",
      "    \"name\": \"Aidoo Enoch Kwadwo\",\n",
      "    \"email\": \"aidoenochkwadwo@gmail.com\",\n",
      "    \"phone\": \"0240542834\",\n",
      "    \"summary\": \"A Data Analyst with about two years of professional experience specialized in transforming complex datasets into strategic business solutions. I've consistently delivered actionable insights that have driven key business decisions. Combining strong analytical skills with business acumen, I excel at identifying data patterns and translating them into clear, implementable strategies that solve real-world challenges.\",\n",
      "    \"work_experience\": [\n",
      "      {\n",
      "        \"company\": \"Really Great Tech\",\n",
      "        \"title\": \"Data Analytics/AI/ML Engineer\",\n",
      "        \"location\": null,\n",
      "        \"start_date\": \"November 2023\",\n",
      "        \"end_date\": \"October 2024\",\n",
      "        \"responsibilities\": [\n",
      "          \"Conducted Shapelet Analysis on trained machine learning models to interpret performance patterns and identify opportunities for optimization in an AI project.\",\n",
      "          \"Created a dynamic Google Sheets pivot table to track projects, employee assignments, and mentorship relationships for improved team visibility.\",\n",
      "          \"Analyzed patient data and developed a predictive model for heart disease diagnosis, integrated into a user-friendly web application to support clinical decision-making.\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"company\": \"ALX venturers\",\n",
      "        \"title\": \"Data Scientist\",\n",
      "        \"location\": null,\n",
      "        \"start_date\": \"September 2024\",\n",
      "        \"end_date\": \"Present\",\n",
      "        \"responsibilities\": [\n",
      "          \"Led a freelance team to analyze tourist consumer behavior using advanced data analytics techniques to uncover trends and patterns and built a predictive model to forecast revenue and optimize customer pricing strategies for a tourist agency.\"\n",
      "        ]\n",
      "      }\n",
      "    ],\n",
      "    \"education\": [\n",
      "      {\n",
      "        \"degree\": \"Bachelor of Science\",\n",
      "        \"field_of_study\": \"Computer Science\",\n",
      "        \"institution\": \"KNUST\",\n",
      "        \"location\": \"Ghana\",\n",
      "        \"start_date\": null,\n",
      "        \"end_date\": \"2023\"\n",
      "      }\n",
      "    ],\n",
      "    \"skills\": [\n",
      "      \"Python\",\n",
      "      \"R\",\n",
      "      \"AWS\",\n",
      "      \"Microsoft Excel\",\n",
      "      \"Google Sheets\",\n",
      "      \"Power BI\",\n",
      "      \"SQL\",\n",
      "      \"NumPy\",\n",
      "      \"Pandas\",\n",
      "      \"Scikit-Learn\",\n",
      "      \"Matplotlib\",\n",
      "      \"Pytorch\",\n",
      "      \"Data Analysis\",\n",
      "      \"Classification Modeling\",\n",
      "      \"Deep Neural Networks\",\n",
      "      \"Regression Modelling\",\n",
      "      \"MLOps\",\n",
      "      \"Computer Vision\",\n",
      "      \"Natural Language Processing\",\n",
      "      \"Recommendation Systems\"\n",
      "    ],\n",
      "    \"soft_skills\": [\n",
      "      \"Curiosity\",\n",
      "      \"Problem Solving\",\n",
      "      \"System Understanding\",\n",
      "      \"Technical Skills\",\n",
      "      \"Analytical Thinking\",\n",
      "      \"Problem Solving Skills\",\n",
      "      \"Teamwork\",\n",
      "      \"Initiative and Self-motivation\",\n",
      "      \"Discipline and Resilient\"\n",
      "    ],\n",
      "    \"certifications\": [\n",
      "      {\n",
      "        \"name\": \"Coursera Crash Course on Python\",\n",
      "        \"issuing_organization\": \"Coursera\",\n",
      "        \"date\": \"Oct 2021\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"AWS Machine Learning Foundation\",\n",
      "        \"issuing_organization\": \"AWS\",\n",
      "        \"date\": \"Oct 2021\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Introduction to Deep Learning with Pytorch\",\n",
      "        \"issuing_organization\": null,\n",
      "        \"date\": \"Sep 2022\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Machine Learning for Trading on Udacity\",\n",
      "        \"issuing_organization\": \"Udacity\",\n",
      "        \"date\": \"May 2023\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Machine Learning\",\n",
      "        \"issuing_organization\": \"Stanford University on Coursera\",\n",
      "        \"date\": \"Present\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Google Data Analytics Professional Certificate\",\n",
      "        \"issuing_organization\": \"Google\",\n",
      "        \"date\": \"Present\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"ALX Data Science\",\n",
      "        \"issuing_organization\": \"ALX\",\n",
      "        \"date\": \"May 2023\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"AWS Certified Cloud Practitioner\",\n",
      "        \"issuing_organization\": \"AWS\",\n",
      "        \"date\": \"Aug 2025\"\n",
      "      }\n",
      "    ],\n",
      "    \"projects\": [\n",
      "      {\n",
      "        \"name\": \"Excel Analysis on Water Access\",\n",
      "        \"description\": \"Conducted advanced Excel analysis on water access datasets, applying data cleaning, formulas, statistical functions, and pivot tables. Designed visualizations (stacked bar charts, box & whisker plots) to uncover disparities across urban/rural areas and income groups, supporting evidence-based resource allocation.\",\n",
      "        \"technologies\": [\n",
      "          \"Excel\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Predictive Model for Quantitative Finance\",\n",
      "        \"description\": \"Built a predictive model for quantitative finance using historical market data, achieving a validation MSE of 0.998. The model improved risk forecasting accuracy and provided actionable insights for financial decision-making.\",\n",
      "        \"technologies\": []\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"SQL-based Water Infrastructure Analysis\",\n",
      "        \"description\": \"Led SQL-based analysis of 60,000+ water infrastructure records, using advanced queries and aggregation techniques to identify critical gaps in community water access. Delivered data-driven recommendations that guided infrastructure planning and resource distribution.\",\n",
      "        \"technologies\": [\n",
      "          \"SQL\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Power BI Dashboards Development\",\n",
      "        \"description\": \"Developed stakeholder-driven Power BI dashboards with advanced DAX calculations and multi-level drill-downs to monitor project progress, budgets, and impact. The dashboards improved transparency, performance tracking, and decision-making at national, provincial, and local levels.\",\n",
      "        \"technologies\": [\n",
      "          \"Power BI\",\n",
      "          \"DAX\"\n",
      "        ]\n",
      "      }\n",
      "    ],\n",
      "    \"languages\": null,\n",
      "    \"hobbies\": null,\n",
      "    \"other\": \"Kumasi, Ghana\"\n",
      "  },\n",
      "  \"format\": \"structured_json\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from docstrange import DocumentExtractor\n",
    "import json\n",
    "\n",
    "# Initialize the document extractor\n",
    "extractor = DocumentExtractor()\n",
    "\n",
    "# Define a sample JSON schema that represents your desired database schema\n",
    "# The schema is designed to extract a comprehensive set of details from a resume\n",
    "resume_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"type\": \"string\"},\n",
    "        \"email\": {\"type\": \"string\"},\n",
    "        \"phone\": {\"type\": \"string\"},\n",
    "        \"summary\": {\"type\": \"string\"},\n",
    "        \"work_experience\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"company\": {\"type\": \"string\"},\n",
    "                    \"title\": {\"type\": \"string\"},\n",
    "                    \"location\": {\"type\": \"string\"},\n",
    "                    \"start_date\": {\"type\": \"string\"},\n",
    "                    \"end_date\": {\"type\": \"string\"},\n",
    "                    \"responsibilities\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\"type\": \"string\"}\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"company\", \"title\"]\n",
    "            }\n",
    "        },\n",
    "        \"education\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"degree\": {\"type\": \"string\"},\n",
    "                    \"field_of_study\": {\"type\": \"string\"},\n",
    "                    \"institution\": {\"type\": \"string\"},\n",
    "                    \"location\": {\"type\": \"string\"},\n",
    "                    \"start_date\": {\"type\": \"string\"},\n",
    "                    \"end_date\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"degree\", \"institution\"]\n",
    "            }\n",
    "        },\n",
    "        \"skills\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"string\"}\n",
    "        },\n",
    "        \"soft_skills\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"string\"}\n",
    "        },\n",
    "        \"certifications\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"name\": {\"type\": \"string\"},\n",
    "                    \"issuing_organization\": {\"type\": \"string\"},\n",
    "                    \"date\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"name\"]\n",
    "            }\n",
    "        },\n",
    "        \"projects\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"name\": {\"type\": \"string\"},\n",
    "                    \"description\": {\"type\": \"string\"},\n",
    "                    \"technologies\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\"type\": \"string\"}\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"name\"]\n",
    "            }\n",
    "        },\n",
    "        \"languages\": {\"type\": \"string\"},\n",
    "        \"hobbies\": {\"type\": \"string\"},\n",
    "        \"other\": {\"type\": \"string\"}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Assume 'my_document.pdf' is a document like a resume\n",
    "# You would replace this with the path to your actual file\n",
    "document_path = './CV_Image.png'\n",
    "\n",
    "# Extract data from the document using the JSON schema\n",
    "try:\n",
    "    result = extractor.extract(document_path)\n",
    "    structured_data = result.extract_data(json_schema=resume_schema)\n",
    "\n",
    "    # Print the resulting structured JSON\n",
    "    print(json.dumps(structured_data, indent=2))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Document not found at '{document_path}'. Please provide a valid file path.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during extraction: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d73a0038",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('extracted_file.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(structured_data, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4298c5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Enoch\\.conda\\envs\\my_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "DEBUG - cv_id: 9d427c27755752a2c7b94210180e91e9\n",
      "DEBUG - cv_id: 9d427c27755752a2c7b94210180e91e9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "CV Hybrid Score: 24.15/100\n",
      "⚠️ FlagReranker error: bad operand type for unary -: 'list'\n",
      "CV Hybrid Score: 24.15/100\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from rapidfuzz import fuzz\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from FlagEmbedding import FlagReranker\n",
    "import hashlib\n",
    "\n",
    "# ✅ Load environment variables\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_Token\")\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"❌ Hugging Face API token missing. Add HF_Token=your_key to .env\")\n",
    "\n",
    "# Initialize FlagReranker\n",
    "reranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True)\n",
    "\n",
    "# --- Helpers ---\n",
    "def normalize(text):\n",
    "    return str(text).lower().strip() if text else \"\"\n",
    "\n",
    "def fuzzy_match(target, candidate_list, threshold=80):\n",
    "    if not target or not candidate_list:\n",
    "        return False\n",
    "    target_norm = normalize(target)\n",
    "    for candidate in candidate_list:\n",
    "        if fuzz.token_set_ratio(target_norm, normalize(candidate)) >= threshold:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "location_synonyms = {\n",
    "    \"us\": [\"usa\", \"united states\", \"america\"],\n",
    "    \"uk\": [\"united kingdom\", \"england\"],\n",
    "    \"remote\": [\"remote\", \"anywhere\"],\n",
    "}\n",
    "\n",
    "def location_match(required, candidate):\n",
    "    if not required or not candidate:\n",
    "        return False\n",
    "    req_norm = normalize(required)\n",
    "    cand_norm = normalize(candidate)\n",
    "    for synonym in location_synonyms.get(req_norm, [req_norm]):\n",
    "        if synonym in cand_norm:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def fuzzy_education_match(required_edu, candidate_edu_list):\n",
    "    if not required_edu or not candidate_edu_list:\n",
    "        return 100.0\n",
    "    if isinstance(required_edu, dict):\n",
    "        degree = required_edu.get(\"degree\", \"\")\n",
    "        field = required_edu.get(\"field\", \"\") or required_edu.get(\"field_of_study\", \"\")\n",
    "        required_str = f\"{degree} in {field}\".strip()\n",
    "    else:\n",
    "        required_str = str(required_edu)\n",
    "    required_norm = normalize(required_str)\n",
    "    max_ratio = 0\n",
    "    for edu in candidate_edu_list:\n",
    "        if isinstance(edu, dict):\n",
    "            degree = edu.get(\"degree\", \"\")\n",
    "            field = edu.get(\"field_of_study\", \"\")\n",
    "            candidate_str = f\"{degree} in {field}\".strip()\n",
    "        else:\n",
    "            candidate_str = str(edu)\n",
    "        ratio = fuzz.token_set_ratio(required_norm, normalize(candidate_str))\n",
    "        max_ratio = max(max_ratio, ratio)\n",
    "    return max_ratio\n",
    "\n",
    "def hash_email(email):\n",
    "    return hashlib.md5(email.lower().strip().encode()).hexdigest() if email else None\n",
    "\n",
    "# --- FlagReranker scoring ---\n",
    "def flag_rerank_score(query, passage, normalize=True):\n",
    "    try:\n",
    "        score = reranker.compute_score([query, passage])\n",
    "        if normalize:\n",
    "            import math\n",
    "            score = 1 / (1 + math.exp(-score)) * 100  # sigmoid to 0–100\n",
    "        return float(score)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ FlagReranker error: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "# --- Main scoring function ---\n",
    "def score_cv_against_job(cv_json_path, job_description_path, min_experience_years=2):\n",
    "    weights = {\n",
    "        \"hard_filters\": 0.25,\n",
    "        \"semantic_similarity\": 0.45,\n",
    "        \"experience_alignment\": 0.15,\n",
    "        \"education_alignment\": 0.15,\n",
    "    }\n",
    "\n",
    "    # ✅ Ollama embeddings (local)\n",
    "    embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "\n",
    "    # Load job description\n",
    "    try:\n",
    "        with open(job_description_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            job_text = f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Job description file not found: {job_description_path}\")\n",
    "        return 0.0\n",
    "\n",
    "    job_req = {\n",
    "        \"required_skills\": [],\n",
    "        \"required_certifications\": [],\n",
    "        \"location_eligibility\": \"\",\n",
    "        \"min_years_experience\": min_experience_years,\n",
    "        \"required_education\": \"\",\n",
    "    }\n",
    "\n",
    "    required_skills = set(normalize(s) for s in job_req.get(\"required_skills\", []) if s)\n",
    "    required_certifications = set(normalize(c) for c in job_req.get(\"required_certifications\", []) if c)\n",
    "    location_req = job_req.get(\"location_eligibility\", \"\")\n",
    "    min_years = job_req.get(\"min_years_experience\", min_experience_years) or min_experience_years\n",
    "    required_education = job_req.get(\"required_education\", \"\")\n",
    "\n",
    "    # Load CV JSON\n",
    "    try:\n",
    "        with open(cv_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cv_data = json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "        print(f\"❌ Error loading CV JSON {cv_json_path}: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "    structured_data = cv_data.get(\"CV_data\", {}).get(\"structured_data\", {})\n",
    "    years_of_experience = float(structured_data.get(\"years_of_experience\", 0.0) or 0.0)\n",
    "    candidate_skills = [normalize(s) for s in structured_data.get(\"skills\", []) + structured_data.get(\"soft_skills\", []) if s]\n",
    "    candidate_certifications = [normalize(c.get(\"name\", \"\")) for c in structured_data.get(\"certifications\", []) if isinstance(c, dict) and c.get(\"name\")]\n",
    "    candidate_location = normalize(structured_data.get(\"location\", \"\"))\n",
    "    candidate_education = structured_data.get(\"education\", [])\n",
    "\n",
    "    email = normalize(structured_data.get(\"email\", \"\"))\n",
    "    cv_id = hash_email(email) or os.path.splitext(os.path.basename(cv_json_path))[0]\n",
    "    print(f\"DEBUG - cv_id: {cv_id}\")\n",
    "\n",
    "    # --- Hard Score ---\n",
    "    hard_score, total_weight = 0.0, 0.0\n",
    "    if required_skills:\n",
    "        skill_matches = sum(1 for s in required_skills if fuzzy_match(s, candidate_skills))\n",
    "        hard_score += (skill_matches / len(required_skills)) * 0.5\n",
    "        total_weight += 0.5\n",
    "    if required_certifications:\n",
    "        cert_matches = sum(1 for c in required_certifications if fuzzy_match(c, candidate_certifications))\n",
    "        hard_score += (cert_matches / len(required_certifications)) * 0.3\n",
    "        total_weight += 0.3\n",
    "    if location_req:\n",
    "        hard_score += 0.2 if location_match(location_req, candidate_location) else 0.0\n",
    "        total_weight += 0.2\n",
    "    hard_score = (hard_score / total_weight * 100) if total_weight > 0 else 0.0\n",
    "\n",
    "    # --- Semantic Similarity with FlagReranker ---\n",
    "    try:\n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=\"./chroma_db\",\n",
    "            embedding_function=embeddings,\n",
    "            collection_name=\"cv_sections\",\n",
    "        )\n",
    "        retrieved = vectorstore.get(where={\"cv_id\": cv_id})\n",
    "        documents, metadatas = retrieved.get(\"documents\", []), retrieved.get(\"metadatas\", [])\n",
    "        if not documents:\n",
    "            print(f\"⚠️ No embeddings found for CV: {cv_json_path}\")\n",
    "            return 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error accessing Chroma: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "    section_scores = []\n",
    "    for doc_text, metadata in zip(documents, metadatas):\n",
    "        score = flag_rerank_score(job_text, doc_text, normalize=True)\n",
    "        weight = 1.5 if metadata.get(\"section\") in [\"work_experience\", \"skills\", \"summary\"] else 1.0\n",
    "        section_scores.append(score * weight)\n",
    "    semantic_score = np.mean(section_scores) if section_scores else 0.0\n",
    "\n",
    "    # --- Experience Alignment ---\n",
    "    experience_score = min(years_of_experience / min_years, 1.0) * 100 if min_years > 0 else 100.0\n",
    "\n",
    "    # --- Education Alignment ---\n",
    "    education_score = fuzzy_education_match(required_education, candidate_education)\n",
    "\n",
    "    # --- Hybrid Score ---\n",
    "    hybrid_score = (\n",
    "        hard_score * weights[\"hard_filters\"]\n",
    "        + semantic_score * weights[\"semantic_similarity\"]\n",
    "        + experience_score * weights[\"experience_alignment\"]\n",
    "        + education_score * weights[\"education_alignment\"]\n",
    "    )\n",
    "    return round(hybrid_score, 2)\n",
    "\n",
    "# --- Example usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    cv_json_path = \"./extracted_files/CV_Image.json\"\n",
    "    job_description_path = \"./job_description.txt\"\n",
    "    score = score_cv_against_job(cv_json_path, job_description_path, min_experience_years=3)\n",
    "    print(f\"CV Hybrid Score: {score}/100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21d33a",
   "metadata": {},
   "source": [
    "**Verifying Stored Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98ff5cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enoch\\AppData\\Local\\Temp\\ipykernel_4688\\80900081.py:5: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=\"./chroma_db\", collection_name=\"cv_sections\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 15 embeddings for cv_id: 9d427c27755752a2c7b94210180e91e9\n",
      "Embedding 0 (section: email, chunk: 0):\n",
      "  Shape: (1024,)\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  First few values: [-0.01467171 -0.02097356 -0.04572035  0.01120208  0.00452306]\n",
      "  Document content (first 100 chars): \"aidoenochkwadwo@gmail.com\"\n",
      "Embedding 1 (section: summary, chunk: 0):\n",
      "  Shape: (1024,)\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  First few values: [-0.00620162  0.03033508  0.00012151 -0.02254748  0.02840543]\n",
      "  Document content (first 100 chars): \"A Data Analyst with about two years of professional experience specialized in transforming complex \n",
      "Embedding 2 (section: work_experience, chunk: 0):\n",
      "  Shape: (1024,)\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  First few values: [-0.00464036 -0.01416096 -0.03717647 -0.04362679 -0.01361977]\n",
      "  Document content (first 100 chars): [{\"company\": \"Really Great Tech\", \"title\": \"Data Analytics/AI/ML Engineer\", \"location\": \"\", \"start_d\n",
      "Embedding 3 (section: work_experience, chunk: 1):\n",
      "  Shape: (1024,)\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  First few values: [ 0.01170141  0.01752633 -0.03993785 -0.02758665  0.00860736]\n",
      "  Document content (first 100 chars): for improved team visibility.\", \"Analyzed patient data and developed a predictive model for heart di\n",
      "Embedding 4 (section: work_experience, chunk: 2):\n",
      "  Shape: (1024,)\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  First few values: [-0.00856625  0.05780131 -0.04984427 -0.03900394  0.02187898]\n",
      "  Document content (first 100 chars): techniques to uncover trends and patterns and built a predictive model to forecast revenue and optim\n",
      "Embedding 5 (section: education, chunk: 0):\n",
      "  Shape: (1024,)\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  First few values: [ 0.00768564 -0.05012675 -0.00363135 -0.02004475 -0.02072769]\n",
      "  Document content (first 100 chars): [{\"degree\": \"Bachelor of Science\", \"field_of_study\": \"Computer Science\", \"institution\": \"KNUST\", \"lo\n",
      "Embedding 6 (section: skills, chunk: 0):\n",
      "  Shape: (1024,)\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  First few values: [ 0.01125404 -0.01655807 -0.01679675 -0.02947805 -0.00061313]\n",
      "  Document content (first 100 chars): [\"Python\", \"R\", \"AWS\", \"Microsoft Excel\", \"Google Sheets\", \"Power BI\", \"SQL\", \"NumPy\", \"Pandas\", \"Sc\n",
      "Embedding 7 (section: soft_skills, chunk: 0):\n",
      "  Shape: (1024,)\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  First few values: [ 0.01346583  0.00182879 -0.04772158 -0.02522753 -0.03351669]\n",
      "  Document content (first 100 chars): [\"Curiosity\", \"Problem Solving\", \"System Understanding\", \"Technical Skills\", \"Analytical Thinking\", \n",
      "Embedding 8 (section: certifications, chunk: 0):\n",
      "  Shape: (1024,)\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  First few values: [-0.00046974 -0.00619338 -0.00150695 -0.0330645  -0.02830324]\n",
      "  Document content (first 100 chars): [{\"name\": \"Coursera Crash Course on Python\", \"issuing_organization\": \"Coursera\", \"date\": \"Oct 2021\"}\n",
      "Embedding 9 (section: certifications, chunk: 1):\n",
      "  Shape: (1024,)\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  First few values: [-0.00422254  0.01032501  0.00150965  0.00598573 -0.02210389]\n",
      "  Document content (first 100 chars): \"Stanford University (via Coursera)\", \"date\": \"Present\"}, {\"name\": \"Google Data Analytics Profession\n",
      "Embedding 10 (section: projects, chunk: 0):\n",
      "  Shape: (1024,)\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  First few values: [-0.01066709  0.0028568   0.01275759 -0.02516082  0.00581131]\n",
      "  Document content (first 100 chars): [{\"name\": \"Water Access Data Analysis\", \"description\": \"Conducted advanced Excel analysis on water a\n",
      "Embedding 11 (section: projects, chunk: 1):\n",
      "  Shape: (1024,)\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  First few values: [-0.01144823  0.01293707 -0.03199129  0.00200888 -0.02408826]\n",
      "  Document content (first 100 chars): \"description\": \"Built a predictive model for quantitative finance using historical market data, achi\n",
      "Embedding 12 (section: projects, chunk: 2):\n",
      "  Shape: (1024,)\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  First few values: [-0.03171603 -0.00563349 -0.0389853   0.00572326 -0.04581907]\n",
      "  Document content (first 100 chars): gaps in community water access. Delivered data-driven recommendations that guided infrastructure pla\n",
      "Embedding 13 (section: projects, chunk: 3):\n",
      "  Shape: (1024,)\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  First few values: [-0.03032082 -0.02611982  0.0038867  -0.00084256 -0.0299408 ]\n",
      "  Document content (first 100 chars): and decision-making at national, provincial, and local levels.\", \"technologies\": [\"Power BI\", \"DAX\"]\n",
      "Embedding 14 (section: years_of_experience, chunk: 0):\n",
      "  Shape: (1024,)\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  First few values: [ 0.00865484 -0.00528088  0.00284232  0.0442119   0.00826974]\n",
      "  Document content (first 100 chars): 1.83\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "import numpy as np\n",
    "\n",
    "cv_id = \"9d427c27755752a2c7b94210180e91e9\"\n",
    "vectorstore = Chroma(persist_directory=\"./chroma_db\", collection_name=\"cv_sections\")\n",
    "retrieved = vectorstore.get(where={\"cv_id\": cv_id}, include=['embeddings', 'metadatas', 'documents'])\n",
    "\n",
    "embeddings = retrieved.get(\"embeddings\", [])\n",
    "metadatas = retrieved.get(\"metadatas\", [])\n",
    "documents = retrieved.get(\"documents\", [])\n",
    "\n",
    "print(f\"Retrieved {len(embeddings)} embeddings for cv_id: {cv_id}\")\n",
    "for i, (emb, meta, doc) in enumerate(zip(embeddings, metadatas, documents)):\n",
    "    emb_array = np.array(emb, dtype=np.float64)\n",
    "    print(f\"Embedding {i} (section: {meta['section']}, chunk: {meta['chunk_id']}):\")\n",
    "    print(f\"  Shape: {emb_array.shape}\")\n",
    "    print(f\"  Type: {type(emb)}\")\n",
    "    print(f\"  First few values: {emb_array[:5]}\")\n",
    "    print(f\"  Document content (first 100 chars): {doc[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0d6fd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - Email: aidoenochkwadwo@gmail.com\n",
      "DEBUG - cv_id: 9d427c27755752a2c7b94210180e91e9\n",
      "⚠️ Error accessing Chroma: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "CV Hybrid Score: 0.0/100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from rapidfuzz import fuzz\n",
    "import hashlib\n",
    "\n",
    "# --- Helper functions ---\n",
    "def normalize(text):\n",
    "    return text.lower().strip()\n",
    "\n",
    "def fuzzy_match(target, candidate_list, threshold=80):\n",
    "    target_norm = normalize(target)\n",
    "    for candidate in candidate_list:\n",
    "        candidate_norm = normalize(candidate)\n",
    "        if fuzz.token_set_ratio(target_norm, candidate_norm) >= threshold:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "location_synonyms = {\n",
    "    \"us\": [\"usa\", \"united states\", \"america\"],\n",
    "    \"uk\": [\"united kingdom\", \"england\"],\n",
    "    \"remote\": [\"remote\", \"anywhere\"]\n",
    "}\n",
    "\n",
    "def location_match(required, candidate):\n",
    "    req_norm = normalize(required)\n",
    "    cand_norm = normalize(candidate)\n",
    "    for synonym in location_synonyms.get(req_norm, [req_norm]):\n",
    "        if synonym in cand_norm:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def fuzzy_education_match(required_edu, candidate_edu_list):\n",
    "    \"\"\"\n",
    "    Compare required education (string or dict) against candidate education list (dicts).\n",
    "    Returns best fuzzy match score (0-100).\n",
    "    \"\"\"\n",
    "    if not required_edu:\n",
    "        return 100.0  # no requirement = full score\n",
    "\n",
    "    # Convert required_edu into string if it's a dict\n",
    "    if isinstance(required_edu, dict):\n",
    "        degree = required_edu.get(\"degree\", \"\")\n",
    "        field = required_edu.get(\"field\", \"\") or required_edu.get(\"field_of_study\", \"\")\n",
    "        required_str = f\"{degree} in {field}\".strip()\n",
    "    else:\n",
    "        required_str = str(required_edu)\n",
    "\n",
    "    required_norm = normalize(required_str)\n",
    "    max_ratio = 0\n",
    "\n",
    "    for edu in candidate_edu_list:\n",
    "        if isinstance(edu, dict):\n",
    "            degree = edu.get(\"degree\", \"\")\n",
    "            field = edu.get(\"field_of_study\", \"\")\n",
    "            candidate_str = f\"{degree} in {field}\".strip()\n",
    "        else:\n",
    "            candidate_str = str(edu)\n",
    "        ratio = fuzz.token_set_ratio(required_norm, normalize(candidate_str))\n",
    "        max_ratio = max(max_ratio, ratio)\n",
    "\n",
    "    return max_ratio\n",
    "\n",
    "def hash_email(email):\n",
    "    return hashlib.md5(email.lower().strip().encode()).hexdigest()\n",
    "\n",
    "# --- Main scoring function ---\n",
    "def score_cv_against_job(cv_json_path, job_description_path, min_experience_years=2):\n",
    "    weights = {\n",
    "        \"hard_filters\": 0.25,\n",
    "        \"semantic_similarity\": 0.45,\n",
    "        \"experience_alignment\": 0.15,\n",
    "        \"education_alignment\": 0.15\n",
    "    }\n",
    "\n",
    "    embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "\n",
    "    # Load job description\n",
    "    try:\n",
    "        with open(job_description_path, 'r', encoding='utf-8') as f:\n",
    "            job_text = f.read().strip()\n",
    "            if not job_text:\n",
    "                print(f\"❌ Job description is empty: {job_description_path}\")\n",
    "                return 0.0\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Job description file not found: {job_description_path}\")\n",
    "        return 0.0\n",
    "\n",
    "    # Parse job requirements using LLM\n",
    "    llm = ChatOllama(model=\"llama3.2:latest\", format=\"json\")\n",
    "    prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "        Extract the following from the job description as JSON:\n",
    "        - required_skills: list of required skills\n",
    "        - required_certifications: list of required certifications\n",
    "        - location_eligibility: required location or \"remote\"\n",
    "        - min_years_experience: minimum years of experience\n",
    "        - required_education: required degree and field\n",
    "\n",
    "        Job Description:\n",
    "        {job_text}\n",
    "\n",
    "        Output only JSON.\n",
    "    \"\"\")\n",
    "    chain = prompt_template | llm\n",
    "    response = chain.invoke({\"job_text\": job_text})\n",
    "    try:\n",
    "        job_req = json.loads(response.content)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"⚠️ Error parsing LLM response; using defaults.\")\n",
    "        job_req = {\n",
    "            \"required_skills\": [],\n",
    "            \"required_certifications\": [],\n",
    "            \"location_eligibility\": \"\",\n",
    "            \"min_years_experience\": min_experience_years,\n",
    "            \"required_education\": \"\"\n",
    "        }\n",
    "\n",
    "    required_skills = set(normalize(s) for s in job_req.get(\"required_skills\", []))\n",
    "    required_certifications = set(normalize(c) for c in job_req.get(\"required_certifications\", []))\n",
    "    location_req = job_req.get(\"location_eligibility\", \"\").lower()\n",
    "    min_years = job_req.get(\"min_years_experience\", min_experience_years)\n",
    "    required_education = job_req.get(\"required_education\", \"\")\n",
    "\n",
    "    # Load CV\n",
    "    try:\n",
    "        with open(cv_json_path, 'r', encoding='utf-8') as f:\n",
    "            cv_data = json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "        print(f\"❌ Error loading CV JSON {cv_json_path}: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "    structured_data = cv_data.get(\"CV_data\", {}).get(\"structured_data\", {})\n",
    "    years_of_experience = structured_data.get(\"years_of_experience\", 0.0)\n",
    "    candidate_skills = [normalize(s) for s in structured_data.get(\"skills\", []) + structured_data.get(\"soft_skills\", [])]\n",
    "    candidate_certifications = [normalize(c.get(\"name\",\"\")) for c in structured_data.get(\"certifications\", []) if isinstance(c, dict)]\n",
    "    candidate_location = structured_data.get(\"location\", \"\").lower()\n",
    "    candidate_education = structured_data.get(\"education\", [])\n",
    "\n",
    "    # --- Generate cv_id from hashed email ---\n",
    "    email = structured_data.get(\"email\", \"\")\n",
    "    if email:\n",
    "        cv_id = hash_email(email)\n",
    "    else:\n",
    "        cv_id = os.path.splitext(os.path.basename(cv_json_path))[0]\n",
    "\n",
    "    print(f\"DEBUG - Email: {email}\")\n",
    "    print(f\"DEBUG - cv_id: {cv_id}\")\n",
    "\n",
    "    # --- Hard Score ---\n",
    "    hard_score = 0.0\n",
    "    skill_matches = sum(1 for s in required_skills if fuzzy_match(s, candidate_skills))\n",
    "    skill_fraction = skill_matches / len(required_skills) if required_skills else 1.0\n",
    "    hard_score += skill_fraction * 0.5\n",
    "\n",
    "    cert_matches = sum(1 for c in required_certifications if fuzzy_match(c, candidate_certifications))\n",
    "    cert_fraction = cert_matches / len(required_certifications) if required_certifications else 1.0\n",
    "    hard_score += cert_fraction * 0.3\n",
    "\n",
    "    if location_req:\n",
    "        hard_score += 0.2 if location_match(location_req, candidate_location) else 0.0\n",
    "\n",
    "    hard_score = hard_score * 100\n",
    "\n",
    "    # --- Semantic Similarity ---\n",
    "    try:\n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=\"./chroma_db\",\n",
    "            embedding_function=embeddings,\n",
    "            collection_name=\"cv_sections\"\n",
    "        )\n",
    "        retrieved = vectorstore.get(where={\"cv_id\": cv_id}, include=['embeddings', 'metadatas', 'documents'])\n",
    "        documents = retrieved.get(\"documents\", [])\n",
    "        metadatas = retrieved.get(\"metadatas\", [])\n",
    "        embeddings_retrieved = retrieved.get(\"embeddings\", [])\n",
    "        if not documents or not embeddings_retrieved:\n",
    "            print(f\"⚠️ No embeddings or documents found for CV: {cv_json_path}\")\n",
    "            return 0.0\n",
    "        print(f\"DEBUG - Retrieved {len(embeddings_retrieved)} embeddings for cv_id: {cv_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error accessing Chroma: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "    try:\n",
    "        job_vector = np.array(embeddings.embed_query(job_text), dtype=np.float64)\n",
    "        print(f\"DEBUG - Job vector shape: {job_vector.shape}\")\n",
    "        if job_vector.ndim != 1 or job_vector.shape[0] != 1024:\n",
    "            print(f\"⚠️ Invalid job vector shape: {job_vector.shape}\")\n",
    "            return 0.0\n",
    "        if not np.all(np.isfinite(job_vector)):\n",
    "            print(\"⚠️ Invalid values in job vector (NaN or inf)\")\n",
    "            return 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error embedding job description: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "    section_scores = []\n",
    "    for i, (doc_vector, metadata) in enumerate(zip(embeddings_retrieved, metadatas)):\n",
    "        try:\n",
    "            # Ensure doc_vector is a 1D NumPy array\n",
    "            doc_vector = np.array(doc_vector, dtype=np.float64).flatten()\n",
    "            print(f\"DEBUG - Embedding {i} (section: {metadata['section']}, chunk: {metadata['chunk_id']}) shape: {doc_vector.shape}\")\n",
    "            # Verify shapes match\n",
    "            if doc_vector.shape != job_vector.shape:\n",
    "                print(f\"⚠️ Shape mismatch for CV section {i}: job_vector {job_vector.shape}, doc_vector {doc_vector.shape}\")\n",
    "                continue\n",
    "            # Check for non-numeric or invalid values\n",
    "            if not np.all(np.isfinite(doc_vector)):\n",
    "                print(f\"⚠️ Invalid values in doc_vector for CV section {i}\")\n",
    "                continue\n",
    "            # Compute cosine similarity\n",
    "            norm_product = np.linalg.norm(job_vector) * np.linalg.norm(doc_vector)\n",
    "            if norm_product == 0:\n",
    "                print(f\"⚠️ Zero norm detected for CV section {i}\")\n",
    "                continue\n",
    "            similarity = np.dot(job_vector, doc_vector) / norm_product\n",
    "            # Ensure similarity is within valid range [-1, 1]\n",
    "            similarity = np.clip(similarity, -1.0, 1.0)\n",
    "            weight = 1.5 if metadata.get(\"section\") in [\"work_experience\", \"skills\", \"summary\"] else 1.0\n",
    "            section_scores.append(similarity * weight)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error computing similarity for CV section {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if section_scores:\n",
    "        mean_score = np.mean(section_scores) * 100\n",
    "        max_score = max(section_scores) * 100\n",
    "        print(f\"DEBUG - Mean similarity score: {mean_score}, Max similarity score: {max_score}\")\n",
    "        # Hybrid pooling (balanced overall coverage vs standout sections)\n",
    "        semantic_score = 0.7 * mean_score + 0.3 * max_score\n",
    "    else:\n",
    "        print(\"⚠️ No valid section scores computed\")\n",
    "        semantic_score = 0.0\n",
    "\n",
    "    # --- Experience Alignment ---\n",
    "    experience_score = min(years_of_experience / min_years, 1.0) * 100 if min_years > 0 else 100.0\n",
    "\n",
    "    # --- Education Alignment ---\n",
    "    education_score = fuzzy_education_match(required_education, candidate_education)\n",
    "\n",
    "    # --- Hybrid Score ---\n",
    "    hybrid_score = (\n",
    "        hard_score * weights[\"hard_filters\"] +\n",
    "        semantic_score * weights[\"semantic_similarity\"] +\n",
    "        experience_score * weights[\"experience_alignment\"] +\n",
    "        education_score * weights[\"education_alignment\"]\n",
    "    )\n",
    "\n",
    "    print(f\"DEBUG - Hard score: {hard_score}\")\n",
    "    print(f\"DEBUG - Semantic score: {semantic_score}\")\n",
    "    print(f\"DEBUG - Experience score: {experience_score}\")\n",
    "    print(f\"DEBUG - Education score: {education_score}\")\n",
    "    return round(hybrid_score, 2)\n",
    "\n",
    "# --- Example usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    cv_json_path = \"./extracted_files/CV_Image.json\"\n",
    "    job_description_path = \"./job_description.txt\"\n",
    "    score = score_cv_against_job(cv_json_path, job_description_path, min_experience_years=3)\n",
    "    print(f\"CV Hybrid Score: {score}/100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003258c2",
   "metadata": {},
   "source": [
    "**Retrieving Collections**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3877be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "#collection = client.get_collection(name=\"cv_sections\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c42f0ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(name=cv_sections)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eb5660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "collections = client.list_collections(limit=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "140e16f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=cv_sections)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a70b2fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CV EMBEDDED VECTORS RETRIEVAL - DIRECT ACCESS\n",
      "================================================================================\n",
      "\n",
      "[Method 1] Using ChromaDB client directly...\n",
      "\n",
      "✓ Found 1 collection(s):\n",
      "  - cv_sections (count: 15)\n",
      "\n",
      "✓ Accessing collection: cv_sections\n",
      "  Total documents: 15\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "RETRIEVING EMBEDDINGS...\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "✓ Successfully retrieved 5 documents with embeddings\n",
      "✓ Embedding dimension: 1024\n",
      "\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "DOCUMENT 1\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "ID: 61de4564-6f20-4a83-a86e-822ee63f73f3\n",
      "CV ID: 9d427c27755752a2c7b94210180e91e9\n",
      "Section: email\n",
      "Chunk ID: 0\n",
      "\n",
      "Content Preview:\n",
      "\"aidoenochkwadwo@gmail.com\"\n",
      "\n",
      "────────────────────────────────────────\n",
      "EMBEDDING VECTOR:\n",
      "────────────────────────────────────────\n",
      "Shape: (1024,)\n",
      "Dtype: float32\n",
      "\n",
      "First 20 values:\n",
      "[-0.01467171 -0.02097356 -0.04572035  0.01120208  0.00452306 -0.01690707\n",
      "  0.02434506  0.04990469  0.04167389  0.00925711  0.00410355  0.02409424\n",
      " -0.02105497 -0.016648   -0.0313089   0.00300834 -0.00914379 -0.01677926\n",
      "  0.03603763 -0.02197966]\n",
      "\n",
      "Last 20 values:\n",
      "[ 0.04058883 -0.00374794  0.02472047  0.03069117 -0.01545474  0.00745469\n",
      " -0.02257814  0.00438435 -0.00029659 -0.03649407  0.02769473 -0.02078614\n",
      "  0.02029825 -0.09213993  0.04310473  0.05980508 -0.02946667 -0.05924019\n",
      "  0.01470332 -0.00658583]\n",
      "\n",
      "Statistics:\n",
      "  Mean:     -0.000155\n",
      "  Std Dev:  0.031250\n",
      "  Min:      -0.100033\n",
      "  Max:      0.195452\n",
      "  L2 Norm:  1.000000\n",
      "\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "DOCUMENT 2\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "ID: e2778ee3-70e6-4f0d-9068-2b31de800d65\n",
      "CV ID: 9d427c27755752a2c7b94210180e91e9\n",
      "Section: summary\n",
      "Chunk ID: 0\n",
      "\n",
      "Content Preview:\n",
      "\"A Data Analyst with about two years of professional experience specialized in transforming complex datasets into strategic business solutions. I've consistently delivered actionable insights that have driven key business decisions. Combining strong analytical skills with business acumen, I excel at identifying data patterns and translating them into clear, implementable strategies that solve real...\n",
      "\n",
      "────────────────────────────────────────\n",
      "EMBEDDING VECTOR:\n",
      "────────────────────────────────────────\n",
      "Shape: (1024,)\n",
      "Dtype: float32\n",
      "\n",
      "First 20 values:\n",
      "[-0.00620162  0.03033508  0.00012151 -0.02254748  0.02840543  0.01388117\n",
      "  0.00983868 -0.00352955  0.01458995  0.03604813 -0.00229119  0.03375928\n",
      "  0.01346318 -0.01132335  0.00480863  0.02164749 -0.0487369  -0.01730052\n",
      " -0.05492701  0.00731655]\n",
      "\n",
      "Last 20 values:\n",
      "[ 0.00812526 -0.03089248 -0.00447505 -0.02926656 -0.0590934  -0.01107278\n",
      "  0.01055673 -0.01091736  0.03917787 -0.04524822 -0.0031958  -0.01607152\n",
      "  0.01966302  0.0040869  -0.0107001   0.01968381 -0.0202074  -0.04909783\n",
      " -0.00348418  0.05922534]\n",
      "\n",
      "Statistics:\n",
      "  Mean:     -0.000194\n",
      "  Std Dev:  0.031249\n",
      "  Min:      -0.092211\n",
      "  Max:      0.236810\n",
      "  L2 Norm:  1.000000\n",
      "\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "DOCUMENT 3\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "ID: 525ac818-6391-4a29-bc14-1fdd114b1098\n",
      "CV ID: 9d427c27755752a2c7b94210180e91e9\n",
      "Section: work_experience\n",
      "Chunk ID: 0\n",
      "\n",
      "Content Preview:\n",
      "[{\"company\": \"Really Great Tech\", \"title\": \"Data Analytics/AI/ML Engineer\", \"location\": \"\", \"start_date\": \"November 2023\", \"end_date\": \"October 2024\", \"responsibilities\": [\"Conducted Shapelet Analysis on trained machine learning models to interpret performance patterns and identify opportunities for optimization in an AI project.\", \"Created a dynamic Google Sheets pivot table to track projects, em...\n",
      "\n",
      "────────────────────────────────────────\n",
      "EMBEDDING VECTOR:\n",
      "────────────────────────────────────────\n",
      "Shape: (1024,)\n",
      "Dtype: float32\n",
      "\n",
      "First 20 values:\n",
      "[-0.00464036 -0.01416096 -0.03717647 -0.04362679 -0.01361977 -0.01894327\n",
      "  0.01610519 -0.02559714  0.03549135  0.02086296  0.01413065 -0.00564408\n",
      "  0.00255996 -0.04487222 -0.01478256 -0.01189214 -0.03731292 -0.00753901\n",
      " -0.02130007 -0.01351904]\n",
      "\n",
      "Last 20 values:\n",
      "[ 0.0048588  -0.02807702  0.01097525 -0.023491   -0.02527001  0.00207298\n",
      "  0.05609238 -0.00299338  0.01676304 -0.02098543  0.02458689 -0.03343664\n",
      "  0.00081432 -0.00717379 -0.01386824  0.02712632  0.00666492 -0.0072631\n",
      " -0.01166331 -0.009255  ]\n",
      "\n",
      "Statistics:\n",
      "  Mean:     -0.000197\n",
      "  Std Dev:  0.031249\n",
      "  Min:      -0.102479\n",
      "  Max:      0.247568\n",
      "  L2 Norm:  1.000000\n",
      "\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "DOCUMENT 4\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "ID: 0ca35b4c-fe90-4135-a410-b2bf70640ebc\n",
      "CV ID: 9d427c27755752a2c7b94210180e91e9\n",
      "Section: work_experience\n",
      "Chunk ID: 1\n",
      "\n",
      "Content Preview:\n",
      "for improved team visibility.\", \"Analyzed patient data and developed a predictive model for heart disease diagnosis, integrated into a user-friendly web application to support clinical decision-making.\"]}, {\"company\": \"Freelancer (ALX venturers)\", \"title\": \"Data Scientist\", \"location\": \"\", \"start_date\": \"September 2024\", \"end_date\": \"Present\", \"responsibilities\": [\"Led a freelance team to analyze ...\n",
      "\n",
      "────────────────────────────────────────\n",
      "EMBEDDING VECTOR:\n",
      "────────────────────────────────────────\n",
      "Shape: (1024,)\n",
      "Dtype: float32\n",
      "\n",
      "First 20 values:\n",
      "[ 0.01170141  0.01752633 -0.03993785 -0.02758665  0.00860736 -0.02163345\n",
      " -0.00477628  0.0111418   0.04356388  0.00072854  0.02489206 -0.02302502\n",
      "  0.02558313 -0.02223285 -0.00777521 -0.01681773 -0.04931128 -0.01663337\n",
      " -0.03130047 -0.0005277 ]\n",
      "\n",
      "Last 20 values:\n",
      "[ 0.00930672 -0.02160753 -0.00295862 -0.02104973 -0.02391512  0.02840602\n",
      "  0.03096728 -0.00924136  0.0350068  -0.0158305  -0.00675313 -0.04710151\n",
      " -0.01221992 -0.01909319  0.0142326   0.01637974  0.00833424 -0.0042684\n",
      " -0.02904228  0.01081838]\n",
      "\n",
      "Statistics:\n",
      "  Mean:     -0.000232\n",
      "  Std Dev:  0.031249\n",
      "  Min:      -0.092680\n",
      "  Max:      0.255248\n",
      "  L2 Norm:  1.000000\n",
      "\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "DOCUMENT 5\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "ID: 9263a64e-04a2-46cd-9b76-55b62cff1dc5\n",
      "CV ID: 9d427c27755752a2c7b94210180e91e9\n",
      "Section: work_experience\n",
      "Chunk ID: 2\n",
      "\n",
      "Content Preview:\n",
      "techniques to uncover trends and patterns and built a predictive model to forecast revenue and optimize customer pricing strategies for a tourist agency.\"]}]\n",
      "\n",
      "────────────────────────────────────────\n",
      "EMBEDDING VECTOR:\n",
      "────────────────────────────────────────\n",
      "Shape: (1024,)\n",
      "Dtype: float32\n",
      "\n",
      "First 20 values:\n",
      "[-0.00856625  0.05780131 -0.04984427 -0.03900394  0.02187898  0.00620839\n",
      " -0.03066237  0.0160264   0.02790312  0.00851637  0.02888026 -0.00664813\n",
      " -0.035021   -0.00699654 -0.0043447  -0.00021562 -0.03834875 -0.00483242\n",
      " -0.00990662  0.00116754]\n",
      "\n",
      "Last 20 values:\n",
      "[-0.01320849 -0.0241467  -0.00463119 -0.01599797 -0.03011006 -0.0088646\n",
      "  0.00093486 -0.00411087  0.00215164  0.00937406  0.01889049 -0.02189621\n",
      " -0.00639346 -0.04062457  0.02651934  0.00203984  0.0128101   0.00749967\n",
      " -0.0173335   0.02597796]\n",
      "\n",
      "Statistics:\n",
      "  Mean:     -0.000186\n",
      "  Std Dev:  0.031249\n",
      "  Min:      -0.091389\n",
      "  Max:      0.219918\n",
      "  L2 Norm:  1.000000\n",
      "\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "FULL COLLECTION ANALYSIS\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Total documents: 15\n",
      "\n",
      "────────────────────────────────────────\n",
      "EMBEDDINGS BY SECTION:\n",
      "────────────────────────────────────────\n",
      "\n",
      "CERTIFICATIONS:\n",
      "  Documents: 2\n",
      "  Avg L2 Norm: 1.000000\n",
      "  Avg Mean: -0.000213\n",
      "  Avg Std: 0.031249\n",
      "\n",
      "EDUCATION:\n",
      "  Documents: 1\n",
      "  Avg L2 Norm: 1.000000\n",
      "  Avg Mean: -0.000189\n",
      "  Avg Std: 0.031249\n",
      "\n",
      "EMAIL:\n",
      "  Documents: 1\n",
      "  Avg L2 Norm: 1.000000\n",
      "  Avg Mean: -0.000155\n",
      "  Avg Std: 0.031250\n",
      "\n",
      "PROJECTS:\n",
      "  Documents: 4\n",
      "  Avg L2 Norm: 1.000000\n",
      "  Avg Mean: -0.000227\n",
      "  Avg Std: 0.031249\n",
      "\n",
      "SKILLS:\n",
      "  Documents: 1\n",
      "  Avg L2 Norm: 1.000000\n",
      "  Avg Mean: -0.000139\n",
      "  Avg Std: 0.031250\n",
      "\n",
      "SOFT_SKILLS:\n",
      "  Documents: 1\n",
      "  Avg L2 Norm: 1.000000\n",
      "  Avg Mean: -0.000190\n",
      "  Avg Std: 0.031249\n",
      "\n",
      "SUMMARY:\n",
      "  Documents: 1\n",
      "  Avg L2 Norm: 1.000000\n",
      "  Avg Mean: -0.000194\n",
      "  Avg Std: 0.031249\n",
      "\n",
      "WORK_EXPERIENCE:\n",
      "  Documents: 3\n",
      "  Avg L2 Norm: 1.000000\n",
      "  Avg Mean: -0.000205\n",
      "  Avg Std: 0.031249\n",
      "\n",
      "YEARS_OF_EXPERIENCE:\n",
      "  Documents: 1\n",
      "  Avg L2 Norm: 1.000000\n",
      "  Avg Mean: -0.000248\n",
      "  Avg Std: 0.031249\n",
      "\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "EXPORTING EMBEDDINGS TO FILES\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "✓ Saved embeddings to: cv_embeddings.npy\n",
      "  Shape: (15, 1024)\n",
      "✓ Saved metadata to: cv_metadata.json\n",
      "✓ Saved human-readable embeddings to: cv_embeddings.txt\n",
      "✓ Saved embeddings as CSV to: cv_embeddings.csv\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FILES CREATED:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "1. cv_embeddings.npy    - NumPy binary format (for loading in Python)\n",
      "2. cv_metadata.json     - Document metadata in JSON format\n",
      "3. cv_embeddings.txt    - Human-readable text file with all vectors\n",
      "4. cv_embeddings.csv    - CSV format (can open in Excel)\n",
      "\n",
      "To load embeddings later:\n",
      "embeddings = np.load('cv_embeddings.npy')\n",
      "with open('cv_metadata.json', 'r') as f:\n",
      "    metadata = json.load(f)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CV EMBEDDED VECTORS RETRIEVAL - DIRECT ACCESS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# --- Method 1: Direct ChromaDB client access ---\n",
    "print(\"\\n[Method 1] Using ChromaDB client directly...\")\n",
    "\n",
    "try:\n",
    "    # Create direct client\n",
    "    client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    \n",
    "    # List all collections\n",
    "    collections = client.list_collections()\n",
    "    print(f\"\\n✓ Found {len(collections)} collection(s):\")\n",
    "    for coll in collections:\n",
    "        print(f\"  - {coll.name} (count: {coll.count()})\")\n",
    "    \n",
    "    # Try both possible collection names\n",
    "    collection_names = [\"cv_sections\", \"cv_sections1\"]\n",
    "    collection = None\n",
    "    \n",
    "    for name in collection_names:\n",
    "        try:\n",
    "            collection = client.get_collection(name=name)\n",
    "            print(f\"\\n✓ Accessing collection: {name}\")\n",
    "            print(f\"  Total documents: {collection.count()}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"  Collection '{name}' not found or error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not collection:\n",
    "        print(\"\\n❌ Could not access any collection. Available collections:\")\n",
    "        for coll in collections:\n",
    "            print(f\"  - {coll.name}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Get data with embeddings using where/limit\n",
    "    print(f\"\\n{'─' * 80}\")\n",
    "    print(\"RETRIEVING EMBEDDINGS...\")\n",
    "    print('─' * 80)\n",
    "    \n",
    "    # Get first 5 documents\n",
    "    result = collection.get(\n",
    "        limit=5,\n",
    "        include=['embeddings', 'metadatas', 'documents']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Successfully retrieved {len(result['ids'])} documents with embeddings\")\n",
    "    \n",
    "    if result['embeddings'] is not None and len(result['embeddings']) > 0:\n",
    "        print(f\"✓ Embedding dimension: {len(result['embeddings'][0])}\")\n",
    "    \n",
    "    # Display each document with its embedding\n",
    "    for i, (doc_id, embedding, metadata, content) in enumerate(zip(\n",
    "        result['ids'],\n",
    "        result['embeddings'],\n",
    "        result['metadatas'],\n",
    "        result['documents']\n",
    "    ), 1):\n",
    "        print(f\"\\n{'═' * 80}\")\n",
    "        print(f\"DOCUMENT {i}\")\n",
    "        print(f\"{'═' * 80}\")\n",
    "        print(f\"ID: {doc_id}\")\n",
    "        print(f\"CV ID: {metadata.get('cv_id', 'N/A')}\")\n",
    "        print(f\"Section: {metadata.get('section', 'N/A')}\")\n",
    "        print(f\"Chunk ID: {metadata.get('chunk_id', 'N/A')}\")\n",
    "        \n",
    "        print(f\"\\nContent Preview:\")\n",
    "        try:\n",
    "            content_json = json.loads(content)\n",
    "            content_str = json.dumps(content_json, indent=2)\n",
    "            print(content_str[:400] + \"...\" if len(content_str) > 400 else content_str)\n",
    "        except:\n",
    "            print(content[:400] + \"...\" if len(content) > 400 else content)\n",
    "        \n",
    "        print(f\"\\n{'─' * 40}\")\n",
    "        print(\"EMBEDDING VECTOR:\")\n",
    "        print('─' * 40)\n",
    "        embedding_array = np.array(embedding, dtype=np.float32)\n",
    "        print(f\"Shape: {embedding_array.shape}\")\n",
    "        print(f\"Dtype: {embedding_array.dtype}\")\n",
    "        print(f\"\\nFirst 20 values:\\n{embedding_array[:20]}\")\n",
    "        print(f\"\\nLast 20 values:\\n{embedding_array[-20:]}\")\n",
    "        print(f\"\\nStatistics:\")\n",
    "        print(f\"  Mean:     {np.mean(embedding_array):.6f}\")\n",
    "        print(f\"  Std Dev:  {np.std(embedding_array):.6f}\")\n",
    "        print(f\"  Min:      {np.min(embedding_array):.6f}\")\n",
    "        print(f\"  Max:      {np.max(embedding_array):.6f}\")\n",
    "        print(f\"  L2 Norm:  {np.linalg.norm(embedding_array):.6f}\")\n",
    "        \n",
    "        # Uncomment to see full vector\n",
    "        # print(f\"\\nFull vector:\\n{embedding_array}\")\n",
    "    \n",
    "    # Get all embeddings for analysis\n",
    "    print(f\"\\n{'═' * 80}\")\n",
    "    print(\"FULL COLLECTION ANALYSIS\")\n",
    "    print(f\"{'═' * 80}\")\n",
    "    \n",
    "    all_result = collection.get(\n",
    "        include=['embeddings', 'metadatas']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTotal documents: {len(all_result['ids'])}\")\n",
    "    \n",
    "    # Organize by section\n",
    "    sections_data = {}\n",
    "    for embedding, metadata in zip(all_result['embeddings'], all_result['metadatas']):\n",
    "        section = metadata.get('section', 'unknown')\n",
    "        if section not in sections_data:\n",
    "            sections_data[section] = []\n",
    "        sections_data[section].append(np.array(embedding, dtype=np.float32))\n",
    "    \n",
    "    print(f\"\\n{'─' * 40}\")\n",
    "    print(\"EMBEDDINGS BY SECTION:\")\n",
    "    print('─' * 40)\n",
    "    for section, embeddings_list in sorted(sections_data.items()):\n",
    "        embeddings_matrix = np.array(embeddings_list)\n",
    "        print(f\"\\n{section.upper()}:\")\n",
    "        print(f\"  Documents: {len(embeddings_list)}\")\n",
    "        print(f\"  Avg L2 Norm: {np.mean([np.linalg.norm(e) for e in embeddings_list]):.6f}\")\n",
    "        print(f\"  Avg Mean: {np.mean(embeddings_matrix):.6f}\")\n",
    "        print(f\"  Avg Std: {np.std(embeddings_matrix):.6f}\")\n",
    "    \n",
    "    # Export options\n",
    "    print(f\"\\n{'═' * 80}\")\n",
    "    print(\"EXPORTING EMBEDDINGS TO FILES\")\n",
    "    print(f\"{'═' * 80}\")\n",
    "    \n",
    "    # Export embeddings as numpy array\n",
    "    all_embeddings = np.array(all_result['embeddings'], dtype=np.float32)\n",
    "    np.save('cv_embeddings.npy', all_embeddings)\n",
    "    print(\"✓ Saved embeddings to: cv_embeddings.npy\")\n",
    "    print(f\"  Shape: {all_embeddings.shape}\")\n",
    "    \n",
    "    # Export metadata\n",
    "    with open('cv_metadata.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            'ids': all_result['ids'],\n",
    "            'metadatas': all_result['metadatas']\n",
    "        }, f, indent=2)\n",
    "    print(\"✓ Saved metadata to: cv_metadata.json\")\n",
    "    \n",
    "    # Export embeddings as human-readable text file\n",
    "    with open('cv_embeddings.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\" * 80 + \"\\n\")\n",
    "        f.write(\"CV EMBEDDINGS - FULL EXPORT\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        for i, (doc_id, embedding, metadata) in enumerate(zip(\n",
    "            all_result['ids'],\n",
    "            all_result['embeddings'],\n",
    "            all_result['metadatas']\n",
    "        ), 1):\n",
    "            f.write(f\"\\n{'=' * 80}\\n\")\n",
    "            f.write(f\"DOCUMENT {i}\\n\")\n",
    "            f.write(f\"{'=' * 80}\\n\")\n",
    "            f.write(f\"ID: {doc_id}\\n\")\n",
    "            f.write(f\"CV ID: {metadata.get('cv_id', 'N/A')}\\n\")\n",
    "            f.write(f\"Section: {metadata.get('section', 'N/A')}\\n\")\n",
    "            f.write(f\"Chunk ID: {metadata.get('chunk_id', 'N/A')}\\n\")\n",
    "            \n",
    "            embedding_array = np.array(embedding, dtype=np.float32)\n",
    "            f.write(f\"\\nEmbedding Vector (dimension: {len(embedding_array)}):\\n\")\n",
    "            f.write(f\"{embedding_array}\\n\")\n",
    "            \n",
    "            f.write(f\"\\nStatistics:\\n\")\n",
    "            f.write(f\"  Mean:     {np.mean(embedding_array):.6f}\\n\")\n",
    "            f.write(f\"  Std Dev:  {np.std(embedding_array):.6f}\\n\")\n",
    "            f.write(f\"  Min:      {np.min(embedding_array):.6f}\\n\")\n",
    "            f.write(f\"  Max:      {np.max(embedding_array):.6f}\\n\")\n",
    "            f.write(f\"  L2 Norm:  {np.linalg.norm(embedding_array):.6f}\\n\")\n",
    "    \n",
    "    print(\"✓ Saved human-readable embeddings to: cv_embeddings.txt\")\n",
    "    \n",
    "    # Export embeddings as CSV for easy viewing\n",
    "    import csv\n",
    "    with open('cv_embeddings.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        # Header\n",
    "        header = ['doc_id', 'cv_id', 'section', 'chunk_id'] + [f'dim_{i}' for i in range(len(all_result['embeddings'][0]))]\n",
    "        writer.writerow(header)\n",
    "        \n",
    "        # Data\n",
    "        for doc_id, embedding, metadata in zip(\n",
    "            all_result['ids'],\n",
    "            all_result['embeddings'],\n",
    "            all_result['metadatas']\n",
    "        ):\n",
    "            row = [\n",
    "                doc_id,\n",
    "                metadata.get('cv_id', 'N/A'),\n",
    "                metadata.get('section', 'N/A'),\n",
    "                metadata.get('chunk_id', 'N/A')\n",
    "            ] + list(embedding)\n",
    "            writer.writerow(row)\n",
    "    \n",
    "    print(\"✓ Saved embeddings as CSV to: cv_embeddings.csv\")\n",
    "    \n",
    "    print(f\"\\n{'─' * 80}\")\n",
    "    print(\"FILES CREATED:\")\n",
    "    print('─' * 80)\n",
    "    print(\"1. cv_embeddings.npy    - NumPy binary format (for loading in Python)\")\n",
    "    print(\"2. cv_metadata.json     - Document metadata in JSON format\")\n",
    "    print(\"3. cv_embeddings.txt    - Human-readable text file with all vectors\")\n",
    "    print(\"4. cv_embeddings.csv    - CSV format (can open in Excel)\")\n",
    "    \n",
    "    print(f\"\\nTo load embeddings later:\")\n",
    "    print(\"embeddings = np.load('cv_embeddings.npy')\")\n",
    "    print(\"with open('cv_metadata.json', 'r') as f:\")\n",
    "    print(\"    metadata = json.load(f)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TROUBLESHOOTING:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"1. Check collection name in embedding script (cv_sections vs cv_sections1)\")\n",
    "    print(\"2. Verify ./chroma_db directory exists and has data\")\n",
    "    print(\"3. Try deleting chroma_db folder and re-running embedding script\")\n",
    "    print(\"4. Check if ChromaDB version is compatible\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a12ca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from jd_extractor import JDExtractor  # Import from your job description extraction file\n",
    "\n",
    "class CVJDScorer:\n",
    "    def __init__(self):\n",
    "        self.jd_extractor = JDExtractor()  # Use your custom JD extractor\n",
    "\n",
    "    def extract_job_requirements(self, jd_document_path):\n",
    "        \"\"\"\n",
    "        Extract job requirements using the custom JDExtractor\n",
    "        \"\"\"\n",
    "        try:\n",
    "            jd_data = self.jd_extractor.extract(jd_document_path)\n",
    "            return jd_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting job description: {e}\")\n",
    "            return None\n",
    "\n",
    "    def calculate_match_score(self, cv_data, jd_data):\n",
    "        \"\"\"\n",
    "        Calculate match score between CV and job description data\n",
    "        \"\"\"\n",
    "        if not jd_data or not cv_data:\n",
    "            return 0\n",
    "        \n",
    "        scores = {}\n",
    "        total_weight = 0\n",
    "        \n",
    "        # 1. Skills Match (30% weight)\n",
    "        if jd_data.get('technical_skills') and cv_data.get('skills'):\n",
    "            skill_match = self._calculate_skills_match(\n",
    "                cv_data['skills'], \n",
    "                jd_data['technical_skills']\n",
    "            )\n",
    "            scores['skills'] = {'score': skill_match, 'weight': 0.3}\n",
    "            total_weight += 0.3\n",
    "        \n",
    "        # 2. Experience Match (25% weight)\n",
    "        if jd_data.get('experience_requirements') and cv_data.get('years_of_experience'):\n",
    "            exp_match = self._calculate_experience_match(\n",
    "                cv_data['years_of_experience'],\n",
    "                jd_data['experience_requirements']\n",
    "            )\n",
    "            scores['experience'] = {'score': exp_match, 'weight': 0.25}\n",
    "            total_weight += 0.25\n",
    "        \n",
    "        # 3. Education Match (20% weight)\n",
    "        if jd_data.get('education_requirements') and cv_data.get('education'):\n",
    "            edu_match = self._calculate_education_match(\n",
    "                cv_data['education'],\n",
    "                jd_data['education_requirements']\n",
    "            )\n",
    "            scores['education'] = {'score': edu_match, 'weight': 0.2}\n",
    "            total_weight += 0.2\n",
    "        \n",
    "        # 4. Certifications Match (15% weight)\n",
    "        if jd_data.get('certifications') and cv_data.get('certifications'):\n",
    "            cert_match = self._calculate_certifications_match(\n",
    "                cv_data['certifications'],\n",
    "                jd_data['certifications']\n",
    "            )\n",
    "            scores['certifications'] = {'score': cert_match, 'weight': 0.15}\n",
    "            total_weight += 0.15\n",
    "        \n",
    "        # 5. Soft Skills Match (10% weight)\n",
    "        if jd_data.get('soft_skills') and cv_data.get('soft_skills'):\n",
    "            soft_skills_match = self._calculate_skills_match(\n",
    "                cv_data['soft_skills'],\n",
    "                jd_data['soft_skills']\n",
    "            )\n",
    "            scores['soft_skills'] = {'score': soft_skills_match, 'weight': 0.1}\n",
    "            total_weight += 0.1\n",
    "        \n",
    "        # Calculate weighted average\n",
    "        if total_weight == 0:\n",
    "            return 0\n",
    "            \n",
    "        weighted_score = sum(\n",
    "            category['score'] * category['weight'] \n",
    "            for category in scores.values()\n",
    "        ) / total_weight\n",
    "        \n",
    "        return {\n",
    "            'overall_score': round(weighted_score, 2),\n",
    "            'category_scores': scores,\n",
    "            'jd_data': jd_data,\n",
    "            'cv_data': cv_data\n",
    "        }\n",
    "\n",
    "    def _calculate_skills_match(self, cv_skills, jd_skills):\n",
    "        \"\"\"Calculate percentage match between CV skills and JD required skills\"\"\"\n",
    "        if not jd_skills:\n",
    "            return 0\n",
    "        \n",
    "        cv_skills_lower = [skill.lower() for skill in cv_skills]\n",
    "        jd_skills_lower = [skill.lower() for skill in jd_skills]\n",
    "        \n",
    "        matches = sum(1 for jd_skill in jd_skills_lower \n",
    "                     if any(jd_skill in cv_skill or cv_skill in jd_skill \n",
    "                           for cv_skill in cv_skills_lower))\n",
    "        \n",
    "        return round((matches / len(jd_skills_lower)) * 100, 2)\n",
    "\n",
    "    def _calculate_experience_match(self, cv_experience, jd_experience_req):\n",
    "        \"\"\"Calculate experience match score\"\"\"\n",
    "        try:\n",
    "            # Extract years from JD requirements (simple parsing)\n",
    "            jd_years_text = jd_experience_req.get('years_of_experience', '0')\n",
    "            jd_years = self._extract_years_from_text(jd_years_text)\n",
    "            \n",
    "            cv_years = float(cv_experience) if cv_experience else 0\n",
    "            \n",
    "            if cv_years >= jd_years:\n",
    "                return 100\n",
    "            else:\n",
    "                return round((cv_years / jd_years) * 100, 2) if jd_years > 0 else 0\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def _calculate_education_match(self, cv_education, jd_education_req):\n",
    "        \"\"\"Calculate education requirements match\"\"\"\n",
    "        if not jd_education_req:\n",
    "            return 100\n",
    "        \n",
    "        cv_degrees = [edu.get('degree', '').lower() for edu in cv_education]\n",
    "        jd_edu_lower = [req.lower() for req in jd_education_req]\n",
    "        \n",
    "        # Check if any CV degree matches JD requirements\n",
    "        for degree in cv_degrees:\n",
    "            for req in jd_edu_lower:\n",
    "                if req in degree or degree in req:\n",
    "                    return 100\n",
    "        \n",
    "        return 0\n",
    "\n",
    "    def _calculate_certifications_match(self, cv_certifications, jd_certifications):\n",
    "        \"\"\"Calculate certifications match\"\"\"\n",
    "        if not jd_certifications:\n",
    "            return 100\n",
    "            \n",
    "        cv_cert_names = [cert.get('name', '').lower() for cert in cv_certifications]\n",
    "        jd_cert_lower = [cert.lower() for cert in jd_certifications]\n",
    "        \n",
    "        matches = sum(1 for jd_cert in jd_cert_lower \n",
    "                     if any(jd_cert in cv_cert or cv_cert in jd_cert \n",
    "                           for cv_cert in cv_cert_names))\n",
    "        \n",
    "        return round((matches / len(jd_certifications)) * 100, 2)\n",
    "\n",
    "    def _extract_years_from_text(self, text):\n",
    "        \"\"\"Extract years from text like '5+ years' or '3 years'\"\"\"\n",
    "        import re\n",
    "        numbers = re.findall(r'\\d+', str(text))\n",
    "        return float(numbers[0]) if numbers else 0\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    scorer = CVJDScorer()\n",
    "    \n",
    "    # Extract job requirements from JD document using your custom extractor\n",
    "    jd_document_path = './job_description.txt'\n",
    "    jd_data = scorer.extract_job_requirements(jd_document_path)\n",
    "    \n",
    "    print(\"Extracted Job Description Data:\")\n",
    "    print(json.dumps(jd_data, indent=2))\n",
    "    \n",
    "    # Your CV data (assuming you have it from your CV extraction process)\n",
    "    cv_data = {\n",
    "        # Your CV data here...\n",
    "    }\n",
    "    \n",
    "    if cv_data and jd_data:\n",
    "        # Calculate match score\n",
    "        match_result = scorer.calculate_match_score(cv_data, jd_data)\n",
    "        print(\"\\nMatch Score Results:\")\n",
    "        print(json.dumps(match_result, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
