{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7889ba96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text extraction from: ./CV_Image.png\n",
      "\n",
      "--- Extracted Text ---\n",
      "# Aidoo Enoch Kwadwo\n",
      "## Data Analyst\n",
      "\n",
      "## Personal Info\n",
      "**Phone**\n",
      "0240542834\n",
      "\n",
      "**Email**\n",
      "aidooenochkwadwo@gmail.com\n",
      "\n",
      "**Kumasi, Ghana**\n",
      "\n",
      "## Qualities\n",
      "* Curiosity\n",
      "* Problem Solving\n",
      "* System Understanding\n",
      "* Technical Skills\n",
      "* Analytical Thinking\n",
      "* Problem Solving Skills\n",
      "* Teamwork\n",
      "* Initiative and Self-motivation\n",
      "* Discipline and Resilient\n",
      "\n",
      "## Key Skills\n",
      "**Tools:** Python, R, AWS, Microsoft Excel, Google Sheets, Power BI, SQL\n",
      "**Packages/Frameworks:** NumPy, Pandas, Scikit-Learn, Matplotlib, Pytorch\n",
      "**Machine Learning:** Data Analysis, Classification Modeling, Deep Neural Networks, Regression Modelling, MLOPs, Computer Vision, Natural Language Processing, Recommendation Systems\n",
      "\n",
      "## About Me\n",
      "A Data Analyst with about two years of professional experience specialized in transforming complex datasets into strategic business solutions. I've consistently delivered actionable insights that have driven key business decisions. Combining strong analytical skills with business acumen, I excel at identifying data patterns and translating them into clear, implementable strategies that solve real-world challenges.\n",
      "\n",
      "## Education\n",
      "**Bachelor of Science in Computer Science.**\n",
      "KNUST, Ghana\n",
      "2023\n",
      "\n",
      "## Courses and Certifications\n",
      "* Coursera Crash Course on Python.\n",
      "  Oct 2021\n",
      "* AWS Machine Learning Foundation.\n",
      "  Oct 2021\n",
      "* Introduction to Deep Learning with Pytorch.\n",
      "  Sep 2022\n",
      "* Machine Learning for Trading on Udacity.\n",
      "  Offered at Georgia Tech as CS 7646.\n",
      "  May 2023\n",
      "* Machine Learning.\n",
      "  Offered by Stanford University on Coursera.\n",
      "  Present\n",
      "* Google Data Analytics Professional Certificate.\n",
      "  Present\n",
      "* ALX Data Science\n",
      "  May 2023\n",
      "* AWS Certified Cloud Practitioner\n",
      "  Aug 2025\n",
      "\n",
      "## Projects\n",
      "* Conducted advanced Excel analysis on water access datasets, applying data cleaning, formulas, statistical functions, and pivot tables. Designed visualizations (stacked bar charts, box & whisker plots) to uncover disparities across urban/rural areas and income groups, supporting evidence-based resource allocation.\n",
      "* Built a predictive model for quantitative finance using historical market data, achieving a validation MSE of 0.998. The model improved risk forecasting accuracy and provided actionable insights for financial decision-making.\n",
      "* Led SQL-based analysis of 60,000+ water infrastructure records, using advanced queries and aggregation techniques to identify critical gaps in community water access. Delivered data-driven recommendations that guided infrastructure planning and resource distribution.\n",
      "* Developed stakeholder-driven Power BI dashboards with advanced DAX calculations and multi-level drill-downs to monitor project progress, budgets, and impact. The dashboards improved transparency, performance tracking, and decision-making at national, provincial, and local levels.\n",
      "\n",
      "## Work Experience\n",
      "**Data Analytics/AI/ML Engineer**\n",
      "@Really Great Tech\n",
      "November 2023 â€“ October 2024\n",
      "* Conducted Shapelet Analysis on trained machine learning models to interpret performance patterns and identify opportunities for optimization in an AI project.\n",
      "* Created a dynamic Google Sheets pivot table to track projects, employee assignments, and mentorship relationships for improved team visibility.\n",
      "* Analyzed patient data and developed a predictive model for heart disease diagnosis, integrated into a user-friendly web application to support clinical decision-making\n",
      "\n",
      "**Data Scientist**\n",
      "@Freelancer (ALX venturers)\n",
      "September 2024 â€“ Present\n",
      "Led a freelance team to analyze tourist consumer behavior using advanced data analytics techniques to uncover trends and patterns and built a predictive model to forecast revenue and optimize customer pricing strategies for a tourist agency.\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from docstrange import DocumentExtractor\n",
    "\n",
    "def extract_document_text(document_path: str):\n",
    "    \"\"\"\n",
    "    Extracts and prints the text content from a given document.\n",
    "\n",
    "    Args:\n",
    "        document_path (str): The file path to the document (e.g., 'document.pdf').\n",
    "    \"\"\"\n",
    "    print(f\"Starting text extraction from: {document_path}\\n\")\n",
    "\n",
    "    # Initialize the DocumentExtractor in local CPU mode.\n",
    "    # This ensures that all processing happens on your machine and no data\n",
    "    # is sent to a cloud API.\n",
    "    try:\n",
    "        extractor = DocumentExtractor()\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing DocumentExtractor: {e}\")\n",
    "        print(\"Please ensure you have installed the necessary dependencies.\")\n",
    "        print(\"If you are running for the first time, you may need an internet connection to download models.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # The extract() method processes the document.\n",
    "        # It handles various formats like PDF, DOCX, and images.\n",
    "        result = extractor.extract(document_path)\n",
    "        \n",
    "        # We can extract the content in various formats. Here, we'll get\n",
    "        # the cleaned, LLM-optimized Markdown text.\n",
    "        extracted_text = result.extract_markdown()\n",
    "\n",
    "        if extracted_text:\n",
    "            print(\"--- Extracted Text ---\")\n",
    "            print(extracted_text)\n",
    "            print(\"----------------------\")\n",
    "        else:\n",
    "            print(\"No text could be extracted from the document.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{document_path}' was not found.\")\n",
    "        print(\"Please check the file path and try again.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during extraction: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # In a real-world scenario, you would replace 'your_document.pdf'\n",
    "    # with the actual path to your document.\n",
    "    # For a command-line script, you could also use sys.argv to get the path.\n",
    "    sample_document_path = './CV_Image.png'\n",
    "\n",
    " \n",
    "\n",
    "    extract_document_text(sample_document_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58372fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a3a0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse JSON content: Expecting value: line 2 column 1 (char 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Extracted and cleaned CV saved to: ./extracted_files/Data_Analyst3_CV.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from docstrange_extractor import CVExtractor\n",
    "from prof_years_extractor import ProfessionalExperienceCalculator\n",
    "\n",
    "class CVProcessor:\n",
    "    \"\"\"A class to handle CV data extraction, cleaning, and processing.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_cv_data(cv_data):\n",
    "        \"\"\"\n",
    "        Recursively clean extracted CV data:\n",
    "        - Replace None/null with [] for list-like fields\n",
    "        - Replace None/null with \"\" for text fields\n",
    "        \"\"\"\n",
    "        if isinstance(cv_data, dict):\n",
    "            cleaned = {}\n",
    "            for key, value in cv_data.items():\n",
    "                if value is None:\n",
    "                    if key in [\"work_experience\", \"education\", \"skills\", \"soft_skills\",\n",
    "                              \"certifications\", \"projects\", \"languages\", \"hobbies\"]:\n",
    "                        cleaned[key] = []\n",
    "                    else:\n",
    "                        cleaned[key] = \"\"\n",
    "                else:\n",
    "                    cleaned[key] = CVProcessor.clean_cv_data(value)\n",
    "            return cleaned\n",
    "        elif isinstance(cv_data, list):\n",
    "            return [CVProcessor.clean_cv_data(item) for item in cv_data]\n",
    "        return cv_data\n",
    "\n",
    "    def extract_and_save_cv(self, cv_file_path, output_dir):\n",
    "        \"\"\"\n",
    "        Extract CV data, calculate years of experience, and save to JSON.\n",
    "        \n",
    "        Args:\n",
    "            cv_file_path (str): Path to the input CV file (e.g., PDF, DOCX)\n",
    "            output_dir (str): Directory to save the output JSON file\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to the saved JSON file, or None if extraction fails\n",
    "        \"\"\"\n",
    "        extractor = CVExtractor()\n",
    "        try:\n",
    "            content = extractor.extract(cv_file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error extracting {cv_file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        if content is None:\n",
    "            print(f\"âš ï¸ Extraction returned None for {cv_file_path}\")\n",
    "            return None\n",
    "\n",
    "        # Clean data before processing\n",
    "        cleaned_content = self.clean_cv_data(content)\n",
    "\n",
    "        # Normalize structure: always provide CV_data.structured_data\n",
    "        if isinstance(cleaned_content, dict) and \"structured_data\" in cleaned_content and isinstance(cleaned_content[\"structured_data\"], dict):\n",
    "            structured = cleaned_content[\"structured_data\"]\n",
    "        else:\n",
    "            structured = cleaned_content if isinstance(cleaned_content, dict) else {\"raw_text\": cleaned_content}\n",
    "\n",
    "        # Ensure list keys exist to avoid downstream KeyErrors\n",
    "        structured.setdefault(\"work_experience\", [])\n",
    "\n",
    "        output_dict = {\"CV_data\": {\"structured_data\": structured}}\n",
    "\n",
    "        # Calculate years of experience safely\n",
    "        years_of_experience = 0.0\n",
    "        try:\n",
    "            calculator = ProfessionalExperienceCalculator(cv_data_dict=output_dict)\n",
    "            years_of_experience = calculator.get_total_years()\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error calculating years of experience for {cv_file_path}: {e}\")\n",
    "        # Guarantee nested path exists before assignment\n",
    "        output_dict.setdefault(\"CV_data\", {}).setdefault(\"structured_data\", {}).setdefault(\"years_of_experience\", years_of_experience)\n",
    "        output_dict[\"CV_data\"][\"structured_data\"][\"years_of_experience\"] = years_of_experience\n",
    "\n",
    "        # Save to JSON\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        base_name = os.path.splitext(os.path.basename(cv_file_path))[0]\n",
    "        output_path = os.path.join(output_dir, f\"{base_name}.json\")\n",
    "\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_dict, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"âœ… Extracted and cleaned CV saved to: {output_path}\")\n",
    "        return output_path\n",
    "\n",
    "    def batch_extract_cvs(self, input_dir, output_dir=\"extracted_files\"):\n",
    "        \"\"\"\n",
    "        Process multiple CV files in a directory and save extracted data as JSON.\n",
    "        \n",
    "        Args:\n",
    "            input_dir (str): Directory containing CV files\n",
    "            output_dir (str): Directory to save JSON outputs\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(input_dir):\n",
    "            print(f\"âŒ Input directory not found: {input_dir}\")\n",
    "            return\n",
    "\n",
    "        files = [f for f in os.listdir(input_dir) if f.lower().endswith(('.pdf', '.docx', '.png', '.jpg', '.jpeg'))]\n",
    "        if not files:\n",
    "            print(f\"âš ï¸ No CV files (.pdf or .docx) found in {input_dir}\")\n",
    "            return\n",
    "\n",
    "        for file_name in files:\n",
    "            cv_path = os.path.join(input_dir, file_name)\n",
    "            self.extract_and_save_cv(cv_path, output_dir)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    processor = CVProcessor()\n",
    "    processor.extract_and_save_cv(\"./CVs/Data_Analyst3_CV.pdf\", \"./extracted_files/\")\n",
    "    # processor.batch_extract_cvs(\"./CVs\", \"./extracted_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0422da8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 10:37:47,859 - INFO - MongoDB client initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ CV-JD RERANKER - REAL WORLD TEST\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 10:37:55,577 - INFO - Initialized sentence-transformers CrossEncoder BAAI/bge-reranker-base on cuda\n",
      "2025-10-23 10:37:55,637 - WARNING - No JD found for sample_jd_data_scientist\n",
      "2025-10-23 10:37:55,656 - WARNING - No JD found for TechCorp/Senior Data Scientist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. RERANK BY JD_ID\n",
      "==================================================\n",
      "\n",
      "--- CV #1 (cv_diana_004) ---\n",
      "Email: diana.wang@ai.com\n",
      "Vector Score: 0.910\n",
      "Cross-Encoder: 0.000\n",
      "Sections: skills:0.95 | experience:0.88\n",
      "\n",
      "--- CV #2 (cv_alice_001) ---\n",
      "Email: alice.chen@tech.com\n",
      "Vector Score: 0.870\n",
      "Cross-Encoder: 0.000\n",
      "Sections: skills:0.92 | experience:0.85\n",
      "\n",
      "--- CV #3 (cv_bob_002) ---\n",
      "Email: bob.smith@data.com\n",
      "Vector Score: 0.820\n",
      "Cross-Encoder: 0.000\n",
      "Sections: skills:0.88 | experience:0.78\n",
      "\n",
      "--- CV #4 (cv_charlie_003) ---\n",
      "Email: charlie.lee@ml.com\n",
      "Vector Score: 0.790\n",
      "Cross-Encoder: 0.000\n",
      "Sections: skills:0.85 | experience:0.75\n",
      "\n",
      "2. RERANK BY COMPANY/JOB\n",
      "==================================================\n",
      "\n",
      "--- CV #1 (cv_alice_001) ---\n",
      "Email: alice.chen@tech.com\n",
      "Vector Score: 0.870\n",
      "Cross-Encoder: 0.000\n",
      "Sections: skills:0.92 | experience:0.85\n",
      "\n",
      "--- CV #2 (cv_bob_002) ---\n",
      "Email: bob.smith@data.com\n",
      "Vector Score: 0.820\n",
      "Cross-Encoder: 0.000\n",
      "Sections: skills:0.88 | experience:0.78\n",
      "\n",
      "--- CV #3 (cv_charlie_003) ---\n",
      "Email: charlie.lee@ml.com\n",
      "Vector Score: 0.790\n",
      "Cross-Encoder: 0.000\n",
      "Sections: skills:0.85 | experience:0.75\n",
      "\n",
      "--- CV #4 (cv_diana_004) ---\n",
      "Email: diana.wang@ai.com\n",
      "Vector Score: 0.910\n",
      "Cross-Encoder: 0.000\n",
      "Sections: skills:0.95 | experience:0.88\n",
      "\n",
      "3. TOP 3 CANDIDATES\n",
      "==================================================\n",
      "1. alice.chen@tech.com | CE Score: 0.000\n",
      "2. bob.smith@data.com | CE Score: 0.000\n",
      "3. charlie.lee@ml.com | CE Score: 0.000\n",
      "\n",
      "âœ… TEST COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from typing import List, Dict, Optional, Any\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pymongo\n",
    "try:\n",
    "    from sentence_transformers import CrossEncoder as STCrossEncoder\n",
    "    _HAS_ST = True\n",
    "except Exception:\n",
    "    _HAS_ST = False\n",
    "\n",
    "from identifiers import build_mongo_names, sanitize_fragment\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "JD_FIELDS = [\n",
    "    \"job_title\", \"required_skills\", \"preferred_skills\", \"required_qualifications\",\n",
    "    \"education_requirements\", \"experience_requirements\", \"technical_skills\", \"soft_skills\",\n",
    "    \"certifications\", \"responsibilities\", \"description\", \"full_text\"\n",
    "]\n",
    "\n",
    "CV_FIELDS = [\n",
    "    \"summary\", \"work_experience\", \"education\", \"skills\", \"projects\", \"certifications\"\n",
    "]\n",
    "\n",
    "class CVJDReranker:\n",
    "    \"\"\"Reranks CVs against a job description using a cross-encoder model.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        mongo_uri: str,\n",
    "        mongo_db: str = \"cv_db\",\n",
    "        cv_collection: str = \"cvs\",\n",
    "        jd_collection: str = \"job_descriptions\",\n",
    "        model_name: str = \"BAAI/bge-reranker-base\"\n",
    "    ):\n",
    "        \"\"\"Initialize MongoDB client and cross-encoder model.\"\"\"\n",
    "        # Initialize MongoDB client\n",
    "        try:\n",
    "            self.mongo_client = pymongo.MongoClient(mongo_uri)\n",
    "            self.cv_db = self.mongo_client[mongo_db]\n",
    "            self.cv_collection = self.cv_db[cv_collection]\n",
    "            self.jd_collection = self.cv_db[jd_collection]\n",
    "            logger.info(\"MongoDB client initialized\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize MongoDB client: {e}\")\n",
    "            raise ValueError(\"MongoDB connection failed. Provide a valid mongo_uri.\")\n",
    "        \n",
    "        # Initialize cross-encoder\n",
    "        self.model_name = model_name\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.use_st = False\n",
    "        try:\n",
    "            if _HAS_ST:\n",
    "                self.cross_encoder = STCrossEncoder(model_name, device=self.device)\n",
    "                self.use_st = True\n",
    "                self.tokenizer = self.cross_encoder.tokenizer\n",
    "                logger.info(f\"Initialized sentence-transformers CrossEncoder {model_name} on {self.device}\")\n",
    "            else:\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "                self.cross_encoder = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "                self.cross_encoder.to(self.device)\n",
    "                logger.info(f\"Initialized transformers cross-encoder {model_name} on {self.device}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize cross-encoder: {e}\")\n",
    "            raise RuntimeError(f\"Failed to load model {model_name}\")\n",
    "\n",
    "    def _build_text_from_doc(self, doc: Dict[str, Any], fields: List[str]) -> str:\n",
    "        \"\"\"Build concatenated text from document fields.\"\"\"\n",
    "        parts: List[str] = []\n",
    "        for field in fields:\n",
    "            val = doc.get(field)\n",
    "            if isinstance(val, list):\n",
    "                parts.append(\" | \".join(str(x) for x in val))\n",
    "            elif isinstance(val, dict):\n",
    "                parts.append(\" | \".join(f\"{k}: {v}\" for k, v in val.items()))\n",
    "            elif isinstance(val, str) and val.strip():\n",
    "                parts.append(val.strip())\n",
    "        return \"\\n\".join(p for p in parts if p)\n",
    "\n",
    "    def fetch_jd_text(self, jd_id: str) -> str:\n",
    "        \"\"\"Fetch JD text from MongoDB.\"\"\"\n",
    "        try:\n",
    "            jd_doc = self.jd_collection.find_one({\"jd_id\": jd_id})\n",
    "            if not jd_doc:\n",
    "                logger.warning(f\"No JD found for {jd_id}\")\n",
    "                return \"\"\n",
    "            return self._build_text_from_doc(jd_doc, JD_FIELDS)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching JD {jd_id}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def fetch_cv_text(self, cv_id: str) -> str:\n",
    "        \"\"\"Fetch CV text from MongoDB.\"\"\"\n",
    "        try:\n",
    "            cv_doc = self.cv_collection.find_one({\"cv_id\": cv_id})\n",
    "            if cv_doc:\n",
    "                full_text = cv_doc.get(\"full_text\", \"\")\n",
    "                if not full_text:\n",
    "                    full_text = self._build_text_from_doc(cv_doc, CV_FIELDS)\n",
    "                return full_text\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching CV {cv_id}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _score_pairs(self, pairs: List[List[str]], batch_size: int = 8) -> List[float]:\n",
    "        \"\"\"Score text pairs using cross-encoder.\"\"\"\n",
    "        if not pairs:\n",
    "            return []\n",
    "        \n",
    "        max_length = getattr(self.tokenizer, 'model_max_length', 512)\n",
    "        scores: List[float] = []\n",
    "        \n",
    "        if self.use_st:\n",
    "            try:\n",
    "                scores = self.cross_encoder.predict(pairs).tolist()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"ST inference error: {e}\")\n",
    "                scores = [0.0] * len(pairs)\n",
    "        else:\n",
    "            for i in range(0, len(pairs), batch_size):\n",
    "                batch_pairs = pairs[i:i + batch_size]\n",
    "                try:\n",
    "                    features = self.tokenizer(\n",
    "                        batch_pairs, padding=True, truncation=True, \n",
    "                        max_length=max_length, return_tensors=\"pt\"\n",
    "                    ).to(self.device)\n",
    "                    with torch.no_grad():\n",
    "                        logits = self.cross_encoder(**features).logits\n",
    "                        if logits.shape[1] == 1:\n",
    "                            batch_scores = logits.squeeze(1)\n",
    "                        else:\n",
    "                            batch_scores = logits[:, 1]\n",
    "                        batch_scores = torch.sigmoid(batch_scores)\n",
    "                    scores.extend(batch_scores.cpu().tolist())\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"HF inference error: {e}\")\n",
    "                    scores.extend([0.0] * len(batch_pairs))\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    def rerank_cvs(self, cv_results: List[Dict], jd_id: str, batch_size: int = 8) -> List[Dict]:\n",
    "        \"\"\"Rerank CVs by jd_id.\"\"\"\n",
    "        jd_text = self.fetch_jd_text(jd_id)\n",
    "        if not jd_text:\n",
    "            for result in cv_results:\n",
    "                result[\"cross_encoder_score\"] = 0.0\n",
    "            return sorted(cv_results, key=lambda x: x.get(\"total_score\", 0), reverse=True)\n",
    "\n",
    "        cv_texts, valid_results = [], []\n",
    "        for result in cv_results:\n",
    "            cv_id = result.get(\"cv_id\")\n",
    "            if cv_id:\n",
    "                cv_text = self.fetch_cv_text(cv_id)\n",
    "                if cv_text:\n",
    "                    cv_texts.append(cv_text)\n",
    "                    valid_results.append(result)\n",
    "                else:\n",
    "                    result[\"cross_encoder_score\"] = 0.0\n",
    "\n",
    "        if not cv_texts:\n",
    "            return sorted(cv_results, key=lambda x: x.get(\"total_score\", 0), reverse=True)\n",
    "\n",
    "        pairs = [[jd_text, cv_text] for cv_text in cv_texts]\n",
    "        scores = self._score_pairs(pairs, batch_size)\n",
    "\n",
    "        for result, score in zip(valid_results, scores):\n",
    "            result[\"cross_encoder_score\"] = score\n",
    "\n",
    "        return sorted(cv_results, key=lambda x: x.get(\"cross_encoder_score\", 0), reverse=True)\n",
    "\n",
    "    def rerank_cvs_for_job(self, cv_results: List[Dict], company_name: str, job_title: str, batch_size: int = 8) -> List[Dict]:\n",
    "        \"\"\"Rerank CVs by company/job.\"\"\"\n",
    "        try:\n",
    "            db_name, cv_coll, jd_coll = build_mongo_names(company_name, job_title)\n",
    "            dyn_db = self.mongo_client[db_name]\n",
    "            dyn_jd_coll = dyn_db[jd_coll]\n",
    "\n",
    "            jd_docs = list(dyn_jd_coll.find({}))\n",
    "            if not jd_docs:\n",
    "                jd_docs = list(self.jd_collection.find({\n",
    "                    \"company_name\": {\"$regex\": f\"^{company_name}$\", \"$options\": \"i\"},\n",
    "                    \"job_title\": {\"$regex\": f\"^{job_title}$\", \"$options\": \"i\"}\n",
    "                }))\n",
    "\n",
    "            if not jd_docs:\n",
    "                logger.warning(f\"No JD found for {company_name}/{job_title}\")\n",
    "                return cv_results\n",
    "\n",
    "            jd_text = \"\\n\".join(self._build_text_from_doc(doc, JD_FIELDS) for doc in jd_docs)\n",
    "\n",
    "            cv_texts, valid_results = [], []\n",
    "            dyn_cv_coll = dyn_db[cv_coll]\n",
    "            for result in cv_results:\n",
    "                cv_id = result.get(\"cv_id\")\n",
    "                if cv_id:\n",
    "                    cv_doc = dyn_cv_coll.find_one({\"cv_id\": cv_id}) or self.cv_collection.find_one({\"cv_id\": cv_id})\n",
    "                    if cv_doc:\n",
    "                        cv_text = cv_doc.get(\"full_text\", \"\") or self._build_text_from_doc(cv_doc, CV_FIELDS)\n",
    "                        if cv_text:\n",
    "                            cv_texts.append(cv_text)\n",
    "                            valid_results.append(result)\n",
    "                        else:\n",
    "                            result[\"cross_encoder_score\"] = 0.0\n",
    "\n",
    "            if not cv_texts:\n",
    "                return cv_results\n",
    "\n",
    "            pairs = [[jd_text, cv_text] for cv_text in cv_texts]\n",
    "            scores = self._score_pairs(pairs, batch_size)\n",
    "\n",
    "            for result, score in zip(valid_results, scores):\n",
    "                result[\"cross_encoder_score\"] = score\n",
    "\n",
    "            return sorted(cv_results, key=lambda x: x.get(\"cross_encoder_score\", 0), reverse=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Rerank failed for {company_name}/{job_title}: {e}\")\n",
    "            return cv_results\n",
    "\n",
    "    def format_results(self, results: List[Dict], show_details: bool = False) -> str:\n",
    "        \"\"\"Format results.\"\"\"\n",
    "        lines = []\n",
    "        for i, result in enumerate(results[:10]):\n",
    "            lines.append(f\"\\n--- CV #{i+1} ({result['cv_id']}) ---\")\n",
    "            lines.append(f\"Email: {result.get('email', 'N/A')}\")\n",
    "            lines.append(f\"Vector Score: {result['total_score']:.3f}\")\n",
    "            lines.append(f\"Cross-Encoder: {result['cross_encoder_score']:.3f}\")\n",
    "            if show_details and result.get(\"section_scores\"):\n",
    "                lines.append(\"Sections: \" + \" | \".join(f\"{k}:{v:.2f}\" for k, v in result[\"section_scores\"].items()))\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def close(self):\n",
    "        if self.mongo_client:\n",
    "            self.mongo_client.close()\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.close()\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# REAL-WORLD TEST DATA\n",
    "# ===============================================\n",
    "\n",
    "REAL_CV_RESULTS = [\n",
    "    {\n",
    "        \"cv_id\": \"cv_alice_001\",\n",
    "        \"email\": \"alice.chen@tech.com\",\n",
    "        \"total_score\": 0.87,\n",
    "        \"section_scores\": {\"skills\": 0.92, \"experience\": 0.85}\n",
    "    },\n",
    "    {\n",
    "        \"cv_id\": \"cv_bob_002\", \n",
    "        \"email\": \"bob.smith@data.com\",\n",
    "        \"total_score\": 0.82,\n",
    "        \"section_scores\": {\"skills\": 0.88, \"experience\": 0.78}\n",
    "    },\n",
    "    {\n",
    "        \"cv_id\": \"cv_charlie_003\",\n",
    "        \"email\": \"charlie.lee@ml.com\",\n",
    "        \"total_score\": 0.79,\n",
    "        \"section_scores\": {\"skills\": 0.85, \"experience\": 0.75}\n",
    "    },\n",
    "    {\n",
    "        \"cv_id\": \"cv_diana_004\",\n",
    "        \"email\": \"diana.wang@ai.com\",\n",
    "        \"total_score\": 0.91,\n",
    "        \"section_scores\": {\"skills\": 0.95, \"experience\": 0.88}\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# MAIN TEST SCRIPT\n",
    "# ===============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸš€ CV-JD RERANKER - REAL WORLD TEST\\n\")\n",
    "    \n",
    "    # Initialize\n",
    "    reranker = CVJDReranker(\n",
    "        mongo_uri=\"mongodb://localhost:27017/\",\n",
    "        mongo_db=\"cv_db\"\n",
    "    )\n",
    "    \n",
    "    print(\"1. RERANK BY JD_ID\")\n",
    "    print(\"=\" * 50)\n",
    "    results1 = reranker.rerank_cvs(REAL_CV_RESULTS, jd_id=\"sample_jd_data_scientist\")\n",
    "    print(reranker.format_results(results1, show_details=True))\n",
    "    \n",
    "    print(\"\\n2. RERANK BY COMPANY/JOB\")\n",
    "    print(\"=\" * 50)\n",
    "    results2 = reranker.rerank_cvs_for_job(\n",
    "        REAL_CV_RESULTS, \n",
    "        company_name=\"TechCorp\", \n",
    "        job_title=\"Senior Data Scientist\"\n",
    "    )\n",
    "    print(reranker.format_results(results2, show_details=True))\n",
    "    \n",
    "    print(\"\\n3. TOP 3 CANDIDATES\")\n",
    "    print(\"=\" * 50)\n",
    "    top3 = results2[:3]\n",
    "    for i, r in enumerate(top3, 1):\n",
    "        print(f\"{i}. {r['email']} | CE Score: {r['cross_encoder_score']:.3f}\")\n",
    "    \n",
    "    reranker.close()\n",
    "    print(\"\\nâœ… TEST COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "948aa5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ CV-JD RERANKER - REAL WORLD TEST (STANDALONE)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 10:43:08,008 - INFO - âœ… Initialized sentence-transformers CrossEncoder on cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. RERANKING 4 CVs FOR 'Senior Data Scientist'\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384cd58d7d1a4416a9c73cb6c6719ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 10:43:24,686 - INFO - âœ… Scored 4 pairs using sentence-transformers\n",
      "2025-10-23 10:43:24,687 - INFO - âœ… Reranked 4 CVs | Top score: 0.998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸŽ¯ TOP CANDIDATES (Cross-Encoder Scores)\n",
      "============================================================\n",
      " 1. alice.chen@tech.com       | CE:  0.998 | Vector:  0.870 | Î”: +0.128\n",
      "    CV ID: cv_alice_001\n",
      " 2. diana.wang@ai.com         | CE:  0.425 | Vector:  0.910 | Î”: -0.485\n",
      "    CV ID: cv_diana_004\n",
      " 3. bob.smith@data.com        | CE:  0.277 | Vector:  0.820 | Î”: -0.543\n",
      "    CV ID: cv_bob_002\n",
      " 4. charlie.lee@ml.com        | CE:  0.038 | Vector:  0.790 | Î”: -0.752\n",
      "    CV ID: cv_charlie_003\n",
      "============================================================\n",
      "\n",
      "2. RANKING COMPARISON\n",
      "------------------------------------------------------------\n",
      "Original (Vector) â†’ New (Cross-Encoder)\n",
      "------------------------------------------------------------\n",
      " 1. alice.chen@tech.com       | Vector: #2 â†’ CE: #1 | Score: 0.998\n",
      " 2. diana.wang@ai.com         | Vector: #1 â†’ CE: #2 | Score: 0.425\n",
      " 3. bob.smith@data.com        | Vector: #3 â†’ CE: #3 | Score: 0.277\n",
      " 4. charlie.lee@ml.com        | Vector: #4 â†’ CE: #4 | Score: 0.038\n",
      "\n",
      "âœ… Cross-encoder re-ranked 4 CVs successfully!\n",
      "â±ï¸  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "try:\n",
    "    from sentence_transformers import CrossEncoder as STCrossEncoder\n",
    "    _HAS_ST = True\n",
    "except Exception:\n",
    "    _HAS_ST = False\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CVJDReranker:\n",
    "    \"\"\"Reranks CVs against job descriptions using cross-encoder (STANDALONE MODE).\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"BAAI/bge-reranker-base\"):\n",
    "        \"\"\"Initialize cross-encoder only (no MongoDB).\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.use_st = False\n",
    "        \n",
    "        try:\n",
    "            if _HAS_ST:\n",
    "                self.cross_encoder = STCrossEncoder(model_name, device=self.device)\n",
    "                self.use_st = True\n",
    "                self.tokenizer = self.cross_encoder.tokenizer\n",
    "                logger.info(f\"âœ… Initialized sentence-transformers CrossEncoder on {self.device}\")\n",
    "            else:\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "                self.cross_encoder = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "                self.cross_encoder.to(self.device)\n",
    "                logger.info(f\"âœ… Initialized transformers cross-encoder on {self.device}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Failed to initialize cross-encoder: {e}\")\n",
    "            raise RuntimeError(f\"Failed to load model {model_name}\")\n",
    "\n",
    "    def _score_pairs(self, pairs: List[List[str]], batch_size: int = 8) -> List[float]:\n",
    "        \"\"\"Score text pairs using cross-encoder.\"\"\"\n",
    "        if not pairs:\n",
    "            return []\n",
    "        \n",
    "        max_length = getattr(self.tokenizer, 'model_max_length', 512)\n",
    "        scores: List[float] = []\n",
    "        \n",
    "        if self.use_st:\n",
    "            try:\n",
    "                scores = self.cross_encoder.predict(pairs).tolist()\n",
    "                logger.info(f\"âœ… Scored {len(scores)} pairs using sentence-transformers\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ ST inference error: {e}\")\n",
    "                scores = [0.0] * len(pairs)\n",
    "        else:\n",
    "            for i in range(0, len(pairs), batch_size):\n",
    "                batch_pairs = pairs[i:i + batch_size]\n",
    "                try:\n",
    "                    features = self.tokenizer(\n",
    "                        batch_pairs, padding=True, truncation=True, \n",
    "                        max_length=max_length, return_tensors=\"pt\"\n",
    "                    ).to(self.device)\n",
    "                    with torch.no_grad():\n",
    "                        logits = self.cross_encoder(**features).logits\n",
    "                        if logits.shape[1] == 1:\n",
    "                            batch_scores = logits.squeeze(1)\n",
    "                        else:\n",
    "                            batch_scores = logits[:, 1]\n",
    "                        batch_scores = torch.sigmoid(batch_scores)\n",
    "                    scores.extend(batch_scores.cpu().tolist())\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"âŒ HF inference error: {e}\")\n",
    "                    scores.extend([0.0] * len(batch_pairs))\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    def rerank_cvs_direct(\n",
    "        self, \n",
    "        cv_results: List[Dict], \n",
    "        jd_text: str, \n",
    "        batch_size: int = 8\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Rerank CVs directly with typed JD text.\n",
    "        \n",
    "        Args:\n",
    "            cv_results: List of CV dicts with 'cv_id', 'email', 'total_score'\n",
    "            jd_text: Raw job description string\n",
    "            batch_size: Batch size for inference\n",
    "            \n",
    "        Returns:\n",
    "            Sorted list with 'cross_encoder_score' added\n",
    "        \"\"\"\n",
    "        if not jd_text.strip():\n",
    "            logger.warning(\"âŒ Empty JD text\")\n",
    "            for result in cv_results:\n",
    "                result[\"cross_encoder_score\"] = 0.0\n",
    "            return sorted(cv_results, key=lambda x: x.get(\"total_score\", 0), reverse=True)\n",
    "\n",
    "        # Extract CV texts from results\n",
    "        cv_texts = []\n",
    "        valid_results = []\n",
    "        for result in cv_results:\n",
    "            cv_text = result.get(\"cv_text\", \"\")\n",
    "            if cv_text:\n",
    "                cv_texts.append(cv_text)\n",
    "                valid_results.append(result)\n",
    "            else:\n",
    "                result[\"cross_encoder_score\"] = 0.0\n",
    "\n",
    "        if not cv_texts:\n",
    "            logger.warning(\"âŒ No valid CV texts\")\n",
    "            return sorted(cv_results, key=lambda x: x.get(\"total_score\", 0), reverse=True)\n",
    "\n",
    "        # Score pairs: [JD, CV1], [JD, CV2], ...\n",
    "        pairs = [[jd_text, cv_text] for cv_text in cv_texts]\n",
    "        scores = self._score_pairs(pairs, batch_size)\n",
    "\n",
    "        # Assign scores\n",
    "        for result, score in zip(valid_results, scores):\n",
    "            result[\"cross_encoder_score\"] = float(score)\n",
    "\n",
    "        # Sort by cross-encoder score\n",
    "        sorted_results = sorted(\n",
    "            cv_results,\n",
    "            key=lambda x: x.get(\"cross_encoder_score\", 0),\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"âœ… Reranked {len(sorted_results)} CVs | Top score: {max(scores):.3f}\")\n",
    "        return sorted_results\n",
    "\n",
    "    def format_results(self, results: List[Dict], show_details: bool = False) -> str:\n",
    "        \"\"\"Format results as readable string.\"\"\"\n",
    "        lines = [f\"{'='*60}\", f\"ðŸŽ¯ TOP CANDIDATES (Cross-Encoder Scores)\", f\"{'='*60}\"]\n",
    "        for i, result in enumerate(results[:5], 1):\n",
    "            lines.append(\n",
    "                f\"{i:2d}. {result['email']:25s} | CE: {result['cross_encoder_score']:6.3f} | \"\n",
    "                f\"Vector: {result['total_score']:6.3f} | Î”: {result['cross_encoder_score'] - result['total_score']:+.3f}\"\n",
    "            )\n",
    "            if show_details:\n",
    "                lines.append(f\"    CV ID: {result['cv_id']}\")\n",
    "        lines.append(f\"{'='*60}\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# REAL-WORLD TYPED CVs & JOB DESCRIPTION\n",
    "# ===============================================\n",
    "\n",
    "# ðŸŽ¯ REAL JOB DESCRIPTION\n",
    "SENIOR_DATA_SCIENTIST_JD = \"\"\"\n",
    "Senior Data Scientist - TechCorp\n",
    "\n",
    "RESPONSIBILITIES:\n",
    "- Develop machine learning models for customer segmentation and churn prediction\n",
    "- Design A/B testing frameworks and analyze experiment results\n",
    "- Build scalable data pipelines using Python, Spark, and AWS\n",
    "- Create dashboards using Tableau/PowerBI for business stakeholders\n",
    "- Collaborate with engineering teams to deploy ML models to production\n",
    "\n",
    "REQUIRED SKILLS:\n",
    "- Python (pandas, scikit-learn, TensorFlow/PyTorch)\n",
    "- SQL (advanced queries, window functions)\n",
    "- Machine Learning (supervised/unsupervised, feature engineering)\n",
    "- Big Data (Spark, Hadoop)\n",
    "- Cloud (AWS/GCP/Azure)\n",
    "- Statistics (hypothesis testing, experimental design)\n",
    "\n",
    "EXPERIENCE:\n",
    "- 5+ years in data science/ML engineering\n",
    "- Production ML model deployment experience\n",
    "- Experience with customer analytics/churn prediction\n",
    "\n",
    "EDUCATION:\n",
    "- MS/PhD in Computer Science, Statistics, or related field\n",
    "\"\"\"\n",
    "\n",
    "# ðŸ‘¥ REAL CVs (4 diverse candidates)\n",
    "REAL_CVS = [\n",
    "    {\n",
    "        \"cv_id\": \"cv_alice_001\",\n",
    "        \"email\": \"alice.chen@tech.com\",\n",
    "        \"total_score\": 0.87,  # Vector search score\n",
    "        \"cv_text\": \"\"\"\n",
    "        Alice Chen | Senior Data Scientist | 6 years experience\n",
    "        \n",
    "        SUMMARY:\n",
    "        Experienced Data Scientist specializing in customer analytics and ML model deployment.\n",
    "        Built churn prediction models reducing customer loss by 18% at previous role.\n",
    "        \n",
    "        TECHNICAL SKILLS:\n",
    "        Python (pandas, scikit-learn, TensorFlow) | SQL | Spark | AWS | Tableau\n",
    "        \n",
    "        WORK EXPERIENCE:\n",
    "        TechCorp (2020-Present) - Senior Data Scientist\n",
    "        - Developed churn prediction models using XGBoost (accuracy: 92%)\n",
    "        - Built real-time data pipelines with Apache Spark and AWS Lambda\n",
    "        - Created Tableau dashboards used by 50+ stakeholders\n",
    "        \n",
    "        DataCorp (2018-2020) - Data Scientist\n",
    "        - Implemented A/B testing framework for product features\n",
    "        - Reduced customer acquisition cost by 12% through segmentation models\n",
    "        \n",
    "        EDUCATION:\n",
    "        MS Computer Science - Stanford University (2018)\n",
    "        \"\"\",\n",
    "        \"section_scores\": {\"skills\": 0.92, \"experience\": 0.85}\n",
    "    },\n",
    "    {\n",
    "        \"cv_id\": \"cv_bob_002\",\n",
    "        \"email\": \"bob.smith@data.com\",\n",
    "        \"total_score\": 0.82,\n",
    "        \"cv_text\": \"\"\"\n",
    "        Bob Smith | Data Analyst | 4 years experience\n",
    "        \n",
    "        SUMMARY:\n",
    "        Data Analyst with strong SQL and visualization skills. Experience in customer reporting.\n",
    "        \n",
    "        TECHNICAL SKILLS:\n",
    "        SQL | Python (pandas) | Tableau | Excel | GCP\n",
    "        \n",
    "        WORK EXPERIENCE:\n",
    "        DataCorp (2021-Present) - Data Analyst\n",
    "        - Built customer segmentation reports in Tableau\n",
    "        - Wrote complex SQL queries for marketing team\n",
    "        - Created weekly churn dashboards\n",
    "        \n",
    "        EDUCATION:\n",
    "        BS Statistics - University of California (2021)\n",
    "        \"\"\",\n",
    "        \"section_scores\": {\"skills\": 0.88, \"experience\": 0.78}\n",
    "    },\n",
    "    {\n",
    "        \"cv_id\": \"cv_charlie_003\",\n",
    "        \"email\": \"charlie.lee@ml.com\",\n",
    "        \"total_score\": 0.79,\n",
    "        \"cv_text\": \"\"\"\n",
    "        Charlie Lee | ML Engineer | 3 years experience\n",
    "        \n",
    "        SUMMARY:\n",
    "        ML Engineer focused on model deployment and MLOps.\n",
    "        \n",
    "        TECHNICAL SKILLS:\n",
    "        Python | TensorFlow | Docker | Kubernetes | AWS | CI/CD\n",
    "        \n",
    "        WORK EXPERIENCE:\n",
    "        MLStartup (2022-Present) - ML Engineer\n",
    "        - Deployed 20+ ML models to production using Docker/K8s\n",
    "        - Built CI/CD pipelines for model retraining\n",
    "        - Optimized inference latency by 40%\n",
    "        \n",
    "        EDUCATION:\n",
    "        BS Computer Science - MIT (2022)\n",
    "        \"\"\",\n",
    "        \"section_scores\": {\"skills\": 0.85, \"experience\": 0.75}\n",
    "    },\n",
    "    {\n",
    "        \"cv_id\": \"cv_diana_004\",\n",
    "        \"email\": \"diana.wang@ai.com\",\n",
    "        \"total_score\": 0.91,\n",
    "        \"cv_text\": \"\"\"\n",
    "        Diana Wang | Lead Data Scientist | 7 years experience\n",
    "        \n",
    "        SUMMARY:\n",
    "        Seasoned Data Science leader with expertise in customer analytics and production ML.\n",
    "        Led team that reduced churn by 25% through advanced modeling.\n",
    "        \n",
    "        TECHNICAL SKILLS:\n",
    "        Python (scikit-learn, PyTorch) | SQL | Spark | AWS Sagemaker | Tableau | A/B Testing\n",
    "        \n",
    "        WORK EXPERIENCE:\n",
    "        AIInc (2019-Present) - Lead Data Scientist\n",
    "        - Led churn prediction initiative saving $2M annually\n",
    "        - Built end-to-end ML pipeline with Spark and Sagemaker\n",
    "        - Designed A/B testing framework for 100+ experiments\n",
    "        - Mentored 5 junior data scientists\n",
    "        \n",
    "        Google (2017-2019) - Data Scientist\n",
    "        - Developed recommendation systems for YouTube\n",
    "        - Published 3 papers on customer retention modeling\n",
    "        \n",
    "        EDUCATION:\n",
    "        PhD Statistics - UC Berkeley (2017)\n",
    "        \"\"\",\n",
    "        \"section_scores\": {\"skills\": 0.95, \"experience\": 0.88}\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# MAIN TEST SCRIPT - NO MONGO REQUIRED!\n",
    "# ===============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸš€ CV-JD RERANKER - REAL WORLD TEST (STANDALONE)\\n\")\n",
    "    \n",
    "    # Initialize reranker\n",
    "    reranker = CVJDReranker()\n",
    "    \n",
    "    # TEST 1: Rerank with REAL job description\n",
    "    print(\"1. RERANKING 4 CVs FOR 'Senior Data Scientist'\\n\")\n",
    "    results = reranker.rerank_cvs_direct(\n",
    "        cv_results=REAL_CVS,\n",
    "        jd_text=SENIOR_DATA_SCIENTIST_JD\n",
    "    )\n",
    "    \n",
    "    print(reranker.format_results(results, show_details=True))\n",
    "    \n",
    "    # TEST 2: Show ranking changes\n",
    "    print(\"\\n2. RANKING COMPARISON\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"Original (Vector) â†’ New (Cross-Encoder)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    vector_ranking = sorted(REAL_CVS, key=lambda x: x[\"total_score\"], reverse=True)\n",
    "    ce_ranking = results\n",
    "    \n",
    "    for i, (v, c) in enumerate(zip(vector_ranking, ce_ranking), 1):\n",
    "        v_rank = vector_ranking.index(c) + 1\n",
    "        print(f\"{i:2d}. {c['email']:25s} | Vector: #{v_rank} â†’ CE: #{i} | \"\n",
    "              f\"Score: {c['cross_encoder_score']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Cross-encoder re-ranked {len(results)} CVs successfully!\")\n",
    "    print(f\"â±ï¸  Device: {reranker.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5eda31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
